# 摘要

我们通过研究如何扩展卷积神经网络中池化的作用来解决显著目标检测的问题。

在u型架构的基础上，我们首先在自下而上的路径上构建了一个全局引导模块(GGM)，旨在在不同的特征层次上提供潜在显著对象的位置信息。

我们进一步设计了一个特征聚合模块(FAM)，使粗级语义信息与来自自上而下路径的精细级特征很好地融合。

通过在上顶路径的融合操作后添加FAMs，GGM的粗级特征可以与不同尺度的特征无缝合并。

这两个基于池化的模块允许逐步细化高级语义特征，产生细节丰富的显著性map

实验结果表明，我们提出的方法能够更准确地定位突出的对象，从而与以往的技术水平相比，大大提高了性能。我们的方法也很快，在处理300×400图像时，可以以超过30FPS的速度运行

# 一、引言

受益于从给定图像中检测出视觉上最独特的物体的能力，显著目标检测在许多计算机视觉任务中扮演着重要的角色，如视觉跟踪[8]、内容感知图像编辑[4]和机器人导航[5]。传统的[11,25,14,31,2,12,41,3]方法大多依靠手工制作的特征来单独或同时捕获局部细节和全局上下文，但由于缺乏高级语义信息，限制了它们在复杂场景中检测整体显著对象的能力。幸运的是，卷积神经网络(CNNs)极大地促进了显著目标检测模型的发展，因为它们能够提取在多尺度空间中高级语义信息和低级细节特征相同的贡献。

​     许多以前的方法[9,28,44]，由于网络的金字塔结构特征，较浅层通常有更大的尺度空间并保持丰富的，详细的低级信息，而更深层次的阶段包含更多的高级语义知识和更好的定位突出对象的确切位置。基于上述知识，设计了各种用于显著目标检测的新体系结构[9,17,38,10]。在这些方法中，基于u型的结构[32,22]最受关注，因为它们能够通过在分类网络上构建自上而下的路径来构建丰富的特征图。

​	尽管这种方法取得了良好的性能，但仍有很大的改进空间。首先，在u型结构中，高级语义信息逐步传递到较浅的层，因此被较深层捕获的位置信息可能同时逐渐被稀释。第二，正如[47]中指出的，CNN的接受域大小与其层深度不成正比。现有的方法通过在u形结构中引入注意机制[46,24]，以循环的方式[23,46,36]细化特征map，结合多尺度特征信息[9,28,44,10]，或在显著性映射中添加额外的约束条件来解决上述问题

​	在本文中，与上述方法不同，我们研究了如何通过扩展池化技术在基于u型的架构中的作用来解决这些问题。一般来说，我们的模型由基于特征金字塔网络(FPNs)[22]的两个主要模块组成：一个全局引导模块(GGM)和一个特征聚合模块(FAM)。

![image-20220608114607656](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220608114607656.png)

我们的GGM由一个修改版本的金字塔池模块(PPM)和一系列的全局引导流(GGFS)组成。与直接将PPM插入u形网络的[37]不同，我们的GGM是一个单独的模块。更具体的是，PPM被放置在主干的顶部，以捕获全局引导信息（显著对象所在的位置）。

通过引入GGFS，PPM收集的高级语义信息可以传递到在所有金字塔层的特征map，弥补了u型网络的自上向下信号逐渐稀释的缺点。

考虑从GGFS融合粗级特征映射与金字塔不同尺度上的特征的融合问题，我们进一步提出了一个以融合后的特征映射作为输入的特征聚合模块(FAM)。该模块首先将融合的特征映射转换为多个特征空间，以捕获不同尺度上的局部上下文信息，然后将这些信息组合起来，更好地权衡融合的输入特征映射的组成。

由于上述两个模块都是基于池化技术的，所以我们称我们的方法为PoolNet。

据我们所知，这是第一篇旨在研究如何设计各种基于池化的模块，以帮助提高显著目标检测的性能的论文。作为这项工作的扩展，我们还为我们的架构配备了一个边缘检测分支，通过联合训练我们的边缘检测的模型来进一步锐化突出对象的细节。为了评估我们提出的方法的性能，我们报告了多个流行的显著目标检测基准测试的结果。我们的池网在很大程度上超过了以前所有最先进的方法。此外，我们进行了一系列的消融实验，让读者更好地理解我们架构中每个组件对性能的影响，并展示与边缘检测的联合训练如何有助于增强预测结果的细节。

我们的网络可以在一个NVIDIA泰坦XpGPU上以超过30fps的速度运行，为一个大小为300×400的输入图像。当没有合并边缘分支时，在一个包含5000张图像的训练集上进行训练只需要不到6个小时，这比以前的大多数方法[24,43,28,44,45,9]要快得多。这主要是由于池化技术的有效利用。因此，PoolNet可以被视为一个基线，以帮助简化未来对显著目标检测的研究。

# 二、相关工作

近年来，由于CNN强大的特征提取能力，大多数传统的基于手工特征[3,12,20,31]的显著性检测方法已经逐渐被超越。

Li等人的[18]使用从CNN中提取的多尺度特征来计算每个超像素的显著性值。

Wang等人[34]采用了两种cnn，旨在将局部超像素估计和全局建议搜索相结合，生成显著性图。

Zhao等人[48]提出了一个多上下文深度学习框架，该框架通过使用两个独立的cnn来提取局部和全局上下文信息。

Lee等人[6]结合了低级的启发式特征，如颜色直方图和Gabor响应，以及从cnn中提取的高级特征。

所有这些方法都以图像patches作为cnn的输入，因此非常耗时。此外，它们忽略了整个输入图像的基本空间信息。

为了克服上述问题，在全卷积网络的启发下，更多的研究重点都集中在了像素级显著性映射的预测上。

Wang等人[36]利用低级线索生成显著性先验图，进一步利用它来指导显著性的预测。

Liu等人[23]提出了一个两阶段的网络，它首先生成粗糙的显著性映射，然后输入局部上下文信息，以递归和分层地细化它们。

Hou等人[9]在多尺度侧输出中引入了短连接，以捕捉细节。

Luo等人[28]和Zhang等人[44]都提出了U形状结构，并利用多个层次的上下文形成来准确检测显著物体。

Zhang等人[46]和Liu等人[24]将注意机制与u型模型相结合来指导特征整合过程。

Wang等人[38]提出了一种网络，反复定位突出对象，然后利用局部上下文信息对其进行细化。

Zhang等人[43]使用双向结构在cnn提取的多层次特征之间传递信息，以便更好地预测显著性图。

.Xiao等人[39]首先采用一个网络对干扰区域进行调整，然后再使用另一个网络进行显著性检测。



我们的方法与上述方法完全不同。我们没有探索新的网络架构，而是研究了如何将简单的池化技术应用于cnn，以同时提高性能和加速运行速度。



# 三、POOLNET

在[23,9,37,38]中指出，高级语义特征有助于发现突出对象的特定位置。同时，低、中层次特征对于改进从深层提取的特征从粗层次到细层次也很重要。基于上述知识，在本节中，我们提出了两个互补的模块，它们能够准确地捕捉突出物体的确切位置，同时锐化它们的细节。

## **3.1. Overall Pipeline**

![image-20220608114607656](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220608114607656.png)

我们基于特征金字塔网络(FPNs)[22]构建架构，这是一种经典的u型架构，以自上向下的方式设计，如图1左上角所示。由于结合分类网络[7,33]的多层次特征的强大能力，这种体系结构被广泛应用于许多视觉任务，包括显著目标检测。

如图1所示，我们引入了一个建立在自下而上路径顶部的全局引导模块(GGM)。通过将GGM提取的高级信息聚合到每个特征级别上的特征map中，我们的目标是明确地注意到不同特征级别层上的显著对象所在的位置。在将GGM的指导信息与不同层的特征合并后，我们进一步引入了特征聚合模块(FAM)，以确保不同尺度上的特征映射可以无缝合并。在下面，我们将描述上述两个模块的结构，并详细解释它们的功能

![poolnet](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\poolnet.png)

## **3.2. Global Guidance Module**

FPN提供了一个经典的体系结构，用于结合多层次特征从分类主干网络中。

然而，由于自上而下的路径是建立在自下而上的骨干网络上的，这种u型结构的问题之一是，当高级特征被传输到较下层时，它们会逐渐被稀释。[49,47]显示，cnn的经验接受域比理论上的要小得多，特别是对于更深的层次，因此整个网络的接受域不够大，不足以捕获输入图像的全局信息。最直接的影响是，只能发现部分突出的物体，如图2c所示。

关于缺乏高级语义信息精细功能地图在自上而下的路径，我们引入一个全球指导模块包含一个修改版本的金字塔池模块(PPM)[47,37]和一系列全局引导流(ggf)明确地使特性地图在每个级别要意识到突出对象的位置。

更具体地说，我们的GGM中的PPM由四个子分支组成，以捕获输入图像的上下文信息。第一个和最后一个子分支分别是一个标识映射层和一个全局平均池化层(1x1)。对于中间的两个子分支，我们采用自适应平均池化层(adaptiveavgpool2d)，以确保它们的输出特征图的空间大小分别为3×3和5×5。给定PPM，我们现在需要做的是如何保证PPM产生的指导信息可以合理地与自上而下的路径中不同层次的特征图相融合。

与之前的工作[37]简单地将PPM视为u型结构的一部分有很大的不同，我们的GGM独立于u型结构。通过引入一系列的全局引导流（标识映射），可以很容易地将高级语义信息传递到不同层次的特征映射中（见图1中的绿色箭头）。通过这种方式，我们明确地增加了自上而下路径的每个部分的全局指导信息的权重，以确保在构建fpn时，位置信息不会被稀释。



## **3.3 Feature Aggregation Module**

利用我们的GGM，允许将全局指导信息传递到不同金字塔级别的特征地图上。然而，一个值得一问的新问题是，如何使来自GGM的粗层特征图与金字塔的不同尺度上的特征图无缝合并。以FPNs的VGGNet版本为例，金字塔中C={C2、C3、C4、C5}对应的特征图与输入图像大小相比的降采样率分别为{2、4、8、16}。在原始的自上而下的fpn路径中，粗分辨率的特征图被上采样了2倍。因此，在合并操作后添加一个核大小为3×3的卷积层可以有效地降低上采样的混叠效应。然而，我们的ggf需要更大的上采样率(例如， 8).必须有效地弥合ggf和不同尺度的特征图之间的巨大差距。

为此，我们提出了一系列的特征聚合模块，每个模块都包含四个子分支，如图3所示。

![image-20220609115657430](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609115657430.png)

前向传递时，输入特征map首先通过输入到具有不同的降采样率的平均池化层转换为不同尺度空间。然后将来自不同子分支的上采样特征图合并在一起，然后是一个3×3的卷积层。

一般来说，我们的FAM有两个优势。首先，它有助于我们的模型减少上采样的混叠效应，特别是当上采样率很大时(例如， 8).此外，它让每个空间位置在不同的尺度空间上查看local context，进一步扩大了整个网络的接受域。据我们所知，这是第一个揭示fam有助于减少上采样的混叠效应的工作。

为了验证我们提出的fam的有效性，我们将图4中fam附近的特征图可视化。通过比较左侧（w/FAM）和右侧（w/oFAM），FAM（第a列）之后的特征映射可以比无FAM（第c列）更好地捕获突出对象。

除了可视化中间特征图外，我们还在图2中展示了一些由具有不同设置的模型产生的显著性图。通过比较f列(w/oFAMs)和g列(w/FAMs)的结果，可以很容易地发现，多次引入FAM可以使我们的网络更好地锐化突出对象的细节。通过观察图2的第二行，这种现象尤其明显

上述所有的讨论都验证了我们的FAMs在不同尺度上更好地融合特征图的显著影响。在我们的实验部分，我们将给出更多的数值结果。

#  四、 Joint Training with Edge Detection

在Sec3中描述的体系结构在多个流行的显著目标检测基准上，已经超过了所有最先进的单模型结果。尽管如此，通过观察我们的模型产生的显著性图，我们发现许多不准确的（不完整的或高预测的）预测是由不清楚的物体边界造成的。

首先，我们试图通过添加一个构建的基于Sec中提出的架构的额外预测分支来解决这个问题来估计突出物体的边界。

详细的结构见图1的顶部。我们在自上而下路径的三个特征水平上，在FAMs[7]后添加三个残差块，用于信息转换。

这些残余块与[7]的设计相似，从细层到粗的通道数为{128,256,512}。

正如在[26]中所做的那样，每个残差块之后是一个16通道3×3卷积层用于特征压缩，加上一个单通道1×1卷积层用于边缘预测。我们还将这三个16通道的3×3卷积层连接起来，并将它们输入三个连续的3×3卷积层，将捕获的边缘信息传输到显著的目标检测分支以进行细节增强。

与[17]类似，在训练阶段，我们使用显著对象的边界作为我们的gd进行联合训练。但是，这个过程并没有给我们带来任何性能的提高，而且一些结果仍然缺乏对象边界的详细信息。例如，如图5的c列所演示的那样，对于前景和背景之间对比度较低的场景，所得到的显著性图和边界图仍然是模糊的。其原因可能是，来自突出物体的ground-truth边缘图仍然缺乏突出物体的大部分详细信息。它们只是告诉我们突出物体的最外层边界在哪里，特别是在突出物体之间有重叠的情况下

考虑到上述参数，我们尝试使用与[26]中相同的边缘检测数据集[1,29]对边缘检测任务进行联合训练。在训练过程中，交替输入来自显著目标检测数据集和边缘检测数据集的图像。如图5所示，与边缘检测任务的联合训练大大提高了检测到的显著对象的细节。我们将在我们的实验部分提供更多的定量分析。

![image-20220609121435831](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609121435831.png)

# **五、Experimental Results**

在本节中，我们首先描述实验设置，包括实现细节、所使用的数据集和评估度量。然后，我们进行了一系列的消融研究，以证明我们所提出的方法的每个组成部分对性能的影响。最后，我们报告了我们的方法的性能，并将其与以前的最先进的方法进行了比较。

##  **5.1Experiment Setup**

所提出的框架是基于PyTorch存储库实现的。所有实验均使用Adam[13]优化器进行，权重衰减为5e-4，初始学习率为5e-5。我们的网络总共被训练了24个epochs。

我们的网络的主干参数(例如，VGG-16[33]和ResNet-50[7])用在ImageNet数据集[16]上预训练的相应模型初始化，其余的模型随机初始化。

默认情况下，我们的消融实验是基于VGG-16主干和MSRA-B[25]和HKU-IS[18]数据集的联合集，如[17]，除非有特殊解释。我们只使用简单的随机水平翻转来增强数据。在训练和测试中，输入图像的大小都保持不变，如在[9]中所做的那样。

**Datasets & Loss Functions**

为了评估我们提出的框架的性能，我们在6个常用的数据集上进行了实验，包括ECSSD[41]，pascals[21]，DUT-OMRON[42]，HKU-IS[18]，SOD[30]和DUTS[35].

我们使用标准的二值交叉熵损失进行显著目标检测，平衡二值交叉熵损失[40]边缘检测

**Evaluation Criteria**

我们使用三种广泛使用的指标来评估我们的方法和其他方法的性能：精度召回率(PR)曲线、F-measure score和平均绝对误差(MAE)。

F-measure，记为$F_β$，是一种整体性能测量，由精度和召回率的加权调和平均值计算：

![image-20220609171127744](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609171127744.png)

$β^2$设为0.3

MAE score表明了一个显著性map与地面真实值G之间的相似程度

![image-20220609171242114](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609171242114.png)

## **5.2. Ablation Studies**

在本小节中，我们首先研究我们提出的GGM和FAMs的有效性。然后，我们对GGM和FAMs的配置进行了更多的实验。最后，我们展示了联合训练与边缘检测对性能的影响。

**Effectiveness of GGM and FAMs**

为了证明我们提出的GGM和FAMs的有效性，我们进行了基于具有VGG-16主干的FPN基线的消融实验。

![image-20220609171427992](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609171427992.png)

**GGM Only**

添加GGM（表1中的第4行）在两个数据集上的GGM和MAE方面都比FPN基线有性能提高。由GGM产生的全局指导信息使我们的网络能够更多地关注显著性对象的完整性，极大地提高了结果的显著性图的质量。因此，突出物体的细节可以被锐化，这可能会被错误地估计为具有有限接受域的模型的背景

**FAMs Only**

简单地将FAMs（表1的第5行）嵌入到FPN基线中，如图1所示, 也有助于提高在相同的两个数据集上的f-度量和MAE分数的性能。

这可能是因为与基线相比，FAMs内部的池化操作也扩大了整个网络的接受域，而FPN基线仍然需要合并来自不同层次的特征图，这表明了我们的FAMs在解决上采样的混叠效应方面的有效性。

**GGM & FAMs.**

通过在基线中同时引入GGM和FAMs（表1的最后一行），与上述两种情况相比，可以进一步提高在f-测量和MAE评分上的性能。这一现象表明，我们的GGM和FAM是两个互补的模块。利用它们的利用，我们的方法能够准确发现突出物体和细化细节，如图2所示。更多的定性结果也见图6。

**Confifiguration of GGM**

为了更好地理解我们提出的GGM的组成，我们进行了两个消融实验，分别对应于表1的第2行和第3行。我们或者删除其中一个PPM和ggf，同时保持另一个不变。可以看出，与考虑的结果相比（第4行），两种操作都导致性能下降。这些数值结果表明，PPM和ggf在我们的GGM中都起着重要的作用。没有任何一种方法对我们的方法的执行是有害的。

为了进一步提高我们的方法所产生的显著性映射的质量，我们尝试将边缘检测与显著性目标检测的联合训练方式相结合。

在表2中，我们列出了当考虑两种边界信息时的结果。可以看出，使用显著对象的边界作为监督的结果没有改进，而使用标准边界可以大大提高所有三个数据集的性能，特别是在MAE度量上。这说明涉及详细的边缘信息有助于显著的目标检测。

![image-20220609172234002](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609172234002.png)

## **5.3Comparisons to the State-of-the-Arts**

![image-20220609172516269](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220609172516269.png)