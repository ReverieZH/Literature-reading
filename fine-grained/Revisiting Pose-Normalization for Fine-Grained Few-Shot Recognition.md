# **Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition**

少镜头的、细粒度的分类需要一个模型来学习仅基于一些图像的不同类别（如鸟类）之间的细微的、细粒度的区别。

这需要对姿势、发音和背景的显著程度的不变性。

一种解决方案是pose-normalized表示：首先在每个图像中定位语义部分，然后通过表征每个部分的外观来描述图像。

虽然这种表示不适合完全监督分类，但我们证明了它们对于少量的细粒度分类是非常有效的。

通过模型容量的最小增长，姿态归一化为浅层和深度架构提高了10到20个百分点的精度，可以更好地推广到新的领域，并且对多个少镜头算法和网络骨干都很有效。

# **1. Introduction**

用最小的微调来泛化的能力是学习的神经模型的一个关键特性，不仅对不可见的数据，而且对不可见的数据类型。请考虑一下图1中所示的任务。

我们只得到一些鸟类的一张图像（或非常小的数字），仅从这些信息我们必须学会识别它们。

众所周知，人类非常擅长这种少镜头的学习任务[19]，但机器却很挣扎：尽管在视觉识别方面取得了巨大的进展和两年的集中研究，但在几个少镜头基准上的表现仍然远低于完全监督的方法。

这在实践中是一个问题，特别是对于细粒度的分类问题（如图1中所示）。在这种设置下，不同的类可以达到数百个，而正确标记这些类所需的专业知识和努力可能会使注释昂贵。总之，这使得用于细粒度分类的大型标记训练集的集合变得困难，有时甚至非常困难。因此，神经网络处理细粒度、少镜头学习的能力对于现实应用是至关重要的。

机器和人类在这项任务上的巨大差距背后的原因是什么？一个直观的假设是，人类使用了一个更稳定的特征表示，这对大的空间变形是不变的。例如，在鸟类分类任务中，我们可以用鸟类不同部分的属性来描述鸟类的图像：喙的形状、翅膀的颜色、是否有冠。这种特征不仅对图像背景的变化不变，而且对相机姿势和关节的变化也不变，使我们能够有效地感知各种鸟类和个体图像的相似和不同之处。

这种特征化是“姿态归一化”的，并且在重新发现卷积网络之前，它被探索为细粒度分类的一个很有前途的方向。然而，研究人员发现，使用黑盒架构的端到端训练，并且不进行姿态标准化，导致了标准基准测试的很大改进(尽管进行了一致的修改，如双线性池[16]）。事实上，近年来，[1]所面临的年度细粒度分类挑战的赢家主要集中在这些黑盒架构上。pose normalization的直观概念已经被搁置了。

相反，我们认为黑盒架构相对于姿态标准化表示的主导地位是完全监督分类问题的一个人为因素。.在这些设置中，所有要区分的类都是预先已知的，并且我们对每个类都有大量的训练数据。这减少了对姿态和背景不变性的需要，因为训练数据可能包括每个类内广泛的变化。同时，利用姿态和背景中特定类别的偏差可能是有益的，因为表示不需要推广到新的类。这些因素有利于没有内置的归纳偏差的黑盒架构。然而，如果我们希望学习到的模型能够适应来自有限数据的新类，就像在少镜头学习中一样，姿态归一化的直观不变性就会变得更加有用。

在本文中，我们重新讨论了少镜头、细粒度分类任务的姿态标准化，并展示它在这种情况下的有用性。姿态标准化是通过对卷积架构的极其简单的修改来实现的，添加了很少的新参数（与之前的方法将网络大小增加两倍或更高的[34,10]）。我们的方法是正交于选择的少镜头学习技术和主干神经结构。

我们在三种不同的少镜头学习技术、两种不同大小的主干架构和三个鸟类物种和飞机的细粒度分类数据集上评估了我们的方法。

我们发现：

1.姿态标准化提供了显著的全面收益，在某些情况下提供了超过20点的准确性改进，而不需要对新类进行部分注释。

2.在所有设置中，姿态归一化优于神经结构的黑盒修改，如双线性池。

3.即使只有5%的基类训练数据用姿态标注，姿态归一化的优点也是明显的。

4.姿态归一化对于浅层和深度网络架构都是有效的。具有姿态标准化的浅层网络优于更深的黑盒网络。

我们观察到的巨大的性能提高，以及架构本身的简单性，表明了在细粒度、少镜头的分类中，姿态规范化的力量。

# **2. Related Work**

细粒度识别是计算机视觉中的一个经典问题，也是[1]反复出现的挑战。虽然我们关注的是鸟类分类[26]，但提出的想法适用于其他细粒度任务，如识别飞机[17]模型，汽车[15]，或任何其他物体有一组一致部件的问题。在细粒度识别的背景下，Farrell等人[6]提出了姿态标准化的想法：预测对象的部分，并记录每个部分的外观作为描述符。这个想法的许多版本已经被探索过，包括改变部分[32,10,33]的种类，部分检测器[31]，以及这些想法与神经网络[34]的结合。最后一个与我们的工作最相似。.然而，所有这些方法都是关于完全监督识别的，而在这里我们看的是少镜头识别。

姿态标准化也作为黑箱模型的灵感，其中的部分是无监督的。Lin等人[16]引入双线性池作为这种归一化的推广，我们在工作中与这一想法进行了比较。 Spatial Transformer Networks [14]显式地实例化无监督姿态归一化，并进行端到端训练。这种直觉的其他实例也被提出了[4,11,21]。然而，这些无监督的方法增加了显著的复杂性和计算，使得很难辨别姿态归一化的好处。相比之下，我们关注一种轻量级、直接、语义的方法来表明姿态标准化，而不是增加网络能力，负责提高性能。

少镜头学习方法可以松散地分为以下三组：1)迁移学习基线在基类上训练标准分类网络，然后在冻结表示上为新的类学习一种新的线性分类器。最近的研究表明，这是具有竞争性的[3,27,18]。2)元学习技术训练一个“学习者”：一个映射小的标记训练集和测试图像来测试预测的函数。例如，包括ProtoNet[20]、MatchingNet[25]、RelationNet[22]和MAML [7]。这些学习者有时可能包括学习了数据增强[28]，其中一些方法使用姿势注释[5]进行训练。3)权重生成技术为新的类别[8,9]生成分类权重。

大多数少镜头学习方法使用黑盒网络架构，它在给定足够的标记数据的情况下功能良好，但在高度约束的少镜头学习场景中可能会受到影响。韦特海默和哈里哈兰·[29]重新审视了Lin等人[16]的双线性池，发现它的效果很好。它们还引入了一种简单、有效的 localization-

normalized representation，但它仅限于粗糙的对象边界框，而不是细粒度的部分。Zhu等人[35]引入了一个语义引导的多注意模块来帮助零射击学习，但它是完全无监督的。我们与实验中的无监督基线进行了比较。

姿态归一化增加了对常见变异模式的不变性。增加不变性的另一种方法是使用学习数据增强[12,28,5]。然而，这通常需要大量的额外网络和大量的计算。相反，我们专注于一种轻量级的方法。还要注意，我们的一个基线[8]已经优于最近的增强方法[28]。

在下面的部分中，我们首先概述少镜头识别。然后，我们证明了特征的姿态标准化可以在一系列的少镜头学习算法中充当一个即插即用的网络层。

# **3. Few-Shot Recognition**

少镜头学习的目标是建立一个学习者，可以产生一个有效的分类器，只给定一小组标记的例子。

在经典的少镜头设置中，学习者首先被提供一个大的标记集（表示集，$D_{repre}$），由许多来自基类$Y_{base}$的标记图像组成。学习者必须使用这些数据设置其参数和任何超参数。然后它遇到一组不相交的新类$Y_{noivel}$，从中它得到一组小的参考图像$D_{ref er}$。然后，学习者必须从这个集合中学习一个新类的分类器。然后，学习者必须从这个集合中学习一个新类的分类器

在大多数技术中，我们可以将学习者分为三个模块：一个特征映射提取器fθ，一个特征聚合器gφ，和一个学习算法hw。

**特征图提取器**fθ通常是一个深度卷积神经网络，具有可学习的参数θ。对于每个输入图像x，网络生成相应的特征图张量$F=f_θ(x)∈R^{C×H×W}$，其中C、H、W分别表示特征图的通道、高度和宽度维度。

**特征聚合器**gφ是一个由φ参数化的转换，将特征映射转换为全局特征向量：$v=g_φ(F)∈R^d$，其中d是潜在维数。通常，gφ是一个全局平均池化模块。

**学习算法**hw取一个训练特征向量和相应标签的数据集S和一个测试特征向量v，并为后者输出一个标签$\hat p$上的概率分布：$\hat p(x)=h_w（v，S）$。

为了达到我们的目的，我们考虑了三种具有代表性的方法：

**迁移学习**遵循标准的网络预训练和微调程序。hw是由一个简单的线性分类器与学习的权重矩阵和softmax激活。函数fθ、gφ与hw同时训练，最小化Drepre中数据的标准交叉熵损失。

为了使模型适应于新的类，特征提取器参数θ、φ被冻结，并在Dref er中的新类上训练一个新的线性分类器。

**原型网络**[20]是一种具有代表性的 meta-learning方法，它通过平均该类内的特征向量来为每个类生成一个原型表示。hw是一个非参数分类器，基于数据点的特征向量和每个类原型之间的距离来分配类概率。每个训练集从基本类别Ybase中抽取N个类，并从每个类别中抽取一个小的支持集和查询图像集。支持图像形成类原型，而Nway分类在查询集上产生损失，并产生相应的更新梯度到参数θ，φ。

在**动态少镜头学习**[8]中，hw再次是一个线性（或余弦）分类器，但不是直接在Dref er上进行微调，该分类器由学习到的权值发生器G生成。培训过程包括两个阶段。第一个是对Drepre的标准分类训练。在第二阶段，特征提取器参数θ、φ被冻结。为了训练生成器G，算法从Ybase中随机选择几个“假”新类，并将它们当作真正的新类，使用G生成的分类器权值进行分类，并将这些类中模拟的“测试”例子的分类损失最小化。

# **4. Pose-Normalized Feature Vectors**

两种直觉激发了我们提出的方法。首先，对于细粒度识别，两个类之间的外观差异往往非常小。在少量设置中，算法更难捕捉这些细微的差异，因为只有几个例子可供参考。使用姿态归一化将特征表示集中在每个图像中信息最丰富的部分，这将有利于学习过程。其次，因为细粒度识别涉及到相似类型的对象，所以它们很可能共享相同的语义结构。

因此，在基类上训练的姿态估计器很可能会推广，即使是看不见的新类。

我们假设有M个不同的部分。部分注释可用于Drepre中的（一些）基类训练示例，但不适用于新的类。我们将每个图像x的部分注释格式化为M×H×W位置张量m∗，其中H×W是特征图的空间分辨率。

我们现在提出了我们提取姿态归一化特征向量的方法。为此，网络必须首先估计姿态。我们使用一个非常小的，两层卷积网络qφ。

这将对从特征图提取器fθ的中间层中提取的特征图张量$F'∈R^{C'×H'×W'}$进行操作.qφ在最后一层使用s型激活，并对所有注释部分$m=q_φ（F'）∈R^{M×H×W}$生成热图位置预测.我们故意在fθ中使用一个小的qφ和重用计算，以最小化附加参数可能对分类器的最终性能产生的影响。改进的性能应该表明，姿态信息对于细粒度的少镜头学习是有用的，而不是一个更大的网络。

给定热图m和特征图F，我们必须构造一个特征向量v。将m中的每个通道作为空间注意掩模应用于特征图上，生成一个注意归一化的特征向量。将这些M个部分的描述向量连接起来，可以得到图像的最终表示向量。形式上，以$F（h，w）∈R^C$为特征图F中位置（h，w）的特征向量，$m_i（h，w）∈R$为第i部分类别位置（h，w）的热图像素值，v∈RCM计算为：

![image-20220915170759618](D:\文献阅读\fine-grained\image\image-20220915170759618.png)

训练过程中的损失是地面真实部分位置热图m∗与预测的热图m之间的像素级对数损失之和，以及原始的少镜头分类损失之和：

![image-20220915171039815](D:\文献阅读\fine-grained\image\image-20220915171039815.png)

其中，α是一个平衡的超参数。为了便于在分类分支中学习，首先从地面真实部分注释热图m∗而不是预测的热图m中生成用于少射分类的特征向量。然后，冻结姿态估计网络的参数φ。在随后的对新类的自适应/微调和评估/推理阶段中，根据预测的热图m计算出特征向量。图2提供了对我们的方法的概述。

![image-20220915171759134](D:\文献阅读\fine-grained\image\image-20220915171759134.png)

请注意，虽然我们在训练期间假设有一组固定的一致的部件标签，但我们不要求部件在所有对象中一致地出现，也不能要求任何特定的对象包含所有指定的部件。因此，我们的姿态估计器应该广泛推广：任何依赖于不同部件（如汽车、家具、昆虫）外观的物体的细粒度分类都适用于这种方法。

# **5. Experiments**

## **5.1. Datasets and implementation details**

我们用CUB数据集[26]进行了实验，它由来自200个类的11,788张图像组成。它还包括每个图像的15个部分注释，因此是M=15。

根据[29,3]中的评估设置，我们将数据集随机划分为100个碱基类、50个验证类和50个新类。

基本类别图像形成表示集$D^{CUB}_{repre}$。

对于每个验证和新类，我们随机抽取20%的图像，形成参考集$D^{CUB}_{ ref er}$。

剩余的新图像形成查询集$D^{CUB}_{query}$，用于评估算法

请注意，我们的模型只能在基类中访问部分注释。

对于验证类或新类中的任何图像，都没有不可用的部分注释信息，包括它们的引用集和查询集。

**NABird evaluation**:

在CUB的评估集中只有50个新的类，这可能会使评估产生噪声。在域移位[3]存在的情况下，少镜头学习算法之间的精度差异也显著减小。因此，为了验证我们提出的方法的鲁棒性和泛化能力，我们还在另一个更大的鸟类数据集NABird [23]（NA）上评估，该数据集在去除与CUB重叠后，包含418类和35,733张图像。与之前一样，我们从每个类别中随机抽取20%的图像，形成参考集$D^{NA}_{refer}$。其余的图像形成了查询集的$D^{NA}_{query}$。

**Network backbone**:：对于特征图提取器fθ，之前的工作[20,29,8]采用了一个标准的架构：一个4层、64通道的卷积网络，具有批量归一化和ReLU。

在此设置下，输入图像大小为84×84，输出特征图为64×10×10。更深的骨干可以显著降低这些方法[3]之间的性能差异，因此除了4层网络，我们还训练和评估了ResNet18 [13]骨干，并进行了一些技术修改，提高了所有模型的性能。我们将最后一个块的第一个卷积和降采样层的步伐从2更改为1。因此，最后一个块的输出大小保持在14×14，而不是7×7。我们还在原始ResNet18的最后一层添加了一个具有批量归一化的1×1卷积，这将信道的数量从512个减少到32个。我们修改后的ResNet18的输入大小仍然是224×224，但输出大小变成了32×14×14。

**Pose estimation module**:

姿态估计网络qφ的层由Conv-BN-ReLU-Conv组成，其中Conv表示3×3卷积。在4层卷积神经网络中，qφ以第二次卷积后的特征图作为输入。qφ中两个卷积层的输入/输出通道数分别为64/30和30/M，其中M为零件类别的数量。

在ResNet18中，qφ以第三个块的特征图作为输入，对应的卷积通道大小分别为256/64和64/M。可以看出，与原始主干网络相比，qφ引入的可学习参数数量较少。

## **5.2. Baseline methods**