# A Novel Plug-in Module for Fine-Grained Visual Classifification

视觉分类可分为粗粒度分类和细粒度分类。粗粒度分类表示具有很大程度不同的类别，如猫和狗的分类，而细粒度分类表示具有大量相似性的分类，如猫的种类、鸟类种类和车辆的制造或模型。与粗粒度视觉分类不同，细粒度视觉分类通常需要专业专家对数据进行标签，这使得数据更加昂贵。

为了应对这一挑战，许多方法提出了自动找到最具区别性的区域，并使用局部特征来提供更精确的特征。这些方法只需要图像级的注释，从而降低了注释的成本。然而，这些方法大多需要两阶段或多阶段的架构，并且不能进行端到端训练。

因此，我们提出了一种新的插件模块，它可以集成到许多共同的主干上，包括基于cnn或基于**Transformer**的网络，以提供很强的鉴别区域。该插件模块可以输出像素级的特征图，并融合过滤后的特征，以增强细粒度的视觉分类。实验结果表明，所提出的插件模块优于现有的方法，在CUB200-2011和NABirds上的准确率分别显著提高到92.77%和92.83%

# I. INTRODUCTION

视觉分类可分为粗粒度分类和细粒度分类。粗粒度分类代表类别的分类有很大程度的不同，如猫和狗的分类，而细粒度分类代表分类有很大程度的相似性，如鸟类[1] [2]，狗物种[3]，和车辆[4]的制造或模型。

“细粒度”是指在普通物种分类下的更细粒度的划分。

细粒度视觉分类任务的挑战有三重的。

首先，在同一类别中有很多变化。以鸟类为例，同一只鸟从不同角度拍摄的照片在颜色和形状上差异很大，如图1所示。

第二点是，具有不同子类别的对象可能非常相似。如图1所示，这三只鸟的纹理非常相似。

第三点是，与粗粒度分类不同，细粒度分类通常需要专业专家来标记数据，这使得数据更加昂贵。

在粗粒度分类任务上表现非常好的框架，如ResNet [5]、EffificientNet[6]、 Vision Transformer (ViT)（ViT）[7]，对于细粒度分类将非常有限。

为了找到较强的鉴别区域来增强细粒度的视觉分类，所提出的方法可以分为三类。

第一种方法通过区域建议网络（RPN）RPN找到区域，如NTS-Net [9]，FDL [10]，和[8] StackedLSTM[11]。\

第二种方法通过注意机制来加强特征图，如CAL [12]、MA-CNN [13]、MAMC [14]、API-Net [15]和WS-DAN [16]。

第三种方法利用自注意机制中的注意图的强度作为区分区域的判断，如TransFG [17]和FFVT [18]。

前两种方法主要是基于卷积神经网络（CNN），如ResNet [5]、DenseNet [19]和效率网[6]，而第三种方法是基于ViT来实现细粒度的视觉分类。

上述方法在找到这些区域后，通过裁剪和调整大小，将原始图像和特征图重新输入到网络中，或者使用注意机制来加强特征图之间的关系。这些方法的缺点是，它们中的大多数需要两阶段或多阶段的复杂体系结构，并且不能进行端到端训练。

此外，这些定位方法往往会产生较大的预测区域，而不太能够关注细微的局部特征。另一方面，基于vit的方法直接利用自注意机制中的注意图作为选择区域的基础，避免了基于反馈的架构，实现了高效的端到端训练。但这种方法很难推广到卷积神经网络或其他体系结构中，这意味着其可伸缩性是有限的。

为了更好地理解特征映射与目标位置之间的关系，我们探讨了目标检测模型和FGVC方法之间的关系。从Faster-RCNN [8]，YOLO [20] [21]和RetinaNet [22]中，我们可以发现特征地图可以提供丰富的位置和类别信息。诸如Mask-RCNN [23]等分割方法给出了像素级的预测，即细粒度的预测。虽然这些方法证明了特征图具有丰富的语义，但它们是通过依赖人类注释的区域信息来学习的，而FGVC任务只使用图像级注释进行训练。因此，我们讨论了弱监督对象检测（WSOD）方法，如WSDDN [24]、OICR [25]和WCCN [26]等。WSOD方法的概念是特征图的响应反映了对象的位置，我们可以利用这些特征图通过弱监督的多阶段体系结构和损失函数设计来完成边界盒预测。

基于上述分析，我们提出了一个可以添加到许多公共骨干网中的插件模块，它可以在基于cnn或基于变压器的架构上实现。此外，该插件模块输出像素级的特征映射，并融合过滤后的特征。

为了更好地检测不同大小的关键特征，我们在主干网络中添加了特征金字塔网络（FPN）[27]，以混合不同尺度的空间特征。这种体系结构在目标检测任务中非常常见，这种方法可以提高局部表示的质量。

插件模块的思想由三个操作组成，包括划分、竞争和组合。以图2为例，通过分割背景得到的斑块几乎是纯色。这种补丁将广泛地出现在不同类别的数据中。如果我们使用这些补丁进行训练，我们可以预期测试图像的预测分布将非常平坦。相反，如果我们使用包含有区别的对象的补丁进行训练，预测的结果将更容易区分。

本文有以下两个主要贡献。

- 我们提出了一种新的插件网络，可以应用于各种模型。这个网络集成了新的背景分割与特征融合技术,这可以有效地提高细粒度视觉分类的准确性。

- 该网络优于最先进的网络，在CUB200-2011 [1]和NABirds [2]上的准确率分别提高到92.77%（+0.97%）和92.83%（+1.83%）

## II. RELATED WORK

在本节中，我们将介绍针对粗粒度图像识别任务的架构，它主要分为卷积方法和自注意机制。然后，我们将介绍目标检测任务，并讨论我们将使用的想法。最后，我们将分析几个最近表现良好的细粒度视觉分类模型，并在实验结果中比较这些方法。

## *A. Backbones*

CNN在图像识别方面已经相当成功了。

由于在ImageNet [29]上的AlexNet [28]的精度已经大大超过了传统的方法，因此人们提出了各种卷积架构来提高识别的精度。例如，ResNet [5]和DenseNet [19]使用具有快捷连接的深度网络结构在各种识别任务中表现得很好，它们在ImageNet上预先训练的模型也成功地转移到各种计算机视觉任务中。EffificientNet[6]进一步讨论了卷积网络的深度、宽度和输入分辨率，以优化网络结构。该方法在少量参数下成功地保持了较高的精度。自《Transformer[30]》出版以来，自我注意机制在自然语言处理和计算机视觉领域得到了广泛的应用，其性能相当好。

例如，Vit [7]在ImageNet上实现了90.7%的准确率，它通过线性变换将图像的补丁转换为标记，并通过自我注意机制完成全局信息交流。

但其缺点是对局部区域的特征没有分层表达的能力。在 SwinTransformer[31]中，通过多层自注意结构提取不同尺度下的局部区域特征。由于上述架构的成功，我们将使用这些模型作为骨干网络来测试所提出的模块的能力。

## *B. Object detection*

目标检测的目标是找到目标的位置和分类，因此其整体的架构和思想将与细粒度的视觉分类任务非常相似。不同之处在于，细粒度视觉分类的目的不在于是否找到了对象的区域，而是是否找到了具有可区分特征的区域，以及这些特征是否可以更有效地用于识别。

首先，我们探索了有监督的目标检测网络。例如，Faster-RCNN [8]通过区域提案网络（RPN）预测特征地图上的每个像素位置是否为一个对象，然后预测对象区域的类别。YOLO [20] [21]和RetinaNet [22]通过整个网络完成了对位置和类别的预测。以上方法都从手动注释中学习对象的区域，并完成对象区域的识别。

让我们从另一个角度来解释这个过程。在训练阶段，网络需要关注这些有对象的区域，然后完成该区域的分类预测。整个过程与FGVC提出的方法非常接近，但我们打算提出的FGVC方法并没有使用目标位置信息作为训练数据。相反，我们扩展到目标检测中的弱监督学习方法，它不依赖于手动注释对象区域。

## *C. Weakly supervised object detection*

弱监督对象检测（WSOD）依赖于来自深度神经网络的有意义的特征映射。

B. Zhou等人[32]观察到，通过学习对象类标签，可以学习对象在空间中的表示，即通过特征地图上的信息来进行对象位置的虚拟标记。

例如，WSDDN [24]使用预训练好的模型作为特征提取器，通过two-stream网络完成对候选区域特征的预测，然后利用预测结果对区域进行过滤。也就是说，通过对每个区域的预测质量来完成识别。

OICR [25]继续了前一种方法，并添加了一个多阶段分类器来完成更准确的定位。

WCCN [26]首先通过类激活图（CAM）[33]完成对候选区域的划分，CAM的原则是在之前的特征图上绘制地图预测得分，生成类特定的热图，然后使用第二阶段模型筛选更好的候选区域。

ACoL [34]和SPG [35]等方法也是基于CAM来完成定位的。

DANet [36]通过比较融合类别来学习更多多样化的特征，并使用CAM来定位更完整的对象。

CASD [37]通过自蒸馏的方法训练OICR[25]，

MIST[38]通过生成虚拟标签进行自我训练。

上述方法表明，类别标签可以提供丰富的对象位置特征，并定位更多的类识别区域。但是，使用WSOD的任务仍然与使用FGVC的任务不同。FGVC任务的目标不再是检测完整的对象，而是找到最关键的区域，并利用这些区域进行更好的识别。

## *D. Fine-grained visual classifification*

FGVC的主要目的是定位具有高度歧视性的区域。许多方法如NTS-Net[9]，FDL[10]，StackedLSTM[11]找到强大的歧视预选盒子通过Region Proposal Network（RPN），然后调整预选盒子固定大小，并使用这些局部强大的特性来识别，也就是说，这些方法实际上会将识别任务分为两个阶段

Mix+ [39]使用注意力图来找到具有很强辨别能力的区域，并使用这张图来完成混合强化。

CCFR [40]通过这些区域特征识别全局特征，并通过三重损失和尺度分离NMS学习更好的局部区域特征。

注意机制也被广泛应用于FGVC任务中，如MACN [13]、WS-DAN [16]和CAL [12]，以学习对象的位置，并通过注意图在特征图中提取特定位置的特征。

.注意机制除了用于学习位置信息外，还可以提高特征的表达性。

例如，MAMC [14]和API-Net [15]通过注意机制同时训练一对相似图像之间的差异来学习独特的特征，而CAP [41]通过注意机制融合不同的局部特征。以上的方法都表明，注意机制是一种非常强大的方法。

自从 Vision Transformer（ViT）[7]发布以来，ViT体系结构在图像识别领域表现得很好，因此人们提出了许多方法将该体系结构应用于FGVC任务中。例如，FFVT [18]、AFTrans [42]、RAMS-Trans [43]和TransFG [17]使用ViT作为骨干网络，并利用自注意中图像生成的注意力图来寻找强辨别区域。最后，对这些区域的特征进行处理，以完成识别任务。该方法类似于上述基于卷积网络的方法。如图3左侧所示，不同之处在于，我们使用了注意图的强度，而不是特征图的响应。

![image-20220920215321215](image\image-20220920215321215.png)

Fig3 过去的细粒度视觉分类网络的流程示意图。这些方法的模型架构和训练过程如图3所示。中间的浅蓝色方块（块1∼块）表示主主干网络的块（例如ResNet [5]，ViT [7]）。转移方向用箭头表示。主干网络右侧的浅红色实心方块表示输入图像经过卷积网络后的特征图。此特征贴图可用于完成对象定位或重新切割。我们将这个过程命名为本地化模块。最后，将集成的信息发送到第二阶段模型，用虚线方块表示。第二阶段模型可以是专门设计的结构，也可以是原始网络。这样，就可以更准确地利用局部特征来完成识别。主干网络左侧的浅紫色实心正方形表示变压器生成的注意图，后续过程与之前基于卷积模型的过程相同。

# III. A NOVEL PLUG-IN MODULE FOR FINE-GRAINED VISUAL CLASSIFICATION

为了找到细粒度分类任务的强区分区域，我们提出了一个插件模块，可以应用于主流主干网络，如ResNet [5]、EffificientNet[6]和ViT [7]。

总体的设计理念是将特征图上的每个像素（或补丁）视为一个独立的特征，它可以代表其区域。然后，我们对这些特征进行分类，并以分类能力作为区分的基础。最后，可以通过端到端培训来完成整个网络。

在本章中，模块结构的设计，损耗函数的使用，并将详细介绍与每个框架的结合。

## *A. Module design*

上面讨论的FGVC和WSOD方法的共同点是找到一个具有很强辨别能力的区域。

FGVC在二次训练过程中减少了这一区域或增加了注意力，而WSOD框架通常使用多实例学习（MIL）来进行更准确的定位。

这两个任务在定位目标上仍然略有不同，但这些体系结构可以证明特征图中的像素级特征可以代表该区域在分类任务中的重要性。

在下面，我们将特征贴图的像素中的值称为特征点。该特征点的维度为RC，其中C表示该块的输出特征维度尺寸。

我们采用了一个非常简单的设计，它将每个特征点通过一个完全连接的层来预测类别。当softmax后预测结果的最高概率大于某一值时，将特征点作为一个有用的特征，保留用于后续融合。相反，特征点被认为对细粒度分类的帮助较小。

基于上述概念，我们设计了一个可以端到端训练的体系结构，如图4所示。区域由虚线是骨干模型，我们使用fi∈RC×H×W表示特征地图输出i块骨干网络，其中H代表特征地图的高度，W代表特征地图的宽度，C表示特征维度的大小。然后将该特征图输入一个弱监督选择器，每个特征点将由一个线性分类器进行分类。

![image-20220921102113399](D:\文献阅读\fine-grained\image\image-20220921102113399.png)

这一步之后的特征图表示为$f_i∈R^{C'×H×W}$，C‘等于目标类的数量。然后，通过softmax得到每个特征点的类预测概率，并在弱监督选择器中选择置信度得分较高的前几个特征点。

选择算法如算法1所示，所选特征通过融合模型进行融合，其体系结构如图4所示。

![image-20220921102506989](D:\文献阅读\fine-grained\image\image-20220921102506989.png)

为了完成特征融合，我们设计了两种不同的架构。第一个是通过一个完全连接的层来实现的。假设所选特征点的总数为n。在将特征映射输入到全连接层之前，首先将它们连接在一起，作为特征维度。因此，维数为$R^{N×C}$的特征图经过全连接层后，得到维数为$R^{C'}$的预测结果。

该体系结构可以将选定的局部特征重新组合为可以表示整个图像的全局特征。

第二种架构是通过图卷积来实现的，它将所有选择的特征点视为一个图结构，其中节点代表在不同的空间位置和尺度上的特征。将图输入图卷积网络，可以学习不同节点之间的关系。然后，通过池化层将特征点聚合为多个超级节点，最后对这些超级节点的特征进行平均，并使用线性分类器来完成预测。

这种方法的优点是，可以更有效地集成每个点的特征，而不会破坏主干模型输出的结果。因此，我们最终使用图卷积作为特征融合机制。

此外，为了使模型能够更有效地提取小区域的特征，我们在主干网络中加入了FPN，以有效地融合不同尺度的特征，从而获得更准确的识别结果。

## *B. The process of forward and backward propagation*

该体系结构的目标是执行精细细化的分类，因此除了图像级注释之外，它不使用任何人工标签作为训练目标。

第一个训练目标是使每个特征图的特征都具有分类的能力。

为了计算总体损失，我们首先对所有特征点的预测输出进行平均，如下图中的等式所示(1)，其中$f_{l，s}∈R^{C'}$表示特征图第l块位置s处的特征点，s表示该块的输出特征映射空间，大小为H×W。

然后通过交叉熵计算整个块的类损失，如等式所示(2).

![image-20220921111834583](D:\文献阅读\fine-grained\image\image-20220921111834583.png)

除了训练整体分类外，特征图的选定位置用$Mask∈R^{H×W}$标记。

这个Mask是一个二进制数据类型，其中1表示所选的区域，0表示被删除的区域。

因此，$S  \cdot (Mask)$表示具有较强识别能力的特征点，而$S \cdot(∼M ask)$表示下降的特征点。

公式(3)表示第l个块中所选区域的特征之和。

所选类别的损失函数定义为式(4)。

Eq(5)表示未被选择的特征（或可以说被选择作为背景）的总和。下降区域的扁平化损失函数为Ln，定义为等式(6).

![image-20220921112540423](D:\文献阅读\fine-grained\image\image-20220921112540423.png)

扁平输出的设计是为了表示这个区域对分类的帮助较小。事实上，这种方法就像是在目标检测框架中预测前景或背景的“分数”。本文以softmax的最高概率值作为该得分。

然后，将所选的特征$f_s∈R^{N×C}$输入到组合器中，生成混合尺度的预测结果，输出特征为$fcomb∈R^{N'×C}$，其中N'为合并输入节点后的超级节点数。最后，将这些超级节点上的特征平均，输入线性分类器输出预测。通过交叉熵计算组合器类别预测损失，该损失函数用Lc表示。

整个损失函数定义为式(7)。该损失函数为上述损失函数的加权之和，其中λb、λs、λn、λc分别为Lb、Ls、Ln和Lc的权重：

事实上，在训练短语中，我们设置了Lb=1，Ls=0，Ln=5，Ls=1，而不使用所选的类别损失函数Ls作为训练目标，因为预测损失Lc通过组合器类别已经具有相同的功能。

总体训练目标是定位一个具有较强辨别能力的局部区域，并通过该区域的特征来提高识别结果。这个目标与之前的WSOD和FGVC框架的功能非常相似，除了找到具有强识别能力的局部位置（或对象的位置）大多是通过特征图的响应来完成的，通常被称为热图。这种方法的缺点是需要许多算法来找到热图，而许多体系结构需要一个两阶段的方法来修改模型。

在本文中，我们提出了一种简单易实现的方法，主要使用局部特征来预测类。这使得很难“区分”局部背景区域或不同类别的局部相似部分的预测值。在这种情况下，一个强选择准则-最大预测概率。最后，将所选择的局部特征融合为全局特征，完成最终的预测。该方法可以方便地应用于各种主流骨干网络，并且只需要一个阶段的端到端培训

对比学习被广泛应用于FGVC任务中。例如，TransFG [17]通过对特征的对比学习来学习更好的特征。API-Net [15]使用排名损失来学习一对图像特征之间的差异。CCFR [40]使用三联体损失来学习辨别区域。对比学习似乎在FGVC模型中发挥了重要作用。然而，通过对比学习训练模型通常需要额外的参数，如边缘距离或温度等。因为我们希望保持训练尽可能的简单，所以在这个设计中不使用对比学习，只使用交叉熵。