# 哈希算法

## 简介

面向数据的哈希通常使用两种类型的方法，数据独立哈希和数据依赖哈希。如果哈希函数集的定义独立于待哈希的数据，而不涉及来自数据的训练过程，那么我们将这种方法称为数据独立的哈希。否则，它们将被归类为依赖于数据的散列

------

![image-20220307171202477](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220307171202477.png)

### 定义

*Hashing function*

*Nearest neighbor (NN)*：*X* = [*x*^1^*,* *x*^2^,...x^n^] ∈ R*^D^* NN表示X中最接近查询点x~q~的一个或多个数据项

*Approximate nearest neighbor (ANN)*：ANN找到一个数据点x~a~∈X，它是一个查询点x~q~的ε近似最近邻，对于所有的x~a~∈X，x~a~和x之间的距离满足d(x~a~，x)≤（1+ε）d(x~q~，x)。

## 面向数据哈希

面向数据的哈希指的是打算使用哈希来加快数据检索或比较的方法，其中通常为查询维护哈希表。

### 数据独立哈希

哈希函数是独立于要处理的数据而不涉及数据的训练过程，我们将这种哈希技术称为数据无关的哈希。哈希函数是独立于要处理的数据而不涉及数据的训练过程，我们将这种哈希技术称为数据无关的哈希

与数据无关的哈希函数可以根据底层投影模式分为四类：随机投影、局部敏感投影、哈希学习和结构化投影。

![image-20220307172605739](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220307172605739.png)

![image-20220307172644413](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220307172644413.png)

#### 随机哈希

随机投影哈希首先被提出[Donald1999]，使用随机函数h：U→V在域V(对应于k维数据)中生成一个随机哈希值h(x)，并与原始域U中的数据项(对应于d维数据)相关联

{*x* → ((*ax* + *b*) mod *p*) mod *v* | 0 *<* *a* *<* *p,* 0 ≤ *b* *<* *p*}

整个族由参数p和v定义，一个特定的哈希函数由参数a和b定义。在这种通用哈希中，U中的每一组n个元素被均匀地投影到随机和独立的值上，并且相应的族F是n个独立的。

缺点:高度不稳定性。不同的随机哈希函数可能会导致完全不同的哈希值。纯基于随机投影的哈希固有地抛弃了原始特征空间的特征

#### 局部敏感哈希(LSH)

1. 在高维空间中原本距离相近的点投影到低维空间时，仍然会保持相近(可用or手段放大概率)
2. 在高维空间中原本距离相远的点投影到低维空间时，大概率距离仍然会远，但有小概率会变近(可用and手段减少误判概率)

**LSH**: 给定一个包含n个数据点/项目 *X* = [*x*^1^*,* *x*^2^*,...* *x^n^*] ∈ R^D^, 哈希函数 *H*(·) 包含 *K* 个哈希函数, LSH 映射一个数据x^i^ 到一个kbit的哈希码t∈ {0,1}

*H* (*x~i~*) = [h~1~ (*x~i~*) *,* h~2~ (*x~i~*) *,...,*h*~k~* (*x~i~*) ]

LSH的一个重要特征是，在原始特征空间中，距离较小内的两个数据点更有可能具有相似的哈希码。换句话说，在汉明空间中，LSH在很大程度上保留了原始的局部性信息。

这种保持局部的特性可以用两个数据点x和y之间的方程来阐述：

*P*{*H*(*x*) = *H*(*y*)} = *sim*(*x,* *y*)

其中sim(x，y)是相似度度量，可以用一个距离函数表示，例如p范数距离（p∈（0、2]），Mahalanobis distance, angular similarity，kernel similarity

$H(x) = sign(w^Tx + b)$

LSH 满足 
$$
H(x)
\begin{cases}
0, &if\ w^T\ x\ <\ b\\
1, &otherwise
\end{cases}
$$
两点x.y∈ R^D^ LSH 满足 $p\{h_m(x)=h_m(y)=1-\frac {1}{\pi}cos^{-1}(x^Ty)\}$

****

##### 算法思想

定义一系列Hash函数h1,h2,…,hn，随机选取其中的k个函数组成函数g(x)，不妨设选的是h1到hk，则g(x)=（h1(x),h2(x),…,hk(x)），像这样选取L个的g函数：g1(x),g2(x),…,gl(x)，每个函数对应一个哈希表。对于原始空间中的每一个点p，通过每个g函数分别映射到L个哈希桶中。这样，每个点都会在L个哈希表的某个哈希桶中出现。查询时，给定了查询子q，利用L个g函数同样对q进行映射，将与q落在同一个哈希桶中的点作为候选结果集，计算q与候选结果集中的点之间的距离，并从中选出1个或K个距离最近的点。

简单的讲，就是讲原始的点集按距离分成不同的类，查询与q距离较近的点集时，只需比较和q在同一类中的点集，而不需要比较全部点集。从而节省了搜索时间。

------

#####  缺点

LSH不仅保留了哈希空间中的数据特征，而且保证了相似数据点之间的碰撞概率。尽管LSH有明显的优势，但它仍然有一个不可避免的缺点，即哈希代码的效率低下。需要很长比特的哈希码或者非常多的哈希表才能达到预期的性能。

****

#### Learning for Hashing

哈希学习表示一组数据敏感的哈希方法，针对给定的数据学习一个新的哈希空间

##### 1.Lipschitz embedding

低失真地嵌入度量空间到其他空间,原始空间X的一个对象x转换到n维向量V=(v^1^,v^2^...v^n^)，每个元素v^i^对应原始空间元素o和预定义的引用集合之间的距离。$P_o(x)=D_x(x,o)$表示原始空间中一维的Euclidean embedding P~o~。o叫做引用对象，如果Dx服从三角形不等式，则X中的附近点被Po直观地映射到实线R上的附近点。即使Dx违反了三角形不等式，X中的附近点仍然可以被Po映射到R中的附近点。如果Dx违背三角不等式(三角形任意两边和大于第三边)，原始空间中近邻的点就直观地通过P~o~映射到实线R上的近邻点，另一方面原始空间中离的远的点也可能映射到相近的点。

##### 2.FastMap

其基本思想是选择两个数据对象x~1~，x~2~∈x作为枢轴心对象，然后定义任意x的嵌入P^x1，x2^为x在x~1~和x~2~之间的线上的投影。FastMap分别根据x和x~1~之间以及x和x~2~之间的距离来定义投影

![image-20220307235921182](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220307235921182.png)

P^x1、x2^将在原始空间中相互接近的数据对象投影到附近的位置，并保留了数据对象的接近性结构。多对枢轴对象用于投射有限的数据对象集。

![image-20220308000327640](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308000327640.png)

##### 3. BoostMap

BoostMap为底层数据指定的每个任务学习一个新的嵌入空间，这样新的哈希空间就可以最优地显示数据的相似性。它是一种有效的近似最近邻方法，可用于任意距离度量、度量或非度量。

BoostMap[2006]优化了定量度量，并比使用随机选择和启发式保持了更好的相似性排名。BoostMap的学习是独立的，不需要原始的距离测度，如欧几里得或度量性质

将embedding作为分类器，以估计任意三个数据对象的距离，并使用AdaBoost[夏皮尔和Singer1999]将之前所有的低维嵌入合并到一个高维嵌入中，以获得更高精度的相似度排序。

##### 4. Structured Projection



### 数据依赖哈希

依赖于数据的哈希函基于给定的训练数据集学习哈希函数，这样哈希函数就可以为所有数据记录找到最佳的紧凑代码。因为依赖于数据的哈希结果对底层数据高度敏感，因此它们伴随着更快的查询时间和更少的内存消耗。为了更好地保存局部信息和实现更好的选择性，依赖数据的哈希需要通过唯一地定义给定训练数据集的哈希函数族来紧密拟合特征空间中的数据分布。此外，与数据相关的哈希通常考虑与训练数据中的特征的相似性进行比较。

根据训练数据中标签信息的可用性，数据依赖哈希方法被分为三大类，无监督哈希、半监督哈希和监督哈希。

#### 无监督哈希

对于无监督哈希，不提供标签信息，包括弱标签，如实例之间的成对标签。无监督哈希方法使用未标记的数据为给定的训练数据生成二进制代码，并试图在原始特征空间中保留相似性信息

根据用于哈希的函数的实际形式，包括特征函数、线性函数和非线性函数，将无监督哈希方法分为三种类型：频谱哈希、线性哈希和非线性哈希

![image-20220308101214873](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308101214873.png)

##### 1. Spectral Hashing

![image-20220308101455830](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308101455830.png)

其中最左边的列表表示一组数据点。通过使用哈希函数H(x)，可以学习每个数据点对应的压缩代码。然后，通过在哈希表中使用逆查找来获得NN列表。因此，谱哈希不仅在简化的汉明空间中保持了样本的相似性，而且还在寻找那些相似数据点之间的平均汉明距离最小的样本相似性

[谱哈希]: http://blog.sina.com.cn/s/blog_67914f290101d2xp.html

因为在谱哈系松弛过程中有数据点符合均匀分布的假设，因此这种方法不适用于维度通常很高并且数据矩阵特别稀疏的文本查询。普通文本集都具有数万至数百万级别的维度，并且数据矩阵极为稀疏，这种数据集应用谱哈系所得结果会很差，而如果要得到较好的结果，只能对其进行预降维，这样会损失很多信息，也不会得到特别好的结果。因此，谱哈系通常更适用于**图像检索**.

##### 2. Linear unsupervised Hashing

###### (1)AGH(Anchor Graph Hashing)

[论文链接](https://www.researchgate.net/profile/Jun-Wang-79/publication/221345108_Hashing_with_Graphs/links/00b4952917968484e8000000/Hashing-with-Graphs.pdf)

为了使哈希在计算上可行，AGH利用锚定图来获得可处理的、非负的、稀疏的和低秩的亲和矩阵。锚点可以看作是K-means聚类中心，当锚点的数量足够大时，锚点图可以近似出精确的邻域图。

AGH步骤如下：
第一步：对图像训练数据集进行聚类，得到 m 个聚类中心，每个聚类中心称为一个锚点；
第二步：建立锚点与图像训练数据中每个样本点之间的关系，称为锚点图，用矩阵 Z 表示，矩阵中每个元素使用论文中公式(2)计算；
第三步：根据 A=ZX^-1^Z^^T^ 构造近似邻接矩阵 A，最后使用论文中公式(5)(6)得到最终的哈希码。

![image-20220308112103986](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308112103986.png)

##### 3. Nolinear Unsupervised Hashing

非线性无监督哈希不使用线性哈希函数，而是使用非线性函数，通常是一些核函数

###### KLSH(Kernelized LSH)

![image-20220308113005729](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308113005729.png)

KLSH的主要思想是基于中心极限定理在核空间中构造一个随机超平面哈希函数。根据中心极限定理，在非常温和的条件下，随着数据对象数量的增加，来自某些潜在分布的一组数据对象的平均值将在很大程度上遵循高斯分布。根据这个中心极限定理，将通过使用数据库中的数据项来计算一个近似的随机向量。一旦构造了随机超平面哈希函数，KLSH就用的Charikar方法计算一小组近似最近邻的候选对象。之后，KLSH对它们进行排序，通过核函数生成一个散列最近邻列表。因此，可以使用标准的LSH技术在次线性时间内检索到查询的最近邻

KLSH的一个显著优势是没有对输入和数据分布的假设。因此，它使得KLSH非常适合于图像搜索和其他底层数据分布未知的领域。

****

因为无监督的哈希方法不需要任何有标记的数据，所以给定一个预先指定的距离度量，这些方法的参数通常很容易学习。然而，对于某些领域，特别是与视觉相关的问题，数据点之间的相似度（或距离）可能不容易使用一个简单的度量来定义。同时，从一个数据集（或一个域）学习到的度量相似性可能不能很好地用于另一个数据集，也不能保持语义相似性。在这种情况下，包括弱标记数据的标签信息，如成对实例标签，对于哈希很有用。

****

#### 半监督哈希

![image-20220308113337560](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308113337560.png)

##### Linear Semisupervised Hashing

****

一方面，无监督哈希方法不能有效地捕获图像搜索中的语义相似性，因为没有提供标记数据来帮助哈希函数学习语义相似性。虽然监督哈希方法可以利用标记信息来学习语义相似性，但当标记数据的数量非常少或标签有噪声时，它们往往会过拟合。

另一方面，监督方法的训练效率效率较低。基于这些问题，我们提出了SSH方法来处理已标记数据和未标记数据上的度量和语义相似性。

****

###### SSH

SSH方法使用简单的线性映射来处理度量和语义相似性和不同的数据。这种SSH方法主要用于大规模的图像搜索

SH的目标是将n个数据点X=[x~1~，x~2~，...，x~n~]∈R^D^映射到汉明空间，并寻求由y∈{1，−1}^n∗K^给出的K位汉明embedding。给定一个向量w~k~∈R^D^和w=[w~1~，...，w~k~，…，w~K~]∈R^D∗K^，第k个哈希函数定义为$h_k(x_i)=sign(w_k^Tx_i)$

SSH允许使用两类标签信息。假设M表示邻对，C表示非邻对，H=[h~1~，...，h~k~，…，h~K~]是K个哈希函数的序列。SSH定义了目标函数 J(·) 来在标记数据上测量H的经验准确性
$$
J(H)=\sum_{k}\{\sum_{(x_i,x_j)\epsilon M}h_k(x_i)h_k(x_j)-\sum_{(x_i,x_j)\epsilon C}h_k(x_i)h_k(x_j)\}
$$
SH定义了一个矩阵S∈R^L∗L^来合并成对标记信息X~l~并表达J(H)：
$$
S_{i,j}=
\begin{cases}
1:(x_i,x_j) \epsilon M \\
-1:(x_i,x_j) \epsilon C \\
0:otherwise
\end{cases}
$$
假设H(X~l~)∈R^K∗L^将X~l~中的数据点映射到K位码；SSH将J(H)表示为
$$
J(H)=\frac1 2tr\{H(X_l)\ S\ H(X_l)^T\}\Rightarrow J(H)=\frac1 2tr\{sign(W^TX_l)\ S\ sign(W^TX_l)^T\}
$$
SSH的目的是对于(x~i~，x~j~)∈M的学习相同位，而对于(x~i~，x~j~)∈C的学习不同位：
$$
arg\,\max_{W} \frac 12tr(W^T\ M\ W)\\
M=X_l\ S\ X_l^T + \eta XX^T\\
X_lSX_l^T表示有监督项，ηXX^T表示无监督项(η是一个正标量，它相对加权基于方差的正则化项)
$$
M是一个调整后的协方差矩阵。在M中，监督项的目的是大大减少标记数据上的经验误差，而无监督项试图最大化单个位的方差和独立性，以提供有效的正则化。此外，通过放宽正交约束，同时采用正交解和非正交解，在不增加计算代价的情况下生成更好的哈希码。

给定一个具有几个成对标记数据点的大规模未标记数据集，SSH的最终目标是学习具有紧凑存储和快速检索的与数据相关的哈希函数，以及更好的数据点分区。

<font color=red size=3>**缺点**</font>:虽然SSH是有效的，并且实现了一个非常简单的基于特征分解的解决方案，但当从一组训练数据中学习一个紧凑的二进制代码时，它不考虑短二进制代码之间的可分性。

***

###### 其他方法

SSDH：学习了基于线性判别分析的半监督哈希的判别二进制码，并提出了半监督判别哈希(SSDH)。SSDH使用标记数据来最大限度地提高不同类中的二进制代码之间的可分性，而未标记数据被用于正则化。实验和验证表明，对于较短的二进制码，SSDH的性能确实优于SSH。

STPH：对于上述方法，哈希函数的学习**不考虑排序信息**，因此其哈希码固有地忽略了排序信息，这对于相似度搜索非常重要，特别是对于k个最近邻kNN搜索。此外，一些保持距离的哈希方法也**不能很好地保持数据的拓扑结构**。Zhang等人[2014a]提出了一种半监督拓扑保持哈希(STPH)方法，通过将邻域排序信息与哈希函数学习相结合，来解决上述两个问题。此外，STPH还在训练数据中利用了语义标签，因此其哈希结果可以准确地搜索语义邻居

##### Nonlinear Semisupervised Hashing

****

对于线性半监督哈希，现有的方法主要依靠线性特征表示。在现实中，基于内核的特征表示在测量数据项之间的相似性方面可能更有效，特别是对于视觉对象。

***

LAMP: Mu等人[2010]在核空间中开发了弱监督标签正则化最大边际划分(LAMP)算法，以支持基于核的特征表示。LAMP是专门为内核空间设计的，可以通过内核技巧和弱监督生成高质量的哈希函数。LAMP中的随机抽样策略使该方法可用于大规模数据集。LAMP的主要动机和想法如图9(b)所示

![image-20220308162125944](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308162125944.png)

（1）边信息或标签信息可以为更合理的哈希结果提供有用的指导

（2）和（3）显示了两个不同的哈希函数，它们在相同的分布上产生不同的边际γ

更大的边际γ可能意味着更低的相似搜索错误率，并可导致更好的泛化能力

如果使用基于内核的特征表示，那么度量相似性将更自然和更有用。与KLSH类似，LAMP也可以应用于任何图像数据库，因为它不假设输入数据中的数据分布。

****

Bootstrap-NSPLH：

半监督哈希的Bootstrap映射

对于SSH，一个限制是，由于线性投影伴随着平均阈值化，数据点之间的潜在关系可能不能被有效地反映出来。同时，对于高维数据点，SSH往往需要较高的计算成本

为了解决这一限制，提出了一种半监督非线性哈希(boot-NSPLH)[Wu等人，2013]，使用学习Bootstrap映射。在Bootstrap-NSPLH中，使用了一个非线性哈希函数来反映可用于半监督哈希的数据点之间的底层链接。此外，bootstrap-NSPLH可以通过整体考虑之前所学习到的bit来纠正哈希过程中积累的错误。在Bootstrap-NSPLH中，与线性哈希函数相比，计算矩阵中的维数要小得多，并且与原始数据空间中的维数无关。

____

#### 监督哈希

![image-20220308163648929](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308163648929.png)

在监督哈希中，标记数据，如每个图像的标签或指定相似/不同数据项对的成对约束，可以帮助学习哈希模型。监督哈希的目标是除了利用数据的特征值外，还利用基于标签的相似度或语义相似度来训练有效的哈希函数。

##### Linear supervised hashing

为了节省数据存储空间，在hashing中经常采用二进制缩减技术

![image-20220308164023991](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308164023991.png)

演示了使用二进制缩减可以显著减少存储。在图中，gist被定义为场景的抽象表示，自发地激活场景类别的记忆表示[Oliva和托拉尔巴2001]。原始图像有2,097,152位。通过使用gist向量，图像被转换为一个具有256个值的gist向量（即仅使用16位表示）。

****

**Boosting Similarity Sensitive Coding (BoostSSC)**

使用一个加权的汉明距离来计算图像之间的距离，并学习将原始输入空间嵌入到一个新的空间中，如图10(b)所示

![image-20220308164852549](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308164852549.png)

在BoostSSC开始时，对于图像的gist向量中的每个值(图10(b)中的红色单元格)的权重都是一致的)。对于二进制缩减(哈希函数H(X))中的每个位，BoostSSC选择在整个训练集中导致加权误差最小的索引，然后更新下一个计算的权值。最后，二进制缩减中的每个位都有一个相应的索引和一个权重阈值(在图10(b)中最右边的虚线框中)

****

**Binary Reconstructive Embedding (BRE)**

通过最小化输入距离和重构的汉明距离之间的误差的平方损失。BRE是一种监督算法用于学习哈希函数，用于快速和精确的最近邻搜索。

与数据相关的相关位的hash定义如下:

![image-20220308165713973](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308165713973.png)

{x~pq~,q = 1*,...,* *s*} is the training data for learning h~p~,    K（·）是一个核函数，W是一个权值矩阵。

哈希函数的目标是通过最小化原始距离和重构汉明距离之间的重构误差，在映射到汉明空间时显式地保留输入距离。

原欧氏距离d~M~和重建的汉明距离d~R~定义为

![image-20220308170037437](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308170037437.png)

其目标是通过最小化以下重构误差来推导出最优的W：

![image-20220308170110770](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308170110770.png)

样本对的集合N可以根据应用程序选择训练数据。

由于sign（.）的不可微分性，很难优化之前的目标函数。因此，BRE通过应用坐标下降算法迭代地将哈希函数更新到局部最优

通过为每个相同标签对设置零距离，为每个不同标签对设置足够大的距离，更容易将BRE扩展到监督场景。

**缺点**:对于大规模数据集，BRE的训练是低效的，而且由于存储成本昂贵，BRE在大规模数据集上进行训练几乎是不切实际的。

****

**Minimal Loss Hashing (MLH)**

Norouzi和Blei[2011]提出了基于潜在结构SVM框架的最小损失哈希(MLH)，该框架下的一般损失函数适合使用欧几里得距离或标记数据点集进行训练。

****

**Latent Factor Hashing LFH**

为了进一步提高训练效率，Zhang等人[2014b]提出了Latent Factor Hashing(LFH)来学习保持相似性的二进制码。为了在大规模数据集上训练LFH，作者采用了随机线性时变量学习方法。同时，对于大规模数据集，存在检索和匹配问题。为了存储和检索描述符数据，

****

**Linear Discriminant Analysis-based Hashing (LDAHash)**

Strecha等人[2012]提出了基于线性判别分析的散列(LDAHash)来将描述符向量映射到汉明空间，并通过将描述符表示为短的二进制字符串来减少描述符的大小。

****

****

##### NoLinear Supervised Hashing



**Kernel-based Supervised Hashing (KSH)**

当将数据映射到紧凑的二进制码时，该模型使相似数据对之间的汉明距离最小化，并使不同数据对之间的汉明距离最大化。在优化代码内积和调整汉明距离的同时，为了获得短的和有区别的code，利用等价性的方法，一次按1位的顺序训练哈希函数

![image-20220308171046720](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220308171046720.png)

被标记的数据分别有相似的对和不同的对。每个数据点都有一个相应的哈希码。KSH首先通过尝试最小化相似对之间的汉明距离和最大化不相似对之间的汉明距离（参见称为“汉明距离优化”的虚线框）来优化汉明距离，然后将这些哈希码作为一个矩阵组合。通过使用矩阵乘法（参见名为“代码内积优化”的虚线框），KSH通过名为“哈希代码矩阵”的虚线框中的最终哈希代码矩阵进一步调整汉明距离。

在实际应用中，KSH的优化过程被耦合到一个指定的哈希函数上，这限制了优化的应用范围。

****

**Two-Step Hashing (TSH) **

建立在哈希函数学习过程和hash码生成可能被视为分离的步骤，前者可以通过训练标准二进制分类器。因此，哈希学习被分为两个阶段：学习哈希位和借助于学习到的bit的学习哈希函数。

****

**FastHash**

两步哈希方法也被应用于具有高维特征的大规模数据集，其中内核哈希函数的训练和测试成本非常昂贵。为了解决这一问题，FastHash[Linetal.2014]首先使用决策树作为非线性哈希函数来处理大规模的高维训练和测试数据。进一步应用了两步学习策略，将二进制代码推理与简单的二进制分类训练相结合，形成了学习过程。

****

#### 总结

**由于具有强大的泛化能力，非线性哈希函数在整体性能方面通常优于线性哈希函数。**

**综上所述，与无监督哈希方法相比，有监督哈希方法的主要优点是对不同的实际应用程序的灵活性和适应性。然而，训练效率仍然是一个很大的挑战。**

l
