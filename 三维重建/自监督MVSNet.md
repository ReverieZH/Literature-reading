# 摘要

提出了一种基于学习的多视图立体视觉(MVS)的方法。

虽然目前的深度MVS方法取得了令人印象深刻的结果，但它们关键地依赖于地面真实的3D训练数据，而获取这种精确的3D几何图形用于监督是一个主要障碍。

相反，我们的框架利用多个视图之间的光度一致性作为监督信号，在一个较宽的基线MVS设置中学习深度预测。

然而，由于遮挡和照明变化天真地应用照片一致性约束是不可取的。

为了克服这一问题，我们提出了一个稳健的损失公式：

a)强制一阶一致性，b)对每个点，有选择性地强制与一些视图的一致性，从而隐式地处理遮挡。

我们展示了我们在不使用真实数据集进行3D监督的情况下学习MVS的能力，并表明我们提出的鲁棒损失的每个组成部分都导致了显著的改进。

我们定性地观察到，我们的重建往往比获得地面事实的方法更完整，进一步显示了这种方法的优点。

最后，我们经过学习的模型推广到新的设置，我们的方法允许通过无监督微调适应现有的cnn到没有ground-truth三维的数据集

# 一、引言

从图像中恢复场景密集的三维结构一直是计算机视觉的一个长期目标。多年来，有几种方法通过利用潜在的几何和光度约束来解决这个多视图立体视觉(MVS)任务——一个图像中的一个点沿着上极线投射到另一个图像上，并且正确的匹配在光度学上是一致的。虽然操作这一见解导致了显著的成功，这些纯粹的基于几何的方法对每个场景独立，并无法隐式地捕获和利用通用先验对世界, 例如表面往往是平的，因此当信号稀疏时会表现不佳,如无纹理表面。

为了克服这些限制，一项新兴的工作线集中于基于学习的MVS任务解决方案，通常训练cnn来提取和合并跨视图的信息。 

虽然这些方法产生了令人印象深刻的性能，但它们关键地依赖于地面真实的3D数据在学习阶段。我们认为，这种形式的监督过于繁重，不是自然可行的，因此，寻求不依赖这种3D监督的解决方案具有实用和科学意义。

我们建立在这些最近的基于学习的MVS方法的基础上，这些方法呈现了具有几何归纳偏差的CNN架构，但在用于训练这些CNN的监督形式上存在显著差异。

我们不依赖于地面真相的三维监督，而是提出了一个以无监督的方式学习多视图立体视觉的框架，仅依赖于多视图图像的训练数据集。

我们能够使用这种形式的监督的见解类似于经典方法中使用的见解——正确的几何形状将产生光度学上一致的重投影，因此我们可以通过最小化重投影误差来训练我们的CNN。

虽然类似的重投影损失已经被最近的方法成功地用于其他任务，如单眼深度估计，但我们注意到，天真地将它们应用于学习MVS是不够的。**这是因为不同的可用图像可能会捕获场景的不同可见方面**。因此，一个特定的点（像素）不需要在光度学上与所有其他视图一致，而是只需要那些它没有被遮挡的视图。然而，明确地推理遮挡来恢复几何，提出了一个鸡和蛋的问题，因为遮挡的估计依赖于几何，反之亦然。为了避免这一点，我们注意到，虽然正确的几何估计不需要意味着与所有视图的光度一致性，但它应该意味着至少与某些视图的一致性。

此外，在MVS设置中，跨视图的照明变化也很重要，因此只在像素空间中强制执行一致性是不可取的，而我们的洞察力是另外强制执行基于梯度的一致性。我们提出了一个健壮的重投影损失，使我们能够捕获这两个见解，并允许使用所需的监督形式学习MVS。

我们的简单，直观的公式允许处理遮挡，而没有明确地建模他们。我们的设置和示例输出如图1所示。我们的模型在没有三维监督的情况下进行训练，以一组图像作为输入，预测每幅图像的深度图，然后将其结合起来，得到一个密集的三维模型。

![image-20220621152424967](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621152424967.png)

总之，我们的主要贡献是：

- 一种以无监督的方式学习多视图立体视觉的框架，只使用来自新视图的图像作为监督信号。

- 一种用于学习无监督深度预测的鲁棒多视图光度一致性损失，允许隐式地克服跨训练视图之间的照明变化和遮挡。



# 二、相关工作



# 三、方法

MVS设置的目标是重建给定一组输入图像的场景的密集三维结构，其中这些视图的相关内在因素和外部因素是已知的——这些参数通常可以通过前面的运动结构(Sfm)步骤来估计。虽然MVS问题有几个公式集中于不同的3D表示[21,4,5]，但我们在这里关注的是基于深度的MVS设置。因此，我们推断出与每个输入相关联的每像素深度图，然后通过将这些深度图反向投影到一个组合的点云中，从而获得密集的3D场景。

我们利用一个基于学习的系统来预测深度图的步骤，学习一个将相邻视图的图像作为输入，并预测中心图像的每像素深度图的CNN。与以往基于学习的MVS方法也采用了类似的方法不同，我们只依赖于可用的多视图图像作为监控信号，而不需要一个地面真实的3D场景。为了利用这种监督，我们建立在经典方法的见解基础上，并注意到，当投影到其他视图上时，对一个点（图像像素）的精确几何预测应该会产生光度学上一致的预测。

我们实施了这一见解，并使用光度一致性损失来训练我们的深度预测CNN，惩罚原始和可用的新视图中像素强度之间的差异。然而，我们注意到光度一致性的假设并不总是正确的。相同的点并不一定在所有视图中都可见。此外，不同视图之间的照明变化会导致像素强度之间的进一步差异。为了考虑可能的光照变化，我们在光度损失中添加了一阶一致性项，因此也确保除了强度外梯度匹配。然后，我们通过提出一个稳健的光度损失来隐式地处理可能的遮挡，这强制执行一个点应该与某些点一致，但不一定与所有点一致。

## 3.1 **Network architecture**

我们提出的无监督学习框架与网络架构是不可知的。在这里，我们采用[36]中提出的模型作为具有代表性的网络架构，同时注意到[15,34]中也提出了类似的架构。该网络以N幅图像作为输入，使用CNN提取特征，创建一个基于cost volume的平面扫描，并为每个参考图像推断一个深度图。图2给出了该架构的草图。我们工作的重点是研究一种使用鲁棒光度损失以无监督的方式训练这样的cnn的方法，如下节所述。

![image-20220621153538016](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621153538016.png)

我们取一个场景的N个图像作为输入。图像特征是使用CNN生成的。利用可微同调法，通过在一定的深度值范围内扭曲图像特征来构造cost volume。然后使用3D U-Net风格的CNN来细化 cost volume。最终的输出是一个缩小分辨率的深度贴图。关于网络架构的详细信息可以在补充资料中找到

## **3.2. Learning via Photometric Consistency**

我们现在描述了如何在不需要地面真实深度图的情况下有效地训练我们的深度预测网络。其核心思想是使用基于扭曲的视图合成损失，这在立体和单眼深度预测任务[43,27]中相当有效，但尚未在非结构化的多视图场景中进行探索。