# 摘要

提出了一种基于学习的多视图立体视觉(MVS)的方法。

虽然目前的深度MVS方法取得了令人印象深刻的结果，但它们关键地依赖于地面真实的3D训练数据，而获取这种精确的3D几何图形用于监督是一个主要障碍。

相反，我们的框架利用多个视图之间的光度一致性作为监督信号，在一个较宽的基线MVS设置中学习深度预测。

然而，由于遮挡和照明变化天真地应用照片一致性约束是不可取的。

为了克服这一问题，我们提出了一个稳健的损失公式：

a)强制一阶一致性，b)对每个点，有选择性地强制与一些视图的一致性，从而隐式地处理遮挡。

我们展示了我们在不使用真实数据集进行3D监督的情况下学习MVS的能力，并表明我们提出的鲁棒损失的每个组成部分都导致了显著的改进。

我们定性地观察到，我们的重建往往比获得地面事实的方法更完整，进一步显示了这种方法的优点。

最后，我们经过学习的模型推广到新的设置，我们的方法允许通过无监督微调适应现有的cnn到没有ground-truth三维的数据集

# 一、引言

从图像中恢复场景密集的三维结构一直是计算机视觉的一个长期目标。多年来，有几种方法通过利用潜在的几何和光度约束来解决这个多视图立体视觉(MVS)任务——一个图像中的一个点沿着上极线投射到另一个图像上，并且正确的匹配在光度学上是一致的。虽然操作这一见解导致了显著的成功，这些纯粹的基于几何的方法对每个场景独立，并无法隐式地捕获和利用通用先验对世界, 例如表面往往是平的，因此当信号稀疏时会表现不佳,如无纹理表面。

为了克服这些限制，一项新兴的工作线集中于基于学习的MVS任务解决方案，通常训练cnn来提取和合并跨视图的信息。 

虽然这些方法产生了令人印象深刻的性能，但它们关键地依赖于地面真实的3D数据在学习阶段。我们认为，这种形式的监督过于繁重，不是自然可行的，因此，寻求不依赖这种3D监督的解决方案具有实用和科学意义。

我们建立在这些最近的基于学习的MVS方法的基础上，这些方法呈现了具有几何归纳偏差的CNN架构，但在用于训练这些CNN的监督形式上存在显著差异。

我们不依赖于地面真相的三维监督，而是提出了一个以无监督的方式学习多视图立体视觉的框架，仅依赖于多视图图像的训练数据集。

我们能够使用这种形式的监督的见解类似于经典方法中使用的见解——正确的几何形状将产生光度学上一致的重投影，因此我们可以通过最小化重投影误差来训练我们的CNN。

虽然类似的重投影损失已经被最近的方法成功地用于其他任务，如单眼深度估计，但我们注意到，天真地将它们应用于学习MVS是不够的。**这是因为不同的可用图像可能会捕获场景的不同可见方面**。因此，一个特定的点（像素）不需要在光度学上与所有其他视图一致，而是只需要那些它没有被遮挡的视图。然而，明确地推理遮挡来恢复几何，提出了一个鸡和蛋的问题，因为遮挡的估计依赖于几何，反之亦然。为了避免这一点，我们注意到，虽然正确的几何估计不需要意味着与所有视图的光度一致性，但它应该意味着至少与某些视图的一致性。

此外，在MVS设置中，跨视图的照明变化也很重要，因此只在像素空间中强制执行一致性是不可取的，而我们的洞察力是另外强制执行基于梯度的一致性。我们提出了一个健壮的重投影损失，使我们能够捕获这两个见解，并允许使用所需的监督形式学习MVS。

我们的简单，直观的公式允许处理遮挡，而没有明确地建模他们。我们的设置和示例输出如图1所示。我们的模型在没有三维监督的情况下进行训练，以一组图像作为输入，预测每幅图像的深度图，然后将其结合起来，得到一个密集的三维模型。

![image-20220621152424967](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621152424967.png)

总之，我们的主要贡献是：

- 一种以无监督的方式学习多视图立体视觉的框架，只使用来自新视图的图像作为监督信号。

- 一种用于学习无监督深度预测的鲁棒多视图光度一致性损失，允许隐式地克服跨训练视图之间的照明变化和遮挡。



# 二、相关工作



# 三、方法

MVS设置的目标是重建给定一组输入图像的场景的密集三维结构，其中这些视图的相关内在因素和外部因素是已知的——这些参数通常可以通过前面的运动结构(Sfm)步骤来估计。虽然MVS问题有几个公式集中于不同的3D表示[21,4,5]，但我们在这里关注的是基于深度的MVS设置。因此，我们推断出与每个输入相关联的每像素深度图，然后通过将这些深度图反向投影到一个组合的点云中，从而获得密集的3D场景。

我们利用一个基于学习的系统来预测深度图的步骤，学习一个将相邻视图的图像作为输入，并预测中心图像的每像素深度图的CNN。与以往基于学习的MVS方法也采用了类似的方法不同，我们只依赖于可用的多视图图像作为监控信号，而不需要一个地面真实的3D场景。为了利用这种监督，我们建立在经典方法的见解基础上，并注意到，当投影到其他视图上时，对一个点（图像像素）的精确几何预测应该会产生光度学上一致的预测。

我们实施了这一见解，并使用光度一致性损失来训练我们的深度预测CNN，惩罚原始和可用的新视图中像素强度之间的差异。然而，我们注意到光度一致性的假设并不总是正确的。相同的点并不一定在所有视图中都可见。此外，不同视图之间的照明变化会导致像素强度之间的进一步差异。为了考虑可能的光照变化，我们在光度损失中添加了一阶一致性项，因此也确保除了强度外梯度匹配。然后，我们通过提出一个稳健的光度损失来隐式地处理可能的遮挡，这强制执行一个点应该与某些点一致，但不一定与所有点一致。

## 3.1 **Network architecture**

我们提出的无监督学习框架与网络架构是不可知的。在这里，我们采用[36]中提出的模型作为具有代表性的网络架构，同时注意到[15,34]中也提出了类似的架构。该网络以N幅图像作为输入，使用CNN提取特征，创建一个基于cost volume的平面扫描，并为每个参考图像推断一个深度图。图2给出了该架构的草图。我们工作的重点是研究一种使用鲁棒光度损失以无监督的方式训练这样的cnn的方法，如下节所述。

![image-20220621153538016](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621153538016.png)

我们取一个场景的N个图像作为输入。图像特征是使用CNN生成的。利用可微同调法，通过在一定的深度值范围内扭曲图像特征来构造cost volume。然后使用3D U-Net风格的CNN来细化 cost volume。最终的输出是一个缩小分辨率的深度贴图。关于网络架构的详细信息可以在补充资料中找到

## **3.2. Learning via Photometric Consistency(光度一致性)**

我们现在描述了如何在不需要地面真实深度图的情况下有效地训练我们的深度预测网络。

其核心思想是使用基于扭曲的视图合成损失，这在立体和单眼深度预测任务[43,27]中相当有效，但尚未在非结构化的多视图场景中进行探索。

给定一个输入图像$I_s$和额外的相邻视图，我们的CNN输出一个深度图$D_s$。在训练期间，我们还可以获得同一场景{$I_v^m$}的其他新视图，并使用这些视图来监督预测的深度$D_s$。

对于具有相关的内在/相对外在(K，T)参数的特定一对视图($I_s$，$I_v^m$)，预测的深度图$D_s$允许我们使用空间tramsformer网络[16]“逆扭曲”新视图到源帧，然后通过可微双线性采样产生$Iˆ^i_s$。

对于源图像Is中的一个像素u，我们可以得到它在新视图中的坐标：

![image-20220621190355820](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621190355820.png)

然后，通过对扭曲坐标周围的新视图图像进行双线性采样，可以获得扭曲图像：

![image-20220621190415988](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621190415988.png)

在扭曲的图像旁边，还生成了一个二值有效性掩码$V_s^m$，表示合成视图中的“有效”像素，因为一些像素投射到新视图中的图像边界之外。正如之前在学习单眼深度估计[43]中所做的那样，我们可以制定一个照片一致性目标，指定扭曲的图像应该与源图像匹配。在我们的多视图系统场景中，这可以天真地扩展到所有M新视图的参考视图，损失是：

![image-20220621190833840](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621190833840.png)

这种损失使我们能够学习一个没有地面真相3D的深度预测CNN，但是这个公式有几个问题，例如无法解释遮挡和照明的变化。虽然类似的重投影损失已经成功地用于KITTI[8]等单眼或立体重建，但在这些数据集中，不同视图的遮挡和光线变化最小。然而，在MVS数据集中，自遮挡、反射和阴影是一个更大的问题。因此，我们扩展了这种光度损失，并提出了一个更稳健的公式，适合我们的设置。

## 3.3  **Robust Photometric Consistency for MVS**

我们提出的稳健的光度损失公式是基于两个简单的观察结果——图像梯度比亮度变化更不受影响，并且一个点只需要与某些点在光度上一致(并不是所有)新颖的观点。

事实上，我们所做的第一个修改利用了多年来MVS研究[6]的研究成果，其中许多传统方法发现，基于绝对图像强度和图像梯度差异的匹配成本比仅基于前者的匹配效果要好得多。我们还发现，由于图像之间的像素强度变化很大，对绝对图像差项进行绝对损失是很重要的。因此，基于eqn3的 inverse-warping的光度损失被修改以反映这一点

![image-20220621191528137](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621191528137.png)

我们将此称为一阶一致性损失。

接下来，我们将解决在不同图像中遮挡三维结构所引起的问题。上面讨论的Loss公式要求源图像中的每个像素应该在光度学上与所有其他视图一致。如图3所示，这是不可取的，因为一个特定的点由于遮挡，可能只能在新视图的一个子集中可见

![image-20220621193051887](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621193051887.png)

我们的关键见解是只使用topk(M)视图来加强每个像素的摄影一致性。设$L^m(u)$表示一个特定像素u w.r.t的新视图的一阶一致性损失。我们最终的稳健光度损失可以表述为：

![image-20220621193812580](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621193812580.png)

上面的方程简单地说明，对于每个像素u，在像素投影有效的视图中，我们使用最佳的K个不相交视图计算损失。这一点如图4所示。为了实现这种鲁棒的光度损失，我们将M个新视图图像与参考图像进行逆扭曲，并计算每像素的一阶一致性“损失图”。所有M损失图然后堆叠成一个尺寸W×H×M的3D损失。对于每个像素，我们找到具有有效掩码的K个最小值条目，并将它们求和得到像素级一致性损失。

![image-20220621193937050](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621193937050.png)

用于训练的鲁棒像素级聚合损失的可视化。从网络中预测的深度图与参考图像一起用于扭曲和计算每个M个非参考相邻视图的损失图，如eqn4所示。然后这些M损失映射被连接成一个维度为H×W×M的体积，其中H和W是图像维度。这个体积用于执行像素级选择来选择K“最佳”（最低损失）值，沿着体积的第三维(即M损失图)，使用它我们取平均值来计算我们的鲁棒光度损失

## **3.4. Learning Setup and Implementation Details**

在训练过程中，我们的深度预测网络的输入包括一个源图像和N=2个附加视图。然而，我们使用更大的视图集(M=6，K=3)来加强光度一致性。这允许我们从更大的图像集中提取监控信号，而推断时只需要更小的推理集。

除了上述鲁棒光度损失外，我们还添加了[27]提出的结构相似性(LSSIM)和深度平滑性(LSmooth)目标。平滑性损失强制在预测的视差映射上具有与边缘相关的平滑性。SSIM损失在扭曲的图像上是一个更高阶的重建损失，但由于它是基于更大的图像斑块，我们不应用我们的像素级选择方法的鲁棒光度损失。相反，使用两个相邻视图中视图选择得分最高的两个相邻视图来计算SSIM损失。我们将在附录中更详细地描述了这个公式。

我们最终的端到端无监督学习目标是前面描述的损失的加权组合

![image-20220621194707778](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220621194707778.png)

在我们所有的实验中，我们都使用了α=0.8，β=0.2和γ=0.0067。该网络采用ADAM[23]优化器进行训练，学习速率为0.001，一阶矩衰减因子为0.95。我们使用 Tensorflflow[2]来实现我们的学习管道。正如[36]所指出的，网络的高GPU内存要求意味着在训练中使用更小的图像分辨率和更粗的深度步骤是有效的，而更高的设置可以用于评估。我们注意到在实验部分中使用的图像分辨率。

## **3.5. Inference using Learned Depth Prediction**

在测试时，我们取一组3D场景的图像，并通过我们的网络预测每个图像的深度图。这是通过网络传递一个参考图像和2个相邻的图像来实现的，这些图像是根据相机基线或一个视图选择评分来选择的。然后将这一组深度图像进行融合，形成点云。我们使用Fusibile[6]，一个开源的实用工具，来进行点云融合。

# 四、实验

我们现在描述对我们提出的模型的评估。评估的主要数据集是DTU MVS数据集[18]。在第4.1节中，我们描述了DTU数据集和我们的训练和评估设置，并定性和定量地讨论了我们的结果。接下来，我们对我们提出的鲁棒损失函数的各个成分的影响进行了严格的消融研究（第4.2节）。我们还在第4.3节中表明，我们的方法可以允许我们通过使用我们的鲁棒光度一致性损失来调整预训练的模型以适应数据集，而不使用地面真相。最后，我们演示了在不进行微调的情况下将我们的模型泛化到另一个数据集（第4.4节）。

## **4.1. Benchmarking on DTU**

DTU MVS数据集包含124个不同场景的扫描，具有3D结构和使用机器人臂捕获的高分辨率RGB图像。对于每个场景，有49张图像的相机姿态是高精度的。我们使用与SurfaceNet[19]和MVSNet[36]中使用的相同的序列-数值测试分割。与MVSNet一样，对于一个扫描的给定参考图像，使用视图选择评分[39]选择其邻近图像输入网络(N)，它使用稀疏点云和相机基线为给定的参考视图选择最合适的相邻视图。同样，我们在训练中使用相邻的M视图进行顶部k损失。

与MVSNet一样，对于一个扫描的给定参考图像，使用视图选择评分[39]选择其邻近图像输入网络(N)，它使用稀疏点云和相机基线为给定的参考视图选择最合适的相邻视图。同样，我们在训练中使用相邻的M视图进行top-k损失的自我监督



### **4.1.1 Training setup**

为了进行训练，我们将DTU图像缩放到640x512的分辨率。我们所有的网络都用N=3进行训练，这样在每次迭代中，有一个参考视图和2个新视图用于预测一个深度图。对于基于top-K聚合的鲁棒光度损失，我们使用M=6和K=3。因此，使用6个相邻的视图来计算光度损失体积，每个像素选择最好的3个。稍后我们将讨论变化k的影响。为了对测试集进行评估，在图像分辨率为640x512下生成深度图。网络中平面扫描卷生成的dmin和dmax分别设置为425mm和935mm

