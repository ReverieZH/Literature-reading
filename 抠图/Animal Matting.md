

提出了一种新的目视和聚焦匹配网络(GFM)，它使用一个共享的编码器和两个独立的解码器，以协作的方式学习这两个任务，用于端到端动物图像匹配。此外，我们通过对前景图像和背景图像之间的各种差异进行综合分析，系统地研究了复合图像和自然图像之间的域差距问题。我们发现，一个精心设计的成分路线RSSN，旨在减少差异，可以导致一个更好的模型具有显著的泛化能力

## 一、引言

对于抠像，假设图像I前景F和背景B的线性组合 通过一个alphaα∈[0,1]

![image-20220519190812255](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519190812255.png)

i表示像素索引

q1:<font color='red'>给出I由等式来估计F、B和α是一个典型的不适定问题 </font>。

a1:为了解决这个问题，以前的匹配方法采用了额外的用户输入，如trimap[1]和 scribbles [2]作为先验，以减少未知的程度。

____

q2: 基于采样相邻的已知像素[3]、[4]、[5]或定义亲和矩阵[6]，将已知的alpha值（即前景或背景）传播到未知像素。通常，使用一些边缘感知的平滑约束来使问题易于处。然而，<font color='red'>无论是采样还是计算亲和矩阵，都是基于低水平的颜色或结构特征</font>，这在模糊的过渡区域或细边缘没有很大的区别。他们的性能未知的区域的大小是敏感的，而且可能会遭受模糊的边界和颜色混合。

a2:为了解决这一问题，人们提出了基于深度卷积神经网络(CNN)的抠图方法，利用其较强的代表性能力和学习到的鉴别特征。虽然基于cnn的方法可以获得良好的匹配结果，但前提的trimap或scribbles使它们不太可能用于自动应用

____

q3:<font color='red'>对于动物图像来说，它可能需要很大的努力来表明关于它们毛茸茸的性质和被生活环境遮挡的过渡区域</font>

a3:为了解决这一问题，近年来提出了[7]、[8]、[9]、[10]、[11]等端到端抠图方法。分为两类

 （1）依次进行全局分割、[12]和局部匹配

​		前者的目标是trimap生成[7]、[11]或前景/背景生成[8]，而后者是基于trimap或从前一阶段生成的其他先验的图像匹配

![image-20220519193345769](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519193345769.png)

（2)在进行局部匹配时，提供全局信息作为指导

![image-20220519193557759](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519193557759.png)

对于动物抠图，由于以下两个原因，很难生成可靠的trimap和估计准确的 alpha matte，这将对上述两个管道提出挑战。

1.首先，在动物抠图任务中，有不同类别的形状、大小、颜色和外观，而在其他抠图任务中只有单一类别，如人类肖像抠图[7]、[10]、[11]。

2.动物的保护性颜色使它很难识别前景，并区分动物的皮毛与背景背景。因此，如何设计一个新的框架来学习鉴别特征，并区分语义前景/背景与过渡区域的细节仍然具有挑战性和探索不足。

另一个挑战是当前可用的匹配数据集的局限性。

![image-20220519194102495](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519194102495.png)

![image-20220519194137244](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519194137244.png)

然而，这些合成图像的背景图像是低分辨率的，可能包含显著的物体。

在本文中，我们指出，遵循上述路径的训练图像与自然图像存在显著的域差距，这是由于前景和背景图像的分辨率、锐度、噪声和照明方面的差异。这些伪影是区分前景和背景的廉价特征，在训练过程中会误导模型，导致对自然图像泛化性较差的过拟合模型

————————————————————————————————————————————————————————————

为了解决动物图像匹配中的上述问题，我们研究了语义和细节在动物图像匹配中的不同作用，并探讨了将任务分解为两个并行子任务，即语义分割和细节匹配的思想

![image-20220519194534734](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519194534734.png)

此外，我们还首次尝试建立一个大规模的动物抠图数据集。AM-2k包含来自20个类别的2000幅高分辨率的自然动物图像，以及手工精心标记的精细alpha mattes

![image-20220519194954918](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519194954918.png)

此外，我们系统地研究了领域差距，并对抠图中的合成管道进行了全面的实证分析。我们确定了导致领域差距的几种差异，并指出了它们可能的解决方案。然后，我们设计了一种新的组合路径名称RSSN，可以显著减少由于分辨率、锐度、噪声等差异而产生的域差距。我们提出了一个没有突出前景对象的大规模高分辨率干净背景数据集(BG-20k)，

![image-20220519194839916](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519194839916.png)

贡献：

（1）我们提出了一种名为GFM的端端自然动物图像匹配模型，该模型同时生成全局语义分割和局部alpha matte，没有任何先验作为输入。

（2）我们构建了第一个自然动物图像抠图数据集AM-2k，通过其大规模、不同的类别和高质量的注释，有利于训练具有良好泛化的更好的模型。

（3）我们设计了一种新的合成路径RSSN，以减少各种差异，并提出了一个大规模的高分辨率背景数据集BG-20k，以作为生成高质量合成图像的更好的候选对象。

（4）在AM-2k和BG-20k上的大量实验表明，GFM的性能优于最先进的垫接模型，可以成为未来研究的有力基线。此外，所提出的合成路线通过大幅度减少泛化误差，证明了其价值。



## 二、相关工作

**Image Matting**：

**Matting Dataset**：

Composition-1k：431 training images and 50 test images

HAttMatting：596 training images and 50 test images

**Image Composition**：

**Domain Adaptation** ：合成图像与真实图像之间的域差距降低了深度模型的泛化能力。为了解决这一问题，人们在领域适应领域中做出了许多努力。  合成图像与真实自然图像在图像抠图属性中的域差距是由于在特定的alpha定位过程中引入的前景和背景图像之间的不同类型的差异，如分辨率、清晰度、噪声等。  在本文中，我们系统地研究了导致域间隙的差异，并设计了一种新的组成路径，可以显著减少它。因此，我们成功地减少了不同的模型训练的图像的泛化误差。

## 三、**GFM: GLANCE AND** FOCUS **MATTING** **N**ET**WORK**   

在处理动物图像抠图问题时，我们人类首先看图像，快速识别突出的粗糙前景或背景区域，然后专注于过渡区域，从背景中区分细节。

它可以大致表述为一个粗略的分割阶段和一个抠图的阶段。请注意，这两个阶段可能交织在一起，从第二阶段将会有反馈来纠正第一阶段的错误决定，例如，在一些模糊的区域，由于动物的保护颜色或闭塞。为了模拟人类的经验，并使配对模型在这两个阶段都具有适当的能力，将它们集成到一个单一的模型中，并明确地建模协作是合理的。为此，我们提出了一种新的端到端自然动物图像匹配网络。 

![image-20220519203829944](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519203829944.png)

###  **Shared Encoder**

 如图2所示，GFM具有一个编码器-解码器结构，其中该编码器由两个后续的解码器共享。

该编码器将单个图像作为输入，并通过五个块E0∼E4进行处理，其中每个块的分辨率降低了一半。

我们采用在ImageNet训练集上预先训练过的ResNet-34[35]或DenseNet-121[36]作为我们的骨干编码器。

具体来说，对于DenseNet-121，我们添加了一个卷积层，将输出特征通道减少到512。    

### **Glance Decoder (GD)**                                                          

扫视解码器的目的是识别简单的语义部分，并将其他部分作为未知区域。为此，解码器应该有一个很大的接受域来学习高级语义

如图2所示，我们对称地堆叠5个块$D_4^G∼D_0^G$作为解码器，每个块由三个顺序的3×3卷积层和一个上采样层组成。为了进一步扩大接受域，我们在$E_4$之后添加了一个金字塔池模块(PPM)[37]，[38]来提取全局上下文，该模块通过元素级求和连接到每个解码器块$D_i^G$。我们在解码器输出后采用一个s型激活函数

_____

PPM:对原始特征图进行不同尺度的池化操作，得到多个不同尺寸的特征图（图中为 4 个）。对得到的特征图进行上采样操作，恢复至原始特征图大小 ，最后在通道维度上进行拼接，得到最终的复合特征图；

____________

**Loss Function**:

![image-20220519203545925](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220519203545925.png)

GD的输出是一个二通道或三通道(C=2或3)类的概率映射，它依赖于语义转换表示，

###  **Focus Decoder (FD)**

FD具有与GD相同的基本结构，即对称堆叠五个块$D_4^F∼D_0^F$。

与GD的粗略语义分割不同，FD的目标是提取低级结构特征非常有用的过渡区域的细节。

因此，我们使用桥接块(BB)[39]，而不是E4后的PPM来利用不同接受域中的局部上下文。

具体来说，它由三个扩张的卷积层组成。来自![E_4](https://math.jianshu.com/math?formula=E_4)和**BB**的特征被拼接并馈入![D_{4}^F](https://math.jianshu.com/math?formula=D_%7B4%7D%5EF)。我们遵循U-net[40]的风格，并在每个编码器块$E_i$和解码器块 $D_i^F$之间添加跳连，以保留细节。

**Loss Function**：损失函数FD(LFD)的训练损失包括一个alpha预测损失$L^T_α$和一个在未知过渡区域的拉普拉斯损失$L^T_{lap}$

![image-20220520102842415](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520102842415.png)

1.α损失$L^T_α$被计算为<font color='red'>未知过渡区中</font>的地面真值α和预测α值$α^F$之间的绝对差值。其定义如下：

![image-20220520103015987](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520103015987.png)

其中i表示像素索引,$W_i^T$∈ {0，1}表示像素i是否属于过渡区。

2.拉普拉斯损失$L^T_{lap}$被定义为真值和预测的拉普拉斯金字塔之间的L1距离。

![image-20220520103259844](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520103259844.png)

![img](https://img-blog.csdnimg.cn/20200613141329184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Z0aW1lcw==,size_16,color_FFFFFF,t_70)

### RoSTa：语义和过渡区域的表示

为了研究GFM中监控信号表示格式的影响，我们采用了三种语义和过渡区表示（RoSTa）作为连接GD和FD的桥梁。

**–GFM-TT**  我们使用经典的3级trimap T作为GD的监控信号，GD是由内核大小为25的真值alpha matte的膨胀和侵蚀产生的。我们使用未知过渡区的地面真值α matteα作为FD的监控信号。

**–GFM-FT** 我们使用2级前景分割掩模F作为GD的监控信号，由内核大小为50的真值alpha matte侵蚀生成，以确保正确标记剩下的前景部分。在这种情况下，I（α>0）的面积−F被视为过渡区，其中I（·）表示指示器功能。我们使用过渡区的地面真值α matteα作为FD的监控信号。

**–GFM-BT**  我们使用2级背景分割遮罩B作为GD的监控信号，该信号是通过内核大小为50的真值alpha matte的膨胀生成的，以确保剩下的背景部分正确标记。在这种情况下，B的面积−I（α>0）被视为过渡区。我们使用过渡区的地面真值α matteα作为FD的监控信号。

### **Collaborative Matting (CM)**

CM合并来自GD和FD的预测以生成最终的alpha预测。

CM在使用不同的RoSTa时遵循不同的规则。

在GFM-TT中，CM将GD预测的过渡区替换为FD预测。

在GFM-FT中，CM将来自GD和FD的预测相加，以生成最终alpha matte。

在GFM-BT中，CM从GD的预测中减去FD的预测，作为最终alpha matte。

![image-20220520104614178](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520104614178.png)

![image-20220520104620493](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520104620493.png)

这样，GD负责通过学习全局语义特征来识别粗略的前景和背景，FD负责通过学习局部结构特征来对未知区域中的细节进行抠图。

这种任务分解和专门设计的并行解码器使模型比中的两阶段模型更简单（Chen等人，2018；Zhang等人，2019b）。此外，两个解码器同时接受训练，使损耗可以通过CM模块反向传播到每个解码器。通过这种方式，我们的模型实现了两个解码器之间的交互，因此负责的分支可以及时纠正错误的预测。显然，它预期比两阶段框架更有效，在两阶段框架中，第一阶段的错误分割无法被后续阶段纠正，从而误导了它。

**Loss Function**：损失函数协同matting（LCM）的训练损失由α预测损失Lα、拉普拉斯损失Llap和合成损失LCMP组成，

![image-20220520104719267](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520104719267.png)

Lα和Llap是根据等式进行计算的（4）和等式（5），但是在整个alpha matte中

合成图像的合成损失（Lcomp）计算为基于真值alpha 和预测alpha matte的合成图像之间的绝对差

![image-20220520104830896](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520104830896.png)

其中C（·）表示合成图像，$α^{CM}$是CM预测的α matte，N表示α matte中的像素数。

![image-20220520105010110](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220520105010110.png)

##  四、**RSSN: A N**OVEL COMPOSITION **R**OUTE

### 4.1分辨率差异和语义歧义

背景图像的分辨率较低，可能包含显著的物体，导致以下两种类型的差异

(1)分辨率差异：前景图像和背景图像之间的分辨率差异会导致明显的伪影

![image-20220523092745692](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220523092745692.png)

(2)语义模糊性：收集MSCOCO[14]和PascalVOC[15]中的图像用于分类和对象检测任务，它们通常包含来自不同类别的突出对象，包括各种动物。直接将这些背景图像粘贴到前景图像上，会导致端到端图像匹配的语义模糊。

![image-20220523092856627](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220523092856627.png)

____

为了解决这些问题，我们提出了一个名为BG-20k的大规模高分辨率数据集，作为良好的背景候选。我们只选择了那些最短侧至少有1080像素的图像，以减少分辨率差异。此外，我们删除了那些包含显著对象的图像，以消除语义模糊性。BG-20k的构建细节如下。

(1)收集50k高分辨率(HD)图像.删除那些图像最短的一面小于1080像素和调整剩余图像的1080像素在最短的一面。BG-20k中图像的平均分辨率为1180×1539；

(2)我们通过深度匹配模型[41]去除重复的图像。我们采用YOLO-v3[42]来检测显著对象，然后手动反复检查，以确保每张图像都没有显著对象。通过这种方式，我们构建了包含20,000张高分辨率清晰图像的BG-20k；

(3)我们将BG-20k分割为一个不相交的训练集（15k）和验证集（5k）

我们采用[2]中的方法来计算我们的合成路线中的前景图像。

### 4.2锐度差异

在OpenCV中采用平均滤波器，随机从20、30、40、50、60中选择一个核大小来模糊背景图像。由于一些自然照片可能没有模糊的背景，我们只在合成路线中使用这种技术，概率为0.5。

### 4.3噪声差异

由于前景和背景来自不同的图像源，它们可能包含不同的噪声分布。这是另一种类型的差异，它会误导模型在训练过程中搜索噪声线索，导致过拟合。为了解决这一差异，我们采用BM3D[43]来去除RSSN中前景和背景图像中的噪声。此外，我们在复合图像中加入标准差为10的高斯噪声，使前景和背景区域的噪声分布是相同的。我们发现，它可以有效地提高训练模型的泛化能力。

### 4.4建议的合成路线

![image-20220523094531668](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220523094531668.png)

## 五、**AM-2**K: ANIMAL **M**ATTING **D**ATASET

### 5.1数据集的收集和标记

（1）从第4.1节中拥有开放许可的网站上收集约5000张动物相关图片，确保它们可以自由使用，它们的最短边超过1080像素；

   (2)保留包含外观清晰的显著动物的图像，并通过深度匹配模型[41]去除重复的图像；

   (3)在保持原来的长宽比的同时，调整保留图像的大小为1080像素，即1080像素；

   (4)使用开源图像编辑软件，如Adobephotoshop、GIMP等，手动标注每个图像和alpha哑光的类别标签。

经过上述过程，我们建立了我们的AM-2k数据集，该数据集平均包含了来自20个类别的2000张动物图像。所有的类别按字母顺序排列如下：羊驼、羚羊、熊、骆驼、猫、牛、鹿、狗、大象、长颈鹿、马、袋鼠、豹、狮子、猴子、兔子、犀牛、羊、老虎、斑马。

我们从原始图像的可用性(AOI)、数据集体积、平均分辨率和动物图像体积的角度，对我们的AM-2k数据集与文献中其他具有代表性的匹配数据集进行了简短的比较。

![image-20220523094955591](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220523094955591.png)

与Comp-1k[1]、LF[8]和HAtt[9]相比，AM-2k具有更多的图像，也提供原始图像，而其他的只提供前景图像

### **5.2 数据集分区和基准测试跟踪**

在构建AM-2k后，我们将其划分为不相交的训练集和验证集，并为不同的目的建立两个评估轨道。详细信息如下：

（1）ORI-Track（基于原始图像的跟踪）被设置为对原始自然图像执行端到端匹配任务。我们在每个类别中随机选择90张图像来形成训练集，同时使用每个类别中的剩余的10张图像来形成验证集。我们还保留了另外500张与训练和验证集不同的其他五种类型动物的图像，作为测试集。ori轨道是主要的基准轨道

  (2)COMP-Track（基于复合图像的跟踪）来进行图像匹配的领域自适应研究。由于合成伪影，与自然图像有较大的域差距。如果我们能减少域间隙并学习域不变特征表示，我们就能得到一个更好泛化的模型。为此，我们通过第一次尝试进入这一研究方向，建立了这条轨道。具体来说，我们通过将每个前景与COCO数据集[14](表示为COCOMP-COCOCO)和我们的BG-20k数据集(表示为COMP-BG20K)的5个背景图像混合，或者采用第4.4节提出的基于COMP-RSSN)的合成路线RSSN来构建复合训练集。我们在ORI-Track中的验证集的自然图像上评估抠图方法，以验证其泛化能力

## 六、实验研究

### 6.1实验设置

**数据集**:实验在我们的AM-2k数据集的两个轨道上进行：

1)为了比较我们提出的方法与最先进的(SOTA)方法，我们在ORI-Track上对它们进行训练和评估；

2)为了评估成分造成的域差距的副作用，我们分别对COMP轨道的GFM和SOTA方法，即COMP-COCO、COMPBG20k和COMP-RSSN。

**评价指标**:使用了均方误差(MSE)，绝对差分之和(SAD)，梯度(Grad.)，和连通性(Conn.)作为评估alpha哑光预测质量的主要指标。

$MAD=\frac 1 n \sum_i∣αi−αi^∗∣$

MSE和SAD度量评估了预测和地面真实的alpha哑光之间的定量差异，而梯度和连通性度量更倾向于清晰的细节。此外，我们还使用平均绝对差(MAD)、SAD-TRAN(过渡区域的SAD)、SAD-fg(前景区域的SAD)和SAD-bg(背景区域的SAD)来综合评估alpha哑光预测的质量。MAD评估图像大小的平均数量差异，SAD-TRAN、SAD-fg和SAD-bg分别评价不同语义区域的SAD

此外，我们还比较了不同方法的模型复杂度、参数复杂度和推理时间的数量、计算复杂度。

**实施细节**:

在训练过程中，我们使用了类似于[1]的多尺度增强方法。

具体来说，我们随机裁剪了每个大小来自{640×640,960×960,1280×1280}的选定图像，将裁剪后的图像大小调整为320×320，并以0.5的概率随机翻转它。GFM的编码器用在ImageNet数据集上预训练的ResNet-34[35]或DenseNet-121[36]初始化。GFM接受了两个NVIDIA特斯拉V100图形处理器上的训练。DenseNet-121[36]的批量大小为4，ResNet-34[35]的批量大小为32。

对于COMP-Track，我们在训练过程中使用每个前景的5个不同的背景来合成5个训练图像。

GFM在ORI-TRACK上训练了500轮和在COMP-Track上训练了100轮。

ORI-Track的学习速率固定为$1×10^{−4}$，comp-Track的学习速率固定为$1×10^{−5}$

对于基线方法LF[8]和SSS[12]，我们使用了作者发布的官方代码。对于没有公共代码的SHM[7]、HAtt[9]和SHMC[10]，我们根据论文重新实现了它们。对于没有指定主干网络的SHMC[10]，我们使用ResNet-34[35]进行了公平的比较。这些模型使用ORI-Track或COMP-Track上的训练集进行训练。

### 6.2定量评价和主观评价

#### **Results on the ORI-Track.**

我们在AM-2k的ORI-Track上对几种SOTA方法[7]、[8]、[9]、[10]、[12]进行了基准测试。结果汇总在表2的顶部。

GFM-tt、GFM-ft和GFM-bt表示如第3.4节所述的GFM模型，采用不同的RoSTa。(d)代表使用DenseNet-121[36]作为主干编码器，而(r)代表使用ResNet-34[35]作为主干编码器。从表中有几个实证结果

![image-20220526113052079](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220526113052079.png)

- 首先，SSS[12]的性能最差，前景SAD误差较大，为401.66，原因有两方面：1)采用预先训练的Deeplab-ResNet-101[44]模型作为语义特征提取器来计算亲和度。预先训练好的模型可能会在我们的高分辨率动物matting数据集上产生有限的代表性特征，从而降低性能；2)该方法的目的是提取图像中的所有语义区域，而其他matting训练方法只提取突出的动物前景。

- 其次，使用全局指导[10]的SHMC和阶段性的方法LF[8]的在所有评价指标上都优于SSS[12]。然而，在过渡区域的SAD误差主导总误差，分别为35.23和19.68。原因是两者都没有明确定义过渡区域，因此当需要同时使用同一网络来分割前景和背景区域时，匹配网络区分过渡区域细节的能力有限。

- 第三，HAtt[9]在过渡区和前景区的SAD误差方面的表现优于SHMC[10]和LF[8]，因为它所采用的注意模块可以提供更好的全局外观过滤。<font color='red'>然而，使用单一网络来建模前景和背景区域以及丰富细节的过渡区域，使得这两个区域很难具有强大的代表性特征</font>，导致大的SAD错误，

- 第四，SHM[7]在所有的SOTA方法中表现最好。与HAtt[9]相比，它将过渡区域的SAD误差从13.36降低到10.26，将背景区域的SAD误差从13.29降低到6.95。我们认为，该改进要归功于第一阶段使用的RoSTa和PSPNet[37]的明确定义，它具有良好的语义分割能力。然而，SHM[7]由于其阶段性管道，在背景区域仍然存在较大的误差，这将使分割误差累积到matting网络中。

- 最后，与所有的SOTA方法相比，GFM在所有评价指标上都优于它们，通过将前景和背景同时分割，无论使用哪种RoSTa，都能了最好的性能。例如，与之前的最佳方法SHM[7]进行了比较它在不同的区域达到了最低的SAD误差，即在过渡区为.45v.s10.26,前景区域为0.57v.s0.60，和在背景区域0.96v.s6.95。使用不同的RoSTa的结果具有可比性，特别是对于FT和BT，因为它们都在图像中定义了两类用于 Glance Decoder的分割。**使用TT作为RoSTa的GFM表现最好，因为它明确地定义了过渡区域以及前景和背景区域**。我们还尝试了两种不同的主干网络，ResNet-34[35]和DenseNet-121[36]。与其他SOTA方法相比，两者的性能都表现最好，而DenseNet-121[36]则略好一些。

GFM优于其他方法的原因可以解释如下。

- 首先，与SHM[7]、SSS[12]和LF[8]等阶段化方法相比，GFM可以在单个阶段中进行训练，合成模块作为一个有效的网关，将matting错误自适应地传播到负责的分支。
- 其次，与采用HAtt[9]和SHMC[10]等全局指导的方法相比，GFM明确地通过两个不同的解码器将端到端匹配任务建模为两个独立但相互协作的子任务。
- 此外，它还使用了一个合成模块，根据RoSTa的定义来合并预测，它明确地定义了每个解码器的角色。

从图6中，我们可以发现类似的观察结果。SHM[7]、LF[8]和SSS[12]未能对一些前景部分进行分割，表明其阶段性网络结构的劣势，因为它们在模型中没有明确区分前景/背景和过渡区域。对于前者语义分割和后者matting细节分割的作用难以平衡，这分别需要全局语义和局部结构特征。HAtt[9]和SHMC[10]努力在过渡领域获得清晰的细节，因为 global guidance有助于识别语义区域，而对细节的匹配不太有用。与它们相比，GFM由于统一的模型取得了最好的结果，该模型使用单独的解码器处理前景/背景和过渡区域，并以协作的方式进行优化。

![image-20220526115100367](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220526115100367.png)

#### **Results on the  **COMP-Track**.**

我们评估了表现最佳的SOTA方法SHM[7]和我们的GFM，在AM-2k的COMP-Track上有两种不同的骨干，包括COMP-COCO、COMP-BG20K和COMP-RSSN。结果总结在表2的底部，从中我们有几个实证发现。

![](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220526115458016.png)

- 首先，当使用来自MSCOCO数据集[14]的图像作为背景的匹配模型，GFM比SHM[7]表现得好得多，即46.16和30.05 v.s 182.70在全图像SAD中，证实了该模型优于双阶段模型的优越性。

- 其次，使用ResNet-34[35]的GFM比使用DenseNet-121[36]的GFM性能更好，这与ori上的结果不同。我们怀疑DenseNet-121比ResNet-34有稍好的表示能力，因此它可以在ori轨道上获得更好的结果，同时在comp上的复合图像有点过拟合。

- 第三，当使用BG-20k数据集的背景图像训练matting模型时，所有方法的误差都显著减少，特别是SHM[7]，即从182.70减少到52.36，这主要是由于背景区域SAD的减少，即从134.43减少到33.52。GFM(d)和GFM(r)也有同样的趋势。这些结果证实了我们的BG-20k的价值，这有助于减少分辨率差异，消除背景区域的语义歧义

- 第四，当使用所提出的合成路线训练mattiing模型时，可以进一步减少误差，即SHM[7]的误差从52.36减少到23.94，GFM(d)的误差从25.19减少到19.19，GFM(r)的误差从16.44减少到15.88。性能的提高归功于我们的RSSN中的合成技术：1)我们模拟大孔径效应以减少锐度差异；2)我们去除前景/背景的噪声，并在合成图像中添加噪声，以减少噪声差异。值得注意的是，当使用提出RSSN合成与使用传统的合成方法基于MSCOCO据集时，SHM的SAD误差[7]从182.70减少到23.93减少了约87%，，这甚至可以与使用原始图像进行训练得到的图像相比，即17.81。结果表明，所提出的合成路线RSSN可以显著缩小复合图像与真实自然图像之间的域间隙

- 最后，我们还在COMP-RSSN上使用GFM(d)中不同的RoSTa进行了实验，其结果与ori-track上有相似的趋势。

### **6.3 Model Ensemble and Hybrid-resolution Test**

![](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220526121022192.png)

**Model Ensemble**：

由于我们为GFM提出了三种不同的RoSTa，因此研究它们的互补性是很有趣的。为此，我们通过一个模型集成来计算结果，它以三个模型的alpha预测的中位数作为最终预测。

如表3所示，模型集成的结果优于任何单个模型集成的结果，即GFM-TT(d)值为9.21v.s10.27,GFMTT(r)为9.92v.s10.89，证实了不同RoSTa之间的互补性。

**Hybrid-resolution Test**：

在我们的GFM中，我们还提出了一种混合分辨率测试策略来平衡GD和FD。

- 首先将一个降采样的图像输入给GFM，以得到一个初始结果。

- 然后，我们使用全分辨率图像作为输入，只使用FD预测的alpha matte来代替过渡区域的初始预测。为简单起见，我们将每一步的降采样比表示为d1和d2，它们受d1∈{1/2、1/3、1/4}、d2∈{1/2、1/3、1/4}和d1≤d2的影响。我们用表3总结了GFM-TT(d)的结果。较小的d1增加了有效的感受域，有利于 Glance Decoder，而较大的d2代表更高分辨率的图像，并有利于Focus Decoder具有清晰的细节。

- 最后，我们设置d1=1/3和d2=1/2作为权衡。
