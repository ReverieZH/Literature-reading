# **Prototypical Networks for Few-shot Learning**

我们针对少镜头分类问题提出了*prototypical networks*，其中分类器必须推广到训练集中看不到的新类，而只给出每个新类的少量例子。

Prototypical networks学习一个度量空间，在其中可以通过计算到每个类的原型表示的距离来执行分类。

与最近的少镜头学习方法相比，它们反映了一种更简单的归纳偏差，这在这种有限的数据制度下是有益的，并取得了良好的结果。

我们提供了一个分析，表明一些简单的设计决策可以比最近涉及复杂的体系结构选择和元学习的方法产生实质性的改进。我们进一步将原型网络扩展到zero-shot learning，并在cu-birds数据集上获得了最先进的结果。



# **1 Introduction**

少镜头分类[20,16,13]是一项任务，其中分类器必须适应在训练中看不到的新类，只给出这些类的几个例子。一种简单的方法，比如在新数据上重新训练模型，会严重地过度拟合。虽然这个问题相当困难，但已经证明，人类有能力执行一次分类，其中只给出每个新类的一个例子，具有很高程度的准确性[16]。

最近的两种方法在少镜头学习方面取得了重大进展。Vinyals等人[29]提出了 *matching networks*，该网络使用注意机制对学习嵌入的标记例子集（支持集）来预测未标记点（查询集）的类。 *matching networks*可以解释为在嵌入空间内应用的加权最近邻分类器。值得注意的是，该模型在训练期间利用了被称为*episodes* 的采样小批，其中每个episode都被设计为通过子采样类和数据点来模拟少镜头任务。episode的使用使训练问题更忠实于测试环境，从而提高了泛化性。

Ravi和拉罗谢尔·[22]进一步提出了episodic训练的想法，并提出了一种少镜头学习的元学习方法。他们的方法包括训练一个LSTM[9]，在给定一个事件时产生对分类器的更新，这样它将很好地推广到一个测试集。在这里，LSTM meta-learner不是在多个episode中训练一个模型，而不是为每个episode训练一个自定义模型。

我们通过解决过拟合的关键问题来解决少镜头学习的问题。由于数据非常有限，我们假设一个分类器应该有一个非常简单的归纳偏差。

我们的方法，原型网络，是基于这样一种嵌入，即在这一种嵌入中，点围绕每个类的单个原型表示进行集群。为了做到这一点，我们使用神经网络学习输入到嵌入空间的非线性映射，并将一个类的原型作为其在嵌入空间中的支持集的平均值。然后，通过简单地找到最近的类原型来对嵌入式查询点执行分类。我们采用相同的方法来处理zero-shot学习；在这里，每个类都带有元数据，给出了对类的高级描述，而不是少量带标记的示例。因此，我们学习将元数据嵌入到共享空间中，作为每个类的原型。与在少镜头场景中一样，通过为嵌入式查询点找到最近的类原型来执行分类。

在本文中，我们制定了针对few-shot和 zero-shot 设置的原型网络。我们在 one-shot设置中绘制匹配网络的连接，并分析模型中使用的底层距离函数。特别地，我们将 prototypical networks与聚类[4]联系起来，以便证明当距离使用Bregman divergence计算时，使用类均值作为原型，如平方欧几里得距离。我们发现，距离的选择是至关重要的，因为欧几里得距离大大优于更常用的余弦相似度。在几个基准测试任务上，我们实现了最先进的性能。

prototypical network比最近的元学习算法更简单、更有效，这使它们成为zero-shot和few-shot学习的一种很有吸引力的方法。

# **2 Prototypical Networks**

## **2.1 Notation**

在少镜头分类中，我们给出了一个由N个标记的例子组成的小支持集$S=\{(x_1,y_1),....,(x_N,y_N)\}$

其中每个$x_i∈R^D$是一个例子的D维特征向量,$Y_i∈\{1，……，K\}$是相应的标签。$S_k$表示标记为类k的示例集。

## **2.2 Model**

原型网络通过可学习参数φ的嵌入函数$f_φ：\Bbb R^D→\Bbb R^M$计算每个类的M维表示$c_k ∈ \Bbb R^M$ 或原型

每个原型都是属于其类的嵌入式 support points的平均向量：

![image-20220815150821423](D:\文献阅读\few shot learning\image\image-20220815150821423.png)

给定一个距离函数d：$\Bbb R^M×\Bbb R^M→[0，+∞)$，原型网络基于嵌入空间中与原型有一定距离  的 softmax查询点x产生一个类上的分布：

通过SGD最小化真类k的负对数概率$J（φ）=−log\ p_φ(y=k|x)$进行学习。Training episodes是通过从训练集中随机选择类的子集，然后在每个类中选择一个例子的子集作为支持集，而其余部分的一个子集作为查询点来形成的。

在算法1中给出了计算一个训练事件的损失J（φ）的伪代码。

![image-20220815152529509](D:\文献阅读\few shot learning\image\image-20220815152529509.png)

## **2.3 Prototypical Networks as Mixture Density Estimation**

对于一类特殊的距离函数，即*regular Bregman divergences*[4]，该原型网络算法等价于对具有指数族密度的支持集进行混合密度估计。正则布雷格曼散度dϕ的定义为：

![image-20220815155444992](D:\文献阅读\few shot learning\image\image-20220815155444992.png)

ϕ是 Legendre type的一个可微的严格凸函数。布雷格曼发散的例子包括平方欧几里得距离$||z−z'||^2$和马氏距离。

原型计算可以从支持集上的硬聚类来看，每个类有一个聚类，每个支持点分配给其相应的类聚类。对于布雷格曼发散的[4]表明，到其指定点达到最小的距离是聚类平均值。因此，当使用布雷格曼散度时，方程（1）中的原型计算产生了给定支持集标签的最优聚类代表。

此外，任何参数为θ和累积函数ψ的正则指数族分布pψ(z|θ)都可以写为唯一确定的正则布雷格曼散度[4]：



# **5. Design Choices**

**(1)Distance metric：**通过以往工作和本文实验得出，使用欧几里得距离来作为距离度量会明显的优于使用余弦距离作为距离度量。

**(2)Episode composition:** 以往的实验发现，在训练和测试时保持相同的episode设置往往会得出较好的结果。例如，我们在测试时期望使用5-way-1-shot的方式，那么我们训练时就要使得episode的设置为Nc为5、Ns为1，其中Nc代表从episode中选择的类别的个数，Ns代表每个类别中被选择为支持样例的个数。然而，在我们的实验中发现，使用比测试时更高的Nc（“way”）对模型是有益的。

# **6. Zero-Shot Learning**

零样本学习不同于少样本学习，其meta-data向量Vk不是由训练集中的支持样本生成的，而是根据每个类的属性描述、原始数据等生成的。这些信息都是可以提取确定或者从原始数据中得到的。原形网络也能过灵活的转变成零样本学习，我们简单的定义Ck=gv（k）为一个meta-data向量。对于零样本学习和少样本学习我们详见下图：