# **NVAE: A Deep Hierarchical Variational Autoencoder**

归一化流、自回归模型、变分自编码器（VAEs）和基于深度能量的模型是深度生成学习的基于可能性的竞争框架。

其中，VAES具有快速、易于采样和易于访问的编码网络的优点。

然而，它们目前优于其他模型，如归一化流和自回归模型。

虽然大多数关于VAEs的研究都集中在统计学上的挑战上，但我们探索了为分层VAEs仔细设计神经结构的正交方向。

我们提出了新的VAE（NVAE），这是一种利用深度可分离卷积和批处理归一化来生成图像的深度层次VAE。

NVAE采用了正态分布的残差参数化，其训练通过谱正则化来稳定。

我们表明，NVAE在MNIST、CIFAR-10、CelebA 64和CelebA HQ数据集上的基于非自回归似然的模型中获得了最先进的结果，并为FFHQ提供了一个强大的基线。例如，在CIFAR-10上，NVAE将最先进的每个维度从2.98位推到2.91位，并在CelebA总部上产生高质量的图像，如图1所示.据我们所知，NVAE是第一个成功应用于高达256×256像素的自然图像的VAE。

# **1 Introduction**

大多数改进VAEs [1,2]的研究工作都致力于统计挑战，如减少近似和真实后验分布[3,4,5,6,7,8,9,10]之间的差距，制定更严格的边界[11,12,13,14]，减少梯度噪声[15,16]，将VAEs扩展到离散变量[17,18,19,20,21,22,23]，或解决后验崩溃[24,25,26,27]。神经网络架构对VAEs的作用有些被忽视了，因为以前的大多数工作都从分类任务中借鉴了这些架构。

然而，vae可以从设计特殊的网络架构中获益，因为它们有根本不同的需求。

首先，VAEs使输入变量和潜在变量[29,30]之间的交互信息最大化[29,30]，要求网络尽可能多地保留输入数据的信息内容.这与丢弃有关输入的信息的分类网络形成了鲜明对比[31]。

其次，VAEs对神经网络中的过度参数化的反应往往不同。由于边际对数似然只依赖于生成模型，过度参数化解码器网络可能会损害测试对数似然，而强大的编码器可以产生更好的模型，因为减少了摊销差距[6]。

Wu等人[32]观察到，由基于非编码器的方法估计的边际对数似然对编码器的过拟合不敏感（参见[19]中的图9）。此外，VAEs的神经网络应该模拟数据[33,34,35]中的长期相关性，要求网络有较大的接受域。

最后，由于在变分下界中的无界kullbeck-leibler（KL）散度，训练非常深的层次VAEs往往是不稳定的。目前最先进的VAEs [4,36]省略了批处理标准化（BN）[37]，以对抗可能会放大其不稳定性的随机性来源

_____

在本文中，我们的目标是通过架构设计使*VAEs*再次伟大。我们提出了新VAE（NVAE），这是一种深度层次的VAE，具有精心设计的网络体系结构，可以产生高质量的图像。NVAE在基于非自回归似然的生成模型中获得了最先进的结果，减少了与自回归模型的差距。我们的网络的主要组成部分是深度卷积[38,39]，它可以在不显著增加参数数量的情况下迅速增加网络的接受域。

与之前的工作相比，我们发现BN是深层VAEs成功的一个重要组成部分。我们还观察到，当分层群体的数量增加时，训练的不稳定性仍然是一个主要的障碍，独立于BN的存在。为了解决这个问题，我们提出了一个近似后验参数的残差参数化来改进最小化KL项，并证明了频谱正则化是稳定VAE训练的关键。

总之，我们做出了以下贡献： 

1. 我们提出了一种新的深度层次的VAE，称为NVAE，在其生成模型中具有深度卷积。
2. 我们提出了一种新的近似后验的残差参数化方法。
3. 我们用谱正则化来稳定训练深度VAEs。
4. 我们提供了实用的解决方案，以减少vae的内存负担。
5. 我们证明，深度层次的VAEs可以在多个图像数据集上获得最先进的结果，即使使用原始的VAE目标进行训练，也可以产生高质量的样本。据我们所知，NVAE是第一个成功地将VAEs应用于高达256×256像素的图像上。

____

**相关工作**：最近，VQ-VAE-2 [40]对大图像展示了高质量的生成性能。虽然VQ-VAE的公式是由VAEs驱动的，但其目标并不符合数据对数似然的下界。相比之下，NVAE是直接使用VAE目标进行训练的。此外，VQ-VAE-2在其先验中使用PixelCNN [41]对高达128×128的潜在变量进行采样，速度非常慢，而NVAE在数据空间中使用无条件解码器。

我们的工作与具有逆自回归流（IAF-VAEs）[4]的VAEs有关。NVAE借鉴了IAF-VAEs中的统计模型（即层次先验和近似后验等）。但是，它与IAF-VAEs的不同之处在于：

1. 实现这些模型的神经网络
2. 近似后验的参数化
3. 将训练扩展到大图像。

然而，我们在这些方面提供了消融实验，并且我们表明NVAE比原来的IAF-VAEs有很大的差距。最近，BIVA [36]通过将双向推理扩展到潜在变量，展示了最先进的VAE结果。然而，BIVA使用类似于IAF-VAE的神经网络，它在高达64×64px的图像上进行训练。为了保持简单，我们使用了iaf-vae的层次结构，并专注于仔细设计神经网络。如果使用BIVA中的更复杂的层次模型，我们期望性能会得到改善。在早期的作品中，DRAW [5]和Conv DRAW [42]使用递归神经网络来建模层次依赖关系。