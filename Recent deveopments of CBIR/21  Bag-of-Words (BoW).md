# **Video Google: A Text Retrieval Approach to Object Matching in Videos**

Sivic等人[21]提出了单词袋(BoW)方法，它使用k-means算法来创建码本。然后利用最接近特征点的聚类中心来代替特征点。通常，这种聚合方案会丢失一定的详细信息，生成的BoW向量非常稀疏。



我们描述了一种对象和场景检索的方法，它搜索和定位视频中用户概述的对象的所有出现。该对象由一组视点不变的区域描述符表示，因此，即使在视点、照明和部分遮挡方面发生了变化，识别仍能成功地进行。视频在一个镜头内的时间连续性用于跟踪区域，以拒绝不稳定区域，减少描述符中噪声的影响。

与文本检索的类比是在实现中，即用在匹配上的描述符是预先计算的（使用向量量化），并使用倒置的文件系统和文档排名。其结果是，检索是即时的，以谷歌的方式返回一个关键帧/镜头的排序列表

该方法用于两种全长故事片的匹配。

# 引言

这项工作的目的是检索包含特定对象的视频的关键帧和镜头，谷歌可以轻松、快速和准确地检索包含特定单词的文本文档（网页）。

本文研究了一种文本检索方法是否可以成功地用于对象识别。

在图像数据库中识别一个（相同的）对象现在已经达到了一些成熟度。这仍然是一个具有挑战性的问题，因为一个物体的视觉外观可能由于视点和光线而非常不同，它可能被部分遮挡，但现在存在成功的方法。通常，一个对象由一组重叠的区域表示，每个重叠的区域由一个从该区域的外观计算出的向量表示。区域分割和描述符用控制程度的视点和照明条件建立。对数据库中的所有图像计算类似的描述符。

对特定对象的识别通过描述符向量的最近邻匹配进行，然后使用局部空间相干性（如邻域、排序或空间布局）或全局关系（如上极性几何）来消除歧义。

我们探索这种类型的识别方法是否可以被重新定义为文本检索。本质上，这需要一个单词的视觉类比，这里我们通过向量量化描述量向量来提供。然而，可以看到，与文本检索进行类比不仅仅是对不同向量量化的简单优化。在文本检索文献中已经吸取和发展了许多经验教训，值得确定这些是否也可以用于视觉检索。

这种方法的好处是，匹配是有效地预先计算的，以便在运行时可以免费检索包含任何特定对象的帧和镜头。

这意味着，在视频中出现的任何对象（以及对象的连接物）都可以被检索到，在为视频构建描述符时，即使人们对这些对象没有明确的兴趣。然而，我们还必须确定这个向量量化检索是否会遗漏任何，如果使用前一种最近邻匹配方法就会得到的任何匹配。



**Review of text retrieval:**

文本检索系统通常采用一些标准步骤[1]。

这些文档首先被解析成文字。

第二，单词用它们的stems表示，例如“walk”、“walking”和“walks”将用stems的“walk”表示。

第三，停止列表是用来拒绝非常常见的单词，如“the”和“an”，它们出现在大多数文档中，因此对特定的文档没有区别。

然后为其余的单词分配一个唯一的标识符，每个文档由一个向量表示，其分量由文档所包含的单词的出现频率给出。

此外，组件以各种方式加权（在第4节中更详细地描述），在谷歌的情况下，一个网页的权重取决于链接到该特定页面的网页数量。

.上述所有步骤都在实际检索之前完成，并将表示语料库中所有文档的向量集组织为一个倒置文件[18]，便于高效检索。

一个倒置的文件的结构就像一个理想的书籍索引。它为语料库中的每个单词都有一个条目，后面是所有文档的列表

文中简单介绍了文字检索的相关步骤，主要分以下几步：

1. 对文档分词

2. 每个词都用它们的词根代替，如walking、walks、walk都用walk表示

3. 用stop list删除一些常见的词，如a an the等一些常出现的词

4. 一篇文章的表示法，即以每个字出现的频率组成一个向量来表示

5. 对于每个字其实都有一个某种形式的weight，例如Google利用PageRank的方式来做weighting

6. 在后台用上述方法把上面的文档处理后表示成一个向量，然后生成一个倒排文档(inverted file)


通过计算其单词频率的向量并返回具有最接近的（通过角度测量）向量的文档来检索文本。此外，可以使用单词排序和分离的匹配来对返回的文档进行排序。

论文大纲：在这里，我们将探索这些步骤的视觉类比。第2节描述了所使用的可视化描述符。第3节然后描述了它们的向量量化为可视化的“词”，第4节是向量模型的加权和索引。然后在第5节中的地面真实帧集上对这些想法进行评估。最后，在第6节中介绍了一个停止列表和排名（通过空间布局的匹配），并用于评估两部故事片的对象检索：“运行运行”(“罗拉Rennt”)[Tykwer，1999]和“土拨鼠日”[Ramis，1993]。虽然之前的工作借鉴了文本检索文献中关于从数据库中进行图像检索的想法（例如，[15]使用了加权和倒置文件方案），但据我们所知，这是这些想法首次系统地应用于视频中的对象检索。



# **2. Viewpoint invariant description**

第一种SA是以图像中的角点、拐点（corner）为基础提取出相应的椭圆区域；MS是通过对最大稳定极值区域（MSER）的提取得到椭圆区域表示。然后对提取出的这两种区域用sift特征描述子

为每一帧计算了两种类型的视点协变区域。

第一个是由一个兴趣点的椭圆形状自适应构造的。该方法包括迭代地确定椭圆的中心、尺度和形状。尺度是由拉普拉斯矩阵的局部极值（跨尺度）决定的，形状是由椭圆区域[2,4]的强度梯度各向同性决定的。实现细节在[8,13]中给出。此区域类型称为形状适应(SA)。

第二种区域是通过从强度分水岭图像分割中选择区域来构建的。这些区域是指当强度阈值变化时，面积近似静止的区域。实现细节在[7]中给出。这种区域类型被称为最大稳定型(MS)。

采用两种类型的区域，因为它们检测不同的图像区域，从而提供一个帧的互补表示。SA区域倾向于以角状特征为中心，而MS区域对应于与周围环境具有高对比度的斑点，如灰色墙上的深色窗口。这两种类型的区域都用椭圆号表示。这些计算在两倍的最初检测到的区域大小，以便图像外观更有鉴别性。对于一个720x576像素的视频帧，计算的区域数通常为1600。图1中显示了一个示例。

![image-20220711152859720](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711152859720.png)

每个椭圆仿射不变区域用一个使用SIFT描述符的128维向量表示。在[9]这个描述符被证明是优于其他文献中使用的响应，如一组可操纵的过滤器[8]或正交过滤器[13]，我们还发现正正是优越的（通过比较场景检索结果对地面真相在5.1节）。这种优越性能的原因是，SIFT不同像其他描述符，被设计为对区域位置的几个像素的移动不变，而这种定位错误是经常发生的。将SIFT描述符与仿射协变区域相结合，给出了对图像的仿射变换不变的区域描述向量。注意，区域检测和描述都是在帧的单色版本上计算的，颜色信息目前还没有在这项工作中使用。

为了减少噪声和拒绝不稳定区域，信息被聚集在一系列的帧上。在视频中，使用一个简单的constant velocity dynamical model 和相关性来跟踪在每一帧中检测到的区域。任何不能存活超过三帧的区域都将被拒绝。轨迹的每个区域都可以看作是一个独立测量的共同场景区域。而这个场景区域的描述符的估计值是通过平均整个轨迹的描述符来计算的。这对描述符的信噪比有了可测量的改进（使用第5.1节的地面真实测试再次证明了这一点）

# **3. Building a visual vocabulary**

这里的目标是将描述符向量量化为集群，这些集群将是用于文本检索的视觉“单词”。

 然后，当观察到电影的一个新帧时，该帧的每个描述符都被分配到最近的集群，这将立即为整个电影中的所有帧生成匹配。                    词汇表由电影的一个子部分构建，其匹配的准确性和表达能力在电影的其余部分进行评估，如下面的部分所述。

向量量化在这里是通过K-means聚类进行的，尽管其他方法(K-medoids，直方图分箱等)当然是可能的

## **3.1. Implementation**

文章中提到的两部视频按48个镜头大约10000帧的图像进行visual words的提取，然后将提取到的visual words用K-means的方法进行聚类，得到一副词典，这样每帧图像我们就可以像文字检索中的一篇文档那样，用词典中相关词的频率来描述。

区域通过连续帧跟踪，并为每个i区域计算平均向量描述符$\overline x_i$。为了拒绝不稳定区域，拒绝了具有最大的对角线协方差矩阵的10%的轨迹。这平均每帧产生大约1000个区域。

每个描述符都是一个128个向量，而同时聚类电影中的所有描述符将是一项巨大的任务。相反，我们选择了48个镜头的子集（这些镜头在第5.1节中有更详细的讨论），覆盖了约10k帧，这约占电影中所有帧的10%。即使有这样的减少，仍然有200K的平均轨道描述符必须被聚集。

为了确定聚类的距离函数，马氏距离计算如下：假设所有轨迹的协方差Σ是相同的，这是通过估计所有可用数据，即48次镜头中所有轨迹的所有描述符来计算的。马氏距离使128向量中噪声更大的分量被加权降低，并**去相关components**。根据经验，有很小程度的相关性。

两个描述符之间的距离函数

![image-20220711155927335](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711155927335.png)

按照标准的方法，描述符空间由Σ的平方根进行仿射变换，从而可以使用欧氏距离。

大约6k个簇用于形状适应区域，大约10k个簇用于最大稳定区域。

每种类型的集群数量的比率被选择为与检测到的每种类型描述符的比例大致相同。 

集群的数量是根据经验选择的 为了最大限度地提高第节5.1节的地面真实值集的检索结果。K-means算法运行了几次用随机初始分配的点作为集群中心，和这是最好的结果。

![image-20220711160555336](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711160555336.png)

图2显示了属于特定集群的区域的示例，即它们将被视为相同的视觉词。聚类区域反映了SIFT描述符的特性，它惩罚了小于互相关的区域之间的变化。这是因为SIFT强调梯度的方向，而不是区域内特定强度的位置。

SA和MS区域分别聚集的原因是它们覆盖了场景中不同的和很大程度上独立的区域。因此，它们可以被认为是描述同一场景的不同词汇，因此应该有自己的单词集，就像一个词汇描述建筑特征，另一个词汇描述建筑的修复状态一样。

# **4. Visual indexing using text retrieval** **methods**

在文本检索中，每个文档都用一个词频向量来表示。然而，通常是对这个向量[1]的分量应用一个加权，而不是直接使用频率向量来进行索引。这里我们描述了所使用的标准加权，然后是文档检索与框架检索的视觉类比。

标准的权重被称为“术语频率-逆文件频率”

假设有一个包含k个单词的词汇表，然后每个文档用一个k向量表示$V_d=(t_1,...t_i....t_k)^T$,带有分量的加权单词频率

![image-20220711161405053](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711161405053.png)

其中，nid为文档d中的单词i出现的次数，nd为文档d中的单词总数，ni是术语i在整个数据库中出现的次数，N是整个数据库中的文档数量。

权重是两个项的乘积：单词频率$\frac{n_id}{n_d}$和逆文档频率$\frac{logN}{n_i}$。

直觉上，单词频率对经常出现在特定文档中的单词进行权重，从而很好地描述它，而反向文档频率则降低了经常出现在数据库中的单词的权重。

在检索阶段，文档根据查询向量Vq与数据库中所有文档向量Vd之间的归一化标量积（角度余弦）进行排序。

在我们的例子中，查询向量是由包含在用户指定的一个帧的子部分中的视觉单词给出的，而其他帧则根据它们的加权向量与该查询向量的相似度进行排序。下一节将对各种加权模型进行评估。

# **5. Experimental evaluation of scene** **matching using visual words**

这里的目标是匹配场景位置在一个封闭的世界内的镜头[12]。该方法在电影《运行》中19个不同3D位置拍摄的48张照片的164帧上进行了评估。每个位置我们都有4-9帧。图3a显示了来自四个不同位置的三帧的示例。在相同位置显示的三组帧上有显著的视点变化。

triplet的每一帧都来自电影中不同的（时间遥远）的镜头。

在检索测试中，整个帧被用作查询区域。检索性能测量所有164帧，每帧作为查询区域。正确的检索由所有其他显示相同位置的帧组成，而这个地面真相是由完整的164帧集手工确定的。

利用相关图像[10]的平均归一化秩来衡量检索性能：

![image-20220711165153091](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711165153091.png)

其中Nrel为特定查询图像的相关图像数量，N是图像集的大小，Ri是第i个相关图像的排名

本质上，如果先返回所有Nrel图像，则Rank为零。Rank度量在0到1的范围内，0.5对应于随机检索。

## **5.1. Ground truth image set results**

图3b显示了使用数据集的每个图像作为查询图像的平均归一化秩，并使用第4节中描述的tf-idf加权。

拥有两种特征类型的好处是显而易见的。两者的结合显然比单独的任何一种都有更好的性能。每种特征类型的性能因不同的帧或位置而不同。例如，在帧46-49MS区域表现得更好，相反，对于帧126-127SA区域更优越。

![image-20220711165938483](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711165938483.png)

检索排名对19个地点中的17个地点都是完美的，即使是那些有显著视点变化的地点。对于61-70和119-121的图像来说，排名结果并不那么令人印象深刻，尽管即使在这些情况下，帧匹配也不会被错过，只是排名很低。这是由于在场景的重叠部分缺少区域，见图4。这不是向量量化的问题（共同的区域是正确匹配的），而是由于为这种类型的场景（路面纹理）检测到的特征很少。我们将在第7节中回到这一点。

![image-20220711170200322](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711170200322.png)

表1显示了三种标准文本检索术语加权方法[1]从所有164张图像中计算得到的秩度量的平均值。

tf-idf加权优于二值权（即如果图像包含描述符，向量分量为1，否则为零）和term频率权（分量是单词出现的频率）

对于整个地面真相集的平均排名，差异不是非常明显的。然而，对于特定的帧（例如49），差异可以高达0.1。

![image-20220711170412508](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711170412508.png)

所有帧的平均精度召回率曲线如图3c所示。对于每一帧作为查询，我们计算精度为相关图像的数量（即相同位置的数量）相对于检索的帧总数，并召回为正确检索的帧数相对于相关帧数。同样，结合这两种特性类型的好处也很明显。

![image-20220711170537527](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711170537527.png)

![image-20220711174127695](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711174127695.png)

这些检索结果表明，与不变量[12]的直接最近邻（或ε-最近邻）匹配相比，使用向量量化（视觉词）的性能没有损失。

该地面真实集也用于学习系统参数，包括：集群中心的数量；稳定特征的最小跟踪长度；以及基于协方差拒绝的不稳定描述子的比例。

# **6. Object retrieval**

在本节中，我们将评估在整个电影中搜索对象。所感兴趣的对象是由用户作为任何框架的子部分。

一个故事片长度的电影通常有100K-150K帧。为了降低复杂性，视频每秒使用一个关键帧。为每个关键帧中的描述符计算描述区域，并使用关键帧两侧的两帧计算平均值。描述符使用从地面真实集聚集的中心进行向量量化。

在这里，我们也在评估视觉词汇的表达性，因为地面真实集之外的框架包含新的对象和场景，并且它们被检测到的区域没有包括在形成集群中。

## **6.1. Stop list**

使用停止列表类比，几乎所有图像中出现的最常见的视觉单词都被抑制。图5显示了在Lola的所有关键帧上的视觉单词的频率。前5%和后10%被停止。在我们的例子中，非常常见的单词是由于超过3K点的大集群。停止列表边界是根据经验确定的，以减少不匹配的数量和反向文件的大小，同时保持足够的视觉词汇表。

![image-20220711175331933](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711175331933.png)

图6显示了强加一个停止列表的好处——非常常见的视觉单词出现在图像中的许多地方，并且是导致不匹配的原因。一旦应用了停止列表，其中大部分将被删除。下面将描述对剩余的不匹配的删除。

![image-20220711175545565](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711175545565.png)

## **6.2. Spatial consistency**

谷歌增加了搜索到的单词在检索到的文本中显示得很接近的文档的排名（以单词顺序来衡量）。

这种类比特别适合用于通过图像的子部分查询对象，其中检索到的帧中匹配的协变区域应该具有与查询图像中概述的区域具有相似的空间排列[12,14]（例如，紧凑性）。这个想法是通过首先单独使用加权频率向量检索帧，然后根据空间一致性的度量对其进行重新排序来实现的。

**通过简单地要求查询区域中的相邻匹配位于检索框架中的周围区域，就可以相当松散地测量空间一致性。它还可以通过要求相邻匹配在查询区域和检索帧中具有相同的空间布局来非常严格地度量。**

在我们的例子中，匹配的区域提供了查询和检索到的图像之间的仿射变换，因此一个点对点映射可用于这个严格的度量。

我们发现，在这个可能的测量范围的中间，获得了最佳的性能。一个搜索区域由每个匹配的15个最近的邻居定义，并且在该区域内也匹配的每个区域都为该帧投票。没有任何支持的将被拒绝。投票的总数决定了这个帧的排名。这工作得很好，如图6的最后一行所示，它显示了不正确匹配的空间一致性拒绝。图7至图9的对象检索例子采用了这种排名度量，并充分证明了它的有效性。

在某些情况下，可能需要采取能够考虑图像之间仿射映射的其他措施，但这涉及到更大的计算费用。

## **6.3. Object retrieval**

**Implementation – use of inverted fifiles:**

在经典的文件结构中，所有单词都存储在它们出现的文档中。一个倒置的文件结构为每个单词都有一个条目（点击列表），其中存储了所有文档中出现的所有单词。在我们的例子中，倒置的文件对每个视觉单词都有一个条目，它存储所有匹配，即在所有帧中出现相同的一个单词。文档向量非常稀疏，使用倒置文件使检索速度非常快。在2 GHz奔腾上使用Matlab实现，查询一个4k帧的数据库大约需要0.1秒。

**Example queries:**

图7和图8显示了电影“运行LolaRun”的两个对象查询的结果，图9显示了电影“土拨鼠日”上的一个对象查询的结果。这两部电影都包含了大约4K个像素的关键帧。实际返回的帧和它们的排名都是优秀的——就可能而言，没有包含对象的帧被遗漏（没有假阴性），而且排名较高的帧都包含对象（良好的精度）。对象查询结果确实展示了视觉词汇表的表达能力。为萝拉学习到的视觉单词在土拨鼠日检索中使用不变。