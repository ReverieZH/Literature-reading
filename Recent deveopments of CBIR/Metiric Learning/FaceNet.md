# **FaceNet: A Unifified Embedding for Face Recognition and Clustering**



# 一、Abstract

尽管最近在人脸识别[10,14,15,17]领域取得了重大进展，但大规模有效地实现人脸验证和识别对当前的方法提出了严重的挑战。在本文中，我们提出了一个被称为FaceNet的系统，它直接学习从人脸图像到一个紧凑的欧几里得空间的映射，其中距离直接对应于人脸相似度的度量。一旦产生了这个空间，就可以很容易地使用标准技术来实现诸如人脸识别、验证和聚类等任务。

我们的方法使用一个经过训练的深度卷积网络来直接优化嵌入本身，而不是像以前的深度学习方法那样使用中间*bottleneck* layer。

为了训练，我们使用一种新的在线triplet  mining方法生成的匹配/不匹配的面部特征对。

我们的方法的好处是提高了更大的表征效率：我们每个人脸只使用128字节就实现了最先进的人脸识别性能。

在广泛使用的*Labeled Faces in the Wild* (LFW)数据集上，我们的系统达到了99.63%的新记录准确率。在YouTube FacesDB上，它达到了95.12%。与在两个数据集上发布的最佳结果[15]相比，我们的系统将错误率降低了30%

我们还介绍了**谐波嵌入（harmonic embeddings）的概念和谐波三元组损失（harmonic triplet loss）**，它们**描述了相互兼容的不同版本的人脸嵌入（由不同的网络产生），并允许相互直接比较**。

# 二、**Introduction**

 在这个论文里我们呈现了包含人脸验证（是不是同一个人）、识别（他是谁？）、聚集（在人脸中找到相同的人进行归类）的一个完整的系统。

我们的方法是基于每张图像用一个深度卷积网络学习一个Euclidean embedding。

然后把这个网络进行训练这样在embedding space的squared L2距离直接对应人脸相似度：同一个人的人脸具有小的距离不同人的人脸具有较大的距离
一旦这个embedding产生前面提到的任务就变得很简单了：人脸验证仅仅只涉及两个embedding距离的阀值；识别成了一个k-NN分类问题；聚集可以用现成的例如k-means或者agglomerative clustering之类的技术来实现。 

以前基于深度网络工作的人脸识别方法使用经过一组已知人脸身份训练的分类层[15,17]，然后取一个中间的 bottleneck层作为一种表示，用于推广训练中使用的身份集之外的身份。这种方法的缺点是它的间接性和低效率：人们必须希望bottleneck表示能很好地推广到新面孔；通过使用bottleneck层，每个面孔的表示大小通常非常大（1000个维数）。最近的一些工作[15]使用PCA降低了这个维数，但这是一个线性变换，可以很容易地在网络的一层中学习

与上述那些方法比起来，FaceNet运用基于LMNN的triplet- based loss函数直接把输出训练成一个紧凑的128-Dembedding。我们的triplets包含两个匹配的人脸缩略图和一个非匹配的人脸缩略图。

Loss函数的目的就是通过距离边界区分正负类。缩略图为精密剪裁的脸部区域，除了执行缩放平移外没有2D or 3D校准。


选择运用triplets被证明对于取得好的性能是非常重要的，并且受curriculum learning启发，我们提出了一个online negative exemplar mining策略，它确保了随着网络训练triplets的难度持续增长。为了提高聚类准确度，我们也探索了hard-positive mining技术它对于每个人的embeddings激发出球形聚类。

为了说明我们的方法可以处理的令人难以置信的可变性，请参见图1。显示的是来自PIE[13]的图像对，以前被认为是非常困难的人脸验证系统。

![image-20220611153758256](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611153758256.png)

其余论文内容概述如下：

section 2：回顾本领域的相关文献

section 3.1：定义了triplet loss

section 3.2：描述了triplet selection& training procedure

section 3.3：所用的模型结构

section 4 and 5：提出了一些关于embeddings的定量的结论，并且定性地探索了一些聚类结论。

# 三、相关工作

和现在的运用的深度卷积网络的方法类似，我们的方法是一个纯粹的数据驱动方法，该方法从人脸的每一个像素开始直接学习它的表示。我们运用标记人脸的大型数据库去获得合适的姿态光照和其他可变的情况的不变性，而不是运用engineered features。

在论文里我们探究了最近在计算机视觉社区成功使用的两类不同的深度卷积神经网络架构。

第一个架构是基于Zeiler&Fergus模型其包含multiple interleaved layers of convolutions, non-linear activations, local response normalizations,and max pooling layers。受[9]的工作启发我们添加了几个1*1*d卷积层。

第二个架构是基于the Inception model of Szegedy et al 这种架构被称为ImageNet 2014 中最优的方式。这些网络在并联和串联它们的相应的时候运用运行在几个不同的卷积和池化层组成的混合层。我们发现这两种模型都可以减少参数的使用次数达20次并且有减少浮点运算次数的潜在性能。


# 四、Method

FaceNet运用深度卷积网络。我们讨论两个不同的核心结构：

TheZeiler&Fergus架构网络

The recentInception架构网络

这些网络的详细描述在3.3部分

考虑到模型的细节，我们暂且把它视为一个黑盒（见图2），我们的方法最重要的部分在于对整个系统的端到端的学习。系统末端我们运用the triplet loss直接反射我们想要得到的人脸验证、识别、和聚类。换句话来说，我们在争取实现一个embedding f(x)，从一个图像到一个特征空间$R^d$，这样在图像条件独立的情况下，相同特征的所有人脸间的平方距离是比较小的，然而不同特征的人脸图像对的平方距离是比较大的。
![image-20220611154324963](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611154324963.png)

 虽然我们并没有直接比较其他losses，例如在[14] Eq中用于正负类的loss，但是我们相信the triplet loss对于人脸验证是最合适的。激励是某个损失其来自在the embedding space鼓励同一特征的所有人脸投影到单个点。但是The triplet loss试图加强从某一人脸到其他人脸人脸对的边缘距离。这就允许同一特征的人脸可以依靠其一个复本，同时加强上述人脸间距离并且从而分辨其他特征。


## 4.1 **Triplet Loss**

  Embedding用f(x)∈Rd表示。它把一个图像x嵌入到一个d维的欧几里得空间。另外，我们依靠一个d维的超球面约束这个embedding。如：$$||f(x)||_2 = 1$$.这个loss被涉及在[19]的最近邻居分类的上下文中。

在这里我们想确保一个特定人的图像$x^a_i$ (anchor)更接近于这个同一个人的其他图像$x^p_i$ (positive)较远于其他任何人的任何图像$x^n_i$(negative)。见图3


![image-20220611162222861](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611162222861.png)

![image-20220611162237428](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611162237428.png)

其中 α 是在正负对之间强制执行的边距。 $\mathcal{T}$ 是训练集中所有可能的三元组的集合，具有基数 N


最小化的损失是 L =![image-20220611162344802](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611162344802.png)

**生成所有可能的三元组将导致许多容易满足的三元组，即满足等式 (1) 中的约束。这些三元组不会对训练做出贡献，并且会导致收敛速度变慢**，因为它们仍然会通过网络。**选择活跃的难三元组至关重要，因此有助于改进模型**。以下部分讨论了我们用于三元组选择的不同方法。

## 4.2. Triplet Selection

**为了确保快速收敛，选择违反等式 (1) 中的三元组约束的三元组至关重要。这意味着，给定$x^a_i$**，我们想要选择一个$x^p_i$(hard positive) 使得 
![image-20220611163213287](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611163213287.png)并且类似地 $ x^n_i$  (hard negative) 使得![image-20220611163315276](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611163315276.png)

计算整个训练集的 argmin 和 argmax 是不可行的。此外，它可能会导致训练不佳，因为错误标记和图像不佳的人脸会主导难正例和负例。有两个明显的选择可以避免这个问题：

- 每 n 步离线生成三元组，使用最近的网络检查点并在数据子集上计算 argmin 和 argmax。

- 在线生成三元组。这可以通过从小批量中选择硬正/负样本来完成。

在这里，我们专注于在线生成，并使用大约几千个样本的大型 mini-batch，并且只计算 mini-batch 中的 argmin 和 argmax。

为了对 anchor positive (锚点正例) 距离进行有意义的表示，需要确保确保任何一个身份的最小数量的样本出现在每个小批量中。在我们的实验中，我们对训练数据进行采样，使得每个小批量的每个身份选择大约 40 个人脸。

此外，随机采样的负例被添加到每个小批量中。我们没有选择最难的正例，而是在一个小批量中使用所有锚点正例对，同时仍然选择难的负例。我们没有在小批量中对难锚点正例对和所有锚点正例对进行并排比较，但在实践中发现，所有锚点正例方法更稳定，并且在训练开始时收敛稍快。

我们还结合在线生成探索了三元组的离线生成，它可能允许使用较小的批量，但实验没有结论。

在实践中，选择最难的负样本可能会在训练早期导致局部最小值不佳，特别是它可能导致模型崩溃，即 f(x) = 0。为了减轻这种情况，它有助于选择 $x^n_i$使得


![image-20220611170656684](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611170656684.png)

我们将这些负样本称为半难（semi-hard）样本，因为它们比正样本离锚点更远，但仍然很难，因为平方距离接近锚点正例距离。这些负例位于边距 α 内。

如前所述，正确的三元组选择对于快速收敛至关重要。一方面，我们希望使用小型 mini-batch，因为它们往往会在随机梯度下降 (SGD) [20] 期间提高收敛性。另一方面，实施细节使数十到数百个样本的批次更有效率。然而，关于批量大小的主要限制是我们从小批量中选择难相关三元组的方式。在大多数实验中，我们使用大约 1800 个样本的批量大小。

## 4.3. Deep Convolutional Networks

在我们所有的实验中，我们使用带有标准反向传播 [8, 11] 和 AdaGrad [5] 的随机梯度下降 (SGD) 来训练 CNN。

在大多数实验中，我们从 0.05 的学习率开始，我们将其降低以最终确定模型。模型从随机初始化，类似于 [16]，并在 CPU 集群上训练 1,000 到 2,000 小时。训练 500 小时后，损失的减少（和准确性的增加）急剧下降，但额外的训练仍然可以显着提高性能。余量 α 设置为 0.2。

我们使用了两种类型的架构，并在实验部分更详细地探讨了它们的权衡。它们的实际区别在于参数和 FLOPS 的不同。最佳模型可能因应用而异。例如。在数据中心运行的模型可以有很多参数，需要大量的 FLOPS，而在手机上运行的模型需要很少的参数，以便它可以放入内存中。

我们所有的模型都使用 ReLU 作为非线性激活函数


# 五、Datasets and Evaluation

我们在四个数据集上评估我们的方法，除了野外的 Labeled Faces 和 YouTube Faces，我们在人脸验证任务上评估我们的方法。例如，给定一对两张人脸图像，使用平方 L2 距离阈值 D(xi, xj) 来确定相同和不同的分类。所有具有相同身份的人脸对 (i, j) 用 Psame 表示，而所有不同身份的人脸对用 Pdiff 表示。


我们将所有**真实接受（true accepts）**的集合定义为

![image-20220611171056676](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611171056676.png)

这些是在阈值 d 被正确分类为相同的人脸对 (i, j)。相似地

![image-20220611171109803](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611171109803.png)

是被错误分类为相同（错误接受）的所有对的集合。

给定人脸距离 d 的**验证率 VAL(d) 和错误接受率 FAR(d)** 定义为

![image-20220611171124283](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220611171124283.png)

## **5.1 Hold-out Test Set**

我们保留与我们的训练集有同样分布但是有不同特征的大约100万**张照片的留出集。为了便于评估我们把留出集分成5个不相交的子集每个里面有200k图像。FAR和VAL率在100k x100k**的**图像对**上**计**算**。**通**过这**五个分**块**来描述**标准差**。

## **5.2. Personal Photos**

 这是一个和我们的训练集有相同分布的测试集，但是它必须手工验证去保证非常整洁的标签。它由总共12k**图像的三个个人照片集组成。我们在所有12k squared**对图**像上**计**算**FAR 和VAL**率**。

## **5.3. Academic Datasets**

LFW**是事**实**上的人**脸验证**学**术测试集。我们遵守无限制标记外部数据的标准协议并且报告平均分类准确度和平均标准差。

Youtube Faces DB**是新的数据集并且在人**脸识别**社区很受**欢**迎。其上面可以使用**视频**而不是**验证图**像**对

# 六、实验

如果没有另外**说**明我**们**使用由**8M**不同特征**组**成的**在**100M-200M**之**间**的**训练**人**脸缩略图。在每个人脸上运行人脸探测器并在人脸周围生成一个紧密的**bounding box**。**这**些人**脸缩**略**图**被**调**整成相**应**网**络**的**输**入大小。在我**们**的**实验中输入大小的范围是从**96x96**像素到**224x224像素。

## **6.1. Computation Accuracy Trade-off**

在探究更具体的实验细节前我们将会讨论准确度和浮点运算次数的权衡这是一个特定的模型所需要的。图4中用x轴表示浮点运算次数和在4.2中的用户标记数据集中0.001的错误可接受率下的准确度。很愿意看到一个模型所需要的计算和它取得的准确度具有强相关。图示强调了在我们实验中详细讨论的5个模型(NN1, NN2, NN3, NNS1, NNS2)。


![image-20220613160858791](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220613160858791.png)

我们也研究关于模型参数的准确度权衡。但是，这种情况在图片中并不是太清楚。例如，Inception based model NN2与NN1相比有更好的性能，但是仅有一个a 20th of the parameters。尽管浮点运算次数是比得上的。很明显在某些情况下性能是期望减少的，如果参数量进一步减小。其他模型架构可能在不损失准确度的情况下进一步减小，Inception [16]就是这种情况。


##  **6.2 Effect of CNN Model**

下面我们更详细的讨论四个备选模型的性能。

一方面我们有基于1x1卷级架构的Zeiler&Fergus（见表1）.

另一方面大大减小模型尺寸的Inception[16] based models。总之，用这两种架构的顶级模型最后的性能相当。不管怎么样，一些Inceptionbased models例如NN3仍然在减少浮点运算次数和模型大小方面取得了好的性能。
![image-20220613161414830](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220613161414830.png)

图5显示了在我们的个人照片测试集上更加详细的评估。虽然NN2相比紧凑的NNS2模型最大的模型在准确度方面取得了极大的提高，但是紧凑的NNS2可以在移动手机上每个图像运行仅30ms并且对于人脸聚类来说他也是足够准确的。在ROC曲线上当FAR < $10^{-4}$    时的急剧下降表示测试数据的基础真实值中的噪声标签。在一个特别低的可接受错误率下一个错误标注的图像在曲线上可能有很大的影响。


## **6.3. Sensitivity to Image Quality**

表4显示了我们的模型在较广泛的图像大小下的鲁棒性。

这个网络在JPEG压缩的时候有惊人的鲁棒性并且在JPEG质量为20时执行的非常好。

对于下降到120x120像素大小甚至80x80像素的人脸缩略图来说性能下降非常小这明显是较为合格的性能。

这种情况是引人注意的，因为网络在一个220x220的输出图像上训练。在低分辨率人脸上训练可以扩大需要像素的范围。


![image-20220613161812646](C:\Users\13449\AppData\Roaming\Typora\typora-user-images\image-20220613161812646.png)