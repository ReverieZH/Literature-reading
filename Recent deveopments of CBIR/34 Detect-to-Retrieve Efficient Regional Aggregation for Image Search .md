**Detect-to-Retrieve: Effificient Regional Aggregation for Image Search**

# 摘要

在杂乱的场景中高效地检索对象实例需要紧凑而全面的区域图像表示。直观地说，对象语义可以帮助构建关注最相关区域的索引。然而，由于缺少针对检索基准中感兴趣对象的边界框数据集，最近关于区域表示的工作主要集中在统一区域选择或与类无关的区域选择上。在本文中，我们首先通过提供一个新的地标边界框数据集来填补这一空白，这个数据集是基于谷歌Landmarks数据集，其中包括86k张图片和来自15k个unique Landmarks的手动编辑框。然后，我们演示了如何使用我们的新数据集，训练一个地标探测器，可以利用索引图像区域和提高检索精度，同时比现有的区域方法更有效。此外，我们还引入了一种新的区域聚合选择匹配核(R-ASMK)来有效地将检测区域的信息组合成一种改进的整体图像表示。在不增加维数的情况下，R-ASMK大大提高了图像检索的准确性，甚至优于单独的索引图像区域的系统。我们完整的图像检索系统在之前的先进水平的基础上进行了改进，在 Revisited Oxford and Paris datasets有显著优势。

# 引言

本文主要针对图像检索问题:给定一个查询图像，系统应该有效地从数据库中检索相似的图像。

图像检索系统通常由两个主要阶段组成:

(1)过滤，一种根据数据库图像与查询的相似性对其进行排序的有效技术;

(2)重新排序，对第一阶段的少量最相似的数据库图像进行更详细的检查并重新排序。

通常，手工标记的局部特性[21,6]与受单词启发的技术[36,26,27,14,15,16,38]相结合，构建过滤步骤中使用的高维表示。

局部特征匹配和几何验证[26,27,3](https://blog.csdn.net/weixin_42403696/article/details/通常使用RANSAC[8])是有效的重新排序策略。

最近，针对这两个阶段提出了几种深度学习技术。基于卷积神经网络(CNN)的全局图像表示可以产生紧凑的嵌入，从而在滤波步骤中实现快速的相似度计算[5,4,40,1,9,30]。

局部图像表示也可以使用CNNs提取，适合通过空间匹配和几何验证重新排序[25,24,23]。

当前的图像检索系统往往会在相关对象在数据库图像中没有占据足够大的比例时失败，尤其是在一些混乱的场景中。通常，这些对象生成的局部特征可用于在重新排序阶段查找与查询图像匹配的局部匹配。然而，这些杂乱的图像通常无法达到重新排序的阶段，因为与过滤阶段的查询相比，它们的初始表示不会产生很高的相似性。

估计查询图像的改进相似性最常见的解决方案是，使用固定的区域网格[2,31]或类无关的检测器[37,17]，提取并单独存储数据库中感兴趣区域的图像表示形式。然而，现有的区域选择技术产生了大量的不相关区域。在最近的一次大规模实验图像检索评估中，Radenovic等人[28]得出结论，这种区域搜索方法在内存和延迟方面的代价太高，只能获得很小的精度增益。

**贡献** (

1)第一个贡献是改进了区域选择:通过引入了一个人工装箱的地标图像数据集，其中包含来自15k个唯一类的86k个图像，我们证明了可以训练检测器进行健壮的地标定位。

(2)第二个贡献是利用训练有素的探测器，产生更有效的区域搜索系统，该系统只需要稍微增加数据库大小就可以提高小目标的准确性——比以前提出的技术效率高得多。

(3)第三个贡献中，提出了区域聚合匹配内核来利用选定的图像区域，并产生一个有区别的图像表示，如图1所示。这种新的表示方式显著优于区域搜索系统，同时更高效:每个图像只需要存储一个描述符。我们的图像检索系统在重访问牛津硬数据集上的绝对平均精度比以前发表的结果高9.3%，在重访问巴黎硬数据集[28]上的绝对平均精度高1.9%。



# **相关工作**

数据集：

图像检索和聚合：在图像检索系统中，对区域选择进行了研究。它们被用于两个不同的目的:(i)区域搜索:选定的区域在数据库中独立编码，以便检索子图像;(二)区域聚合:利用选定区域改进图像表示。

区域搜索：

许多论文提出使用VLAD[15]或Fisher向量[16]来描述区域：阿兰德耶洛维奇和齐泽曼[2]使用多尺度网格提取每幅图像14个区域；Tao等人[37]使用选择性搜索[41]，每张图像有数千个区域；Kim等人[17]使用最大稳定极值区域(MSER)[22]。Razaian等人[31]使用每幅图像30个区域的多尺度网格，并通过考虑所有区域对之间的距离计算两幅图像的相似性。Iscen等人[13,12]利用多尺度网格结合CNN特性[29]，通过扩散实现查询扩展。最近，拉德诺维奇等人对检索技术进行了全面的评估，并得出结论，现有的区域搜索方法可以提高识别准确性，但需要更大的记忆和复杂性成本。相比之下，我们的检测到检索框架旨在通过使用自定义训练的探测器进行高效的区域搜索。



区域聚合：

Tolias等人[40]利用[31]的网格结构将预训练的CNN特征[18,35]汇集成紧凑的表示；每张图像选择大约20个区域。

[29]等人通过在无监督的方式收集的数据集上重新训练特征。

Gordo等人的[9]从半自动的边界框注释中训练一个区域建议网络[32]，以取代[40]中的网格。在本例中，每幅图像将考虑数百个区域。

我们的工作与这些论文不同，我们使用了一小组区域（每张图像少于5个），并将区域聚合作为一个新的匹配核（而不是[40,9]中的区域sum-pooling）。

# **区域检索和聚合**

我们提出了技术，提高图像检索性能利用边界盒预测训练的地标检测器。

特别是，我们的方法建立在深度局部特征(DELF)[25]和聚合选择匹配内核(ASMK)[38]之上，这些特征最近在大规模图像检索基准[28]上实现了最先进的性能。

## **4.1背景**


通过Tolias [38]等人的聚合匹配核函数框架。

一个图像 X 描述成在每一个维度D上包含M个位置描述符的集合，一个由C个视觉语言组成的码本C，使用k-means学习，用来量化这个描述符。可以利用最近邻算法判断图像X描述符哪些是视觉语言。所以根据这个框架就可以对两个图像之间的相似性进行计算。

一个图像X描述成集合 $\chi=\{x_1,x_2...x_M \}$包含M个局部描述符，每一个维度D上。一个码本C包含C个视觉单词，使用k-means学习

，用来量化这个描述符。

$\chi_c=\{x\in \chi：q(x)=c \}$作为X中的描述子的子集，由最近邻的邻居量化器q(x)分配给视觉词c。

由局部描述符集X和Y表示的两幅图像X和Y之间的相似性可以计算为：

![image-20220711142714318](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711142714318.png)

Φ(X)是一个聚合的向量表示，σ(.)表示标量选择性函数， $\mathcal{Y}(\chi)=(\sum_c \sigma (\Phi (\chi_c)^T\Phi (\chi_c)))^{-\frac12}$ 是归一化因素

这个公式包含了流行的局部特征聚合技术，如单词包[36]、VLAD[15]和ASMK[38]。

https://zhuanlan.zhihu.com/p/448295268

特别是对于VLAD，$\sigma(u)=u$，$\Phi (\chi_c)$对应于一个聚合的残差$V(\chi_c)=\sum_{x\in \chi_c}x-q(x)$

对于ASMK，σ(u)对应于一个阈值的多项式选择性函数

![image-20220711144701044](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711144701044.png)

通常是α=3和τ=0；和Φ(Xc)对应于一个归一化的聚合残差$\hat V(\chi_c)=\frac {V(\chi_c)}{||V(\chi_c)||}$。

![image-20220712180408541](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712180408541.png)

## **4.2. Regional Search**

在本节中，我们将考虑将区域描述符独立存储在数据库中的图像检索系统

将查询图像表示为X，N个图像的数据库表示为{$Y^{(n)}$}，n=1,2，...，N。

我们主要感兴趣的是实验配置，其中查询包含一个定位良好的感兴趣区域（即查询实际中只包含一个区域），这是图像检索中常见的设置。

对于第n个数据库图像，区域$r_n=1，……，R_n$由地标检测器预测，定义子图像{$Y^{(n，r_n)}$}。

我们将Y(n，1)=Y(n)表示为与原始图像对应的子图像，并始终将其视为一个有效的区域。

为了利用整洁的表示，我们为每个子图像独立地存储聚合的描述符，这导致数据库中总共有$\sum _{n=1}^NR_n$项。

计算查询X与数据库图像$Y^{(n)}$之间的相似性,我们分别考虑最大池或平均池的个体区域相似性：

![image-20220712192046513](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712192046513.png)

最大池化对应于分配一个数据库图像的分数，只考虑其得分最高的子图像。

平均池化聚合了来自所有子图像的贡献。这两种变体在第 5节中进行了比较。 

## **4.3. Regional Aggregated Match Kernels**

在数据库中独立存储每个区域的描述符会导致内存和搜索计算的额外成本。

在本节中，我们考虑使用检测到的边界框来改进数据库图像的聚合表示——不需要额外的成本产生鉴别描述符

我们将Tolias等人的[38]的聚合匹配核框架扩展到区域聚合匹配核，如下所示。

我们首先注意到平均池化相似度等式（4）可以重写为：

![image-20220712192622436](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712192622436.png)

**Simple regional aggregation**

对于VLAD

![image-20220712193128728](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712193128728.png)

使用这个定义，请注意$V_R(X_c)=γ(X)V(X_c)$。这个推导表明，区域VLAD相似性的平均池化可以使用聚合的区域描述符来执行，并且不需要单独存储每个区域的表示1。

我们将这个简单的区域聚合内核称为R-VLAD。

在σ（.）是身份函数（即，不应用选择性的情况下)，ASMK也可以得到类似的推导，通过将等式(6)中的$V(X_c)$替换为$\hat V(X_c)$ .

.使用此想法的直接匹配内核，在比较查询ASMK表示与此聚合表示时，将应用选择性函数。我们将这种聚合变体称为Naive-R-ASMK

无论是R-VLAD内核还是Naive-R-ASMK内核，在使用每个图像和大码本的许多检测区域时都存在一个重要的问题。对于给定的图像区域，大多数视觉语言不会与任何局部特征相关联，从而导致该区域出现许多全零残差。对于只在少数区域观察到的与视觉模式相对应的视觉语言，会导致残差显著下降。论文使用如下方式开发R ASMK内核来修复这个缺陷。

**R-ASMK**：定义查询图像与数据库图像间的R-ASMK相似性为

![image-20220712194157337](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712194157337.png)

![image-20220712194229593](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712194229593.png)

$\hat V_R(\{γ_c^{(n,r)}\}_r)$是与视觉词c对应的归一化的区域聚合残差。

**R-AMK：**在本节中给出的内核可以看作是一般区域聚合匹配内核(R-AMK)的不同实例化，定义如下:

![image-20220712194824081](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712194824081.png)

其中，{Xc(r)}r表示从X的每个区域量化为视觉词c的局部描述符集合

$Φ_R$专门为R-VLAD的$V_R$和R-ASMK的$\hat V_R$。请注意，这个定义涉及到两个图像的区域聚合，而在本工作中，我们关注的是非对称的区域聚合，即只应用于数据库图像。当查询图像本身是一个定位良好的感兴趣区域时，非对称情况更相关，这是图像检索基准测试中常见的设置。