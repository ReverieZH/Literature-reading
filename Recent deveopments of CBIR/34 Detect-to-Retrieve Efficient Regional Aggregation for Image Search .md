**Detect-to-Retrieve: Effificient Regional Aggregation for Image Search**

# 摘要

在杂乱的场景中高效地检索对象实例需要紧凑而全面的区域图像表示。直观地说，对象语义可以帮助构建关注最相关区域的索引。然而，由于缺少针对检索基准中感兴趣对象的边界框数据集，最近关于区域表示的工作主要集中在统一区域选择或与类无关的区域选择上。在本文中，我们首先通过提供一个新的地标边界框数据集来填补这一空白，这个数据集是基于谷歌Landmarks数据集，其中包括86k张图片和来自15k个unique Landmarks的手动编辑框。然后，我们演示了如何使用我们的新数据集，训练一个地标探测器，可以利用索引图像区域和提高检索精度，同时比现有的区域方法更有效。此外，我们还引入了一种新的区域聚合选择匹配核(R-ASMK)来有效地将检测区域的信息组合成一种改进的整体图像表示。在不增加维数的情况下，R-ASMK大大提高了图像检索的准确性，甚至优于单独的索引图像区域的系统。我们完整的图像检索系统在之前的先进水平的基础上进行了改进，在 Revisited Oxford and Paris datasets有显著优势。

# 引言

本文主要针对图像检索问题:给定一个查询图像，系统应该有效地从数据库中检索相似的图像。

图像检索系统通常由两个主要阶段组成:

(1)过滤，一种根据数据库图像与查询的相似性对其进行排序的有效技术;

(2)重新排序，对第一阶段的少量最相似的数据库图像进行更详细的检查并重新排序。

通常，手工标记的局部特性[21,6]与受单词启发的技术[36,26,27,14,15,16,38]相结合，构建过滤步骤中使用的高维表示。

局部特征匹配和几何验证[26,27,3](https://blog.csdn.net/weixin_42403696/article/details/通常使用RANSAC[8])是有效的重新排序策略。

最近，针对这两个阶段提出了几种深度学习技术。基于卷积神经网络(CNN)的全局图像表示可以产生紧凑的嵌入，从而在滤波步骤中实现快速的相似度计算[5,4,40,1,9,30]。

局部图像表示也可以使用CNNs提取，适合通过空间匹配和几何验证重新排序[25,24,23]。

当前的图像检索系统往往会在相关对象在数据库图像中没有占据足够大的比例时失败，尤其是在一些混乱的场景中。通常，这些对象生成的局部特征可用于在重新排序阶段查找与查询图像匹配的局部匹配。然而，这些杂乱的图像通常无法达到重新排序的阶段，因为与过滤阶段的查询相比，它们的初始表示不会产生很高的相似性。

估计查询图像的改进相似性最常见的解决方案是，使用固定的区域网格[2,31]或类无关的检测器[37,17]，提取并单独存储数据库中感兴趣区域的图像表示形式。然而，现有的区域选择技术产生了大量的不相关区域。在最近的一次大规模实验图像检索评估中，Radenovic等人[28]得出结论，这种区域搜索方法在内存和延迟方面的代价太高，只能获得很小的精度增益。

**贡献** (

1)第一个贡献是改进了区域选择:通过引入了一个人工装箱的地标图像数据集，其中包含来自15k个唯一类的86k个图像，我们证明了可以训练检测器进行健壮的地标定位。

(2)第二个贡献是利用训练有素的探测器，产生更有效的区域搜索系统，该系统只需要稍微增加数据库大小就可以提高小目标的准确性——比以前提出的技术效率高得多。

(3)第三个贡献中，提出了区域聚合匹配内核来利用选定的图像区域，并产生一个有区别的图像表示，如图1所示。这种新的表示方式显著优于区域搜索系统，同时更高效:每个图像只需要存储一个描述符。我们的图像检索系统在重访问牛津硬数据集上的绝对平均精度比以前发表的结果高9.3%，在重访问巴黎硬数据集[28]上的绝对平均精度高1.9%。



# **相关工作**

数据集：

图像检索和聚合：在图像检索系统中，对区域选择进行了研究。它们被用于两个不同的目的:(i)区域搜索:选定的区域在数据库中独立编码，以便检索子图像;(二)区域聚合:利用选定区域改进图像表示。

区域搜索：

许多论文提出使用VLAD[15]或Fisher向量[16]来描述区域：阿兰德耶洛维奇和齐泽曼[2]使用多尺度网格提取每幅图像14个区域；Tao等人[37]使用选择性搜索[41]，每张图像有数千个区域；Kim等人[17]使用最大稳定极值区域(MSER)[22]。Razaian等人[31]使用每幅图像30个区域的多尺度网格，并通过考虑所有区域对之间的距离计算两幅图像的相似性。Iscen等人[13,12]利用多尺度网格结合CNN特性[29]，通过扩散实现查询扩展。最近，拉德诺维奇等人对检索技术进行了全面的评估，并得出结论，现有的区域搜索方法可以提高识别准确性，但需要更大的记忆和复杂性成本。相比之下，我们的检测到检索框架旨在通过使用自定义训练的探测器进行高效的区域搜索。



区域聚合：

Tolias等人[40]利用[31]的网格结构将预训练的CNN特征[18,35]汇集成紧凑的表示；每张图像选择大约20个区域。

[29]等人通过在无监督的方式收集的数据集上重新训练特征。

Gordo等人的[9]从半自动的边界框注释中训练一个区域建议网络[32]，以取代[40]中的网格。在本例中，每幅图像将考虑数百个区域。

我们的工作与这些论文不同，我们使用了一小组区域（每张图像少于5个），并将区域聚合作为一个新的匹配核（而不是[40,9]中的区域sum-pooling）。

# **区域检索和聚合**

我们提出了技术，提高图像检索性能利用边界盒预测训练的地标检测器。

特别是，我们的方法建立在深度局部特征(DELF)[25]和聚合选择匹配内核(ASMK)[38]之上，这些特征最近在大规模图像检索基准[28]上实现了最先进的性能。

## **4.1背景**


通过Tolias [38]等人的聚合匹配核函数框架。

一个图像 X 描述成在每一个维度D上包含M个位置描述符的集合，一个由C个视觉语言组成的码本C，使用k-means学习，用来量化这个描述符。可以利用最近邻算法判断图像X描述符哪些是视觉语言。所以根据这个框架就可以对两个图像之间的相似性进行计算。

一个图像X描述成集合 $\chi=\{x_1,x_2...x_M \}$包含M个局部描述符，每一个维度D上。一个码本C包含C个视觉单词，使用k-means学习

，用来量化这个描述符。

$\chi_c=\{x\in \chi：q(x)=c \}$作为X中的描述子的子集，由最近邻的邻居量化器q(x)分配给视觉词c。

由局部描述符集X和Y表示的两幅图像X和Y之间的相似性可以计算为：

![image-20220711142714318](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711142714318.png)

Φ(X)是一个聚合的向量表示，σ(.)表示标量选择性函数， $\mathcal{Y}(\chi)=(\sum_c \sigma (\Phi (\chi_c)^T\Phi (\chi_c)))^{-\frac12}$ 是归一化因素

这个公式包含了流行的局部特征聚合技术，如单词包[36]、VLAD[15]和ASMK[38]。

特别是对于VLAD，$\sigma(u)=u$，$\Phi (\chi_c)$对应于一个聚合的残差$V(\chi_c=\sum_{x\in \chi_c}x-q)$

对于ASMK，σ(u)对应于一个阈值的多项式选择性函数

![image-20220711144701044](D:\文献阅读\Recent deveopments of CBIR\image\image-20220711144701044.png)