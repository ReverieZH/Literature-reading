# SCDA



为ImageNet分类任务预训练的深度卷积神经网络模型已成功应用于其他领域的任务，如纹理描述和对象建议生成，但这些任务需要对新领域的图像进行注释。

在本文中，我们关注一个新颖的和具有挑战性的任务在纯无监督的设置：细粒度图像检索。即使有图像标签，细粒度的图像也很难进行分类，更不用说无监督检索任务了。

我们提出了选择性卷积描述子聚合(SCDA)方法。SCDA首先在细粒度图像中定位主要对象，这一步骤可以丢弃噪声背景并保持有用的深度描述符。

然后，使用我们找到的最佳实践，将选定的描述符聚合并将维度降为一个短的特征向量。

SCDA是无监督的，不使用图像标签或边界框注释。在6个细粒度数据集上的实验证实了SCDA在细粒度图像检索中的有效性。

此外，SCDA特征的可视化显示它们对应于视觉属性（甚至是细微的属性），这可能解释了SCDA在细粒度检索中的高平均精度。此外，在一般的图像检索数据集上，SCDA获得了与最先进的一般图像检索方法相当的检索结果。



# 一、引言

利用卷积神经网络(CNN)[1]在图像分类方面的突破，预先训练的CNN模型（如识别或检测）的CNN模型也被应用于不同的领域(例如，用于描述纹理[2]或寻找对象建议。然而，这种预先训练过的CNN模型的适应仍然需要在新领域的进一步注释（例如，图像标签）。在本文中，我们证明了对于只包含类别之间细微差异（如狗的种类）的细粒度图像，预先训练好的CNN模型既可以定位主要对象，又可以找到相同种类的图像。由于没有使用监督，我们称这种新颖且具有挑战性的任务为细粒度图像检索。

在细粒度图像分类[4]、[5]、[6]、[7]、[8]、[9]中，类别对应于同一物种的品种。这些类别都很相似，只是有一些细微的差别。因此，一个精确的系统通常需要很强的注释，例如，对象甚至对象部分的边界框。在许多实际应用程序中，这种注释是昂贵且不现实的。为了解决这一难题，人们尝试对细粒度图像进行分类只使用图像级标签，例如[6], [7], [8], [9].。 

在本文中，我们处理了一个更具挑战性但更现实的任务，即细粒度图像检索(FGIR)。在FGIR中，给定相同物种的数据库图像（例如，鸟、花或狗）和一个查询，我们应该返回与查询具有相同种类的图像，而不诉诸于任何其他监督信号。FGIR在生物研究和生物多样性保护等应用方面很有用。如图1所示，FGIR也不同于通用图像检索。一般的图像检索侧重于基于内容的相似性（如纹理、颜色和形状）检索接近重复的图像，而FGIR侧重于检索相同类型的图像（例如，动物的相同物种和汽车的相同模型）。同时，细粒度图像中的物体只有细微的差别，而且在姿态、尺度和旋转上也有变化。

![image-20220721095709096](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721095709096.png)

为了应对这些挑战，我们提出了选择性Conv演化描述符聚合(SCDA)方法，该方法可以自动在细粒度图像中定位主对象并提取它们的判别表征。在SCDA，只有一个预先训练的CNN模型(来自ImageNet，这不是使用的是细粒度的)，而且我们完全没有使用任何监督。如图2所示，预先训练好的CNN模型首先进行提取对输入图像的卷积激活。我们提出一个确定激活的哪一部分的新方法有用(例如，用于定位对象)。然后，使用我们在SCDA中提出的实践，对这些有用的描述符进行聚合和降维，以形成一个向量表示。最后, 最近邻居搜索结束FGIR进程。

![image-20220721100406632](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721100406632.png)

我们在6个流行的细粒度数据集(CUB200-2011[10]、斯坦福狗[13]、牛津花102[14]、Oxford-IIIT Pets[15]、飞机[16]和汽车[11])上进行了广泛的实验，用于图像检索。此外，我们还在标准的通用检索数据集(INRIA Holady[17]和Oxford SCDA 5K[12])上测试了所提出的建筑方法。此外，我们报告了SCDA方法只使用图像标签的方法的分类精度。检索实验和分类实验都验证了SCDA的有效性。我们的方法的主要优点和主要贡献是：

- 我们提出了一种简单而有效的方法来定位主要对象。这种定位是无监督的，不使用边界框、图像标签、对象建议或额外的学习。SCDA只选择有用的深度描述符，并删除背景或噪声，这有利于检索任务。

- 通过多个CNN层的集成和所提出的降维实践，SCDA比现有的基于深度学习的方法表示得更短但更准确。对于细粒度的图像，如表三所示，SCDA的检索结果最好。此外，SCDA在通用图像检索数据集上也有准确的结果。

- 如图8所示，压缩的SCDA特征与视觉属性（甚至是微妙的属性）具有更强的对应关系，这可能解释了SCDA在细粒度任务中的成功。

  

此外，除了特定的细粒度图像检索任务外，我们提出的方法还可以被视为一种迁移学习，即使用针对一个任务训练的模型(在ImageNet上的图像分类)来解决另一个不同的任务（细粒度图像检索）。它确实揭示了深度卷积神经网络的可重用性。

本文的其余部分组织如下。  Sec. II介绍了一般深度图像检索和细粒度图像任务的相关工作。所提出的SCDA方法的细节见Sec. III.。 Sec. IV，对于细粒度的图像检索，我们将我们的方法与几种基线方法和三种最先进的通用深度图像检索方法进行了比较。此外，还对SCDA特性的质量进行了讨论。 Sec. V 总结了这篇论文。

# 二、 RELATED WORK

我们将简要回顾两项相关的工作：图像检索的深度学习方法和细粒度图像的研究

## *A. Deep Learning for Image Retrieval*

直到最近，大多数图像检索方法都是基于局部特征(SIFT是一个典型的例子)和在这些局部特征之上的特征聚合策略。局部聚合描述子向量(VLAD)[18]和Fisher向量(FV)[19]是两种典型的特征聚合策略。在CNN[1]获得成功后，图像检索也采用了深度学习。从预先训练好的深度网络中获得的开箱即用的功能，在许多与视觉相关的任务中实现了最先进的结果，包括图像检索[20]。

一些研究（如[21]、[22]、[23]、[24]、[25]、[26]，[27]）研究了哪些深度描述符以及如何在图像检索中使用，取得了令人满意的结果。在[21]中，为了在不降低CNN识别能力的情况下提高CNN激活的不变性，他们提出了多尺度无有序池化(MOP-CNN)方法。MOPCNN首先从多个尺度层次的全连接层中提取CNN激活，并在每个层次上分别对这些激活进行无秩序的VLAD[18]池化，最后将特征连接起来。在此之后，[22]通过对相关数据集进行有或没有微调已经广泛地评估了这些特征的性能。这项工作表明，pca压缩的深度特征可以优于用传统的类sift特征计算的紧凑描述符。

后来，[23]发现在最后一个卷积层上使用和池来聚合深度特征可以获得更好的性能，并提出了和池卷积(SPoC)特征。

在此基础上，[25]在和池之前对空间和每个通道的加权来创建最终的聚合。

[27]提出了一种紧凑的图像表示，该表示来自于卷积层激活，编码多个图像区域，而不需要将多个输入反馈到网络。

最近，[26]的作者研究了CNN激活在图像检索和分类中的几种有效应用。特别是，它们聚合了每一层的激活，并将它们连接到最终的表示中，从而取得了令人满意的结果。

然而，这些方法直接使用CNN激活/描述符，并将它们编码为单一表示，而无需评估所获得的深度描述符的有效性。相比之下，我们提出的SCDA方法只能选择有用的深度描述符，并通过无监督地定位主要对象来去除背景或噪声。同时，我们还提出了SCDA用于检索任务的几种良好实践。此外，以往的基于深度学习的图像检索方法都是为一般的图像检索而设计的，这与细粒度的图像检索有很大的不同。正如我们的实验所显示的那样，最先进的一般图像检索方法并不能很好地适用于细粒度的图像检索任务。

此外，在过去的几年中，我们还研究了几种图像检索的变体，如多标签图像检索[28]、基于草图的图像检索[29]和医学CT图像检索[30]。在本文中，我们将重点关注新颖的和具有挑战性的细粒度图像检索任务。

## *B. Fine-Grained Image Tasks*

细粒度分类在过去的几年中非常流行，在文献[4]、[5]、[6]、[7]、[8]、[9]中发展了许多有效的细粒度识别方法。

我们可以将这些方法大致分为三组。第一组，例如，[31]，[8]，试图通过开发强大的深度模型来分类细粒度图像来学习更有区别性的特征表示。第二组将对象在细粒度图像中对齐，以消除姿态变化和相机位置的影响，例如，[5]。最后一组专注于基于部分的表示。然而，由于为大量图像获得强注释（对象边界框和/或部分注释）是不现实的，更多的算法尝试仅使用图像级标签对细粒度的图像进行分类，如[6]、[7]、[8]、[9]。

所有以前的细粒度分类方法都需要图像级标签（其他标签甚至需要部分注释）来训练它们的深度网络。很少有工作涉及到对细粒度图像的无监督检索。Wang等人，[32]提出了深度排名来学习细粒度图像之间的相似性。然而，它需要图像级的标签来构建一组三联体，这不是无监督的，也不能很好地用于大规模的图像检索任务。

与FGIR的相关研究是[33]。[33]的作者提出了一个细粒度的图像搜索问题。[33]使用了具有SIFT特征的词袋模型，而我们使用了预先训练过的CNN模型。除了这个区别之外，一个更重要的区别是数据库的构建方式。[33]通过合并几个现有的图像检索数据集，构建了一个层次数据库，这些数据集包括细粒度的数据集(例如，CUB200-2011和斯坦福狗)和一般的图像检索数据集（例如，牛津建筑和巴黎）。给定一个查询，[33]首先确定它的元类，如果该查询属于细粒度元类别，然后进行细粒度图像搜索。在FGIR中，该数据库包含一个单一物种的图像，这更适合于细粒度的应用程序。例如，一个鸟类保护项目可能不希望给定一个鸟类查询但找到狗的图像。据我们所知，这是第一次尝试使用深度学习进行细粒度图像检索。

# 三. 选择性卷积描述符聚合

在本节中，我们提出了选择性卷积描述符聚合(SCDA)方法。首先，我们将介绍本文中使用的符号。然后，我们给出了描述符的选择过程，最后，我们还描述了特征聚合的细节。

## *A. Preliminary*

在本文的其余部分中，将使用以下符号。

术语“特征图”表示一个通道的卷积结果；

术语“激活”表示卷积层中所有通道的 feature maps；

术语“描述符”表示激活的d维组成向量。

“池5”是指最大池化的最后一个卷积层的激活，

“fc8”是指最后一个完全连接的层的激活

给定一个大小为H×W的输入图像I，卷积层的激活被表示为一个3阶张量T包含hxwxd元素，其中包括一组二维特征映射$S={S_n}(n=1，…，n)$。大小为h×w的Sn是对应通道的第n个特征图(第n个通道)。

从另一个角度来看，T也可以被认为具有 hxw 个细胞，每个细胞包含一个d维深度描述符。

我们将深度描述符表示为$X=\{x_{(i，j)}\}$，其中(i，j)是一个特定的单元格($i∈\{1，……，h\}，j∈\{1，……，w\}，x_{(i，j)}∈R^d$)。例如，通过使用流行的预训练的VGG-16模型[34]来提取深度描述符，如果输入图像为224×224，我们可以在池5中得到一个7×7×512激活张量。因此，一方面，对于这张图像，我们有512个大小为7×7的特征图(即Sn)；另一方面，也得到了49个512-d的深度描述符

## *B. Selecting Convolutional Descriptors*

SCDA与现有的基于深度学习的图像检索方法的区别在于：仅使用预先训练好的模型，SCDA就能够找到有用的深度卷积特征，这实际上是将主要对象定位在图像中，并丢弃无关的和有噪声的图像区域。请注意，预先训练好的模型没有使用目标细粒度数据集进行微调。在下面，我们提出了我们的描述符选择方法，然后给出了定量和定性的定位结果

### **1) Descriptor Selection:**

在获得池5激活后，输入图像I用一个3阶张量T表示，这是一个稀疏分布的表示[35]，[36]。分布式表示论证声称，概念是由一种分布在多个神经元[37]上的分布式活动模式所编码的。在深度神经网络，分布式表示意味着多对多之间的关系（即概念和神经元）：每个概念是由一个模式的活动分布在许多神经元，和每个神经元参与许多概念的表示[35]，[36]。

在图3中，我们展示了一些来自五个细粒度数据集的图像，CUB200-2011[10]，斯坦福狗[13]，牛津花102[14]，飞机[16]和Cars[11]。我们从池5中的512个特征图中随机抽取几个特征图，并将它们叠加到原始图像上，以便更好地可视化。从图3中可以看出，采样特征图的激活区域（用暖色突出显示）可能表示鸟/狗/花/飞机/汽车中有语义意义的部分，但也可以表示这些细粒度图像中的一些背景或噪声部分。

![image-20220721103745162](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721103745162.png)

此外，即使对于同一通道，被激活区域的语义意义也有很大的不同。例如，在右边的第464个鸟类特征图中，第一张图片中激活的区域是松树莺的尾巴，第二张图片是*Black-capped Vireo*的头部。在关于狗的第274个特征图中，第一个显示了德国牧羊犬的头部，而第二个*Cockapoo*甚至没有激活的区域，除了部分嘈杂的背景。

其他的鲜花、飞机和汽车也有相同的特点。此外，还有一些激活区域代表背景，例如，松树莺的第19个特色地图和德国的第418个牧羊犬。

图3表明，并不是所有的深度描述符都是有用的，而且由于这种表示的分布式特性，一个单一的通道最多只能包含较弱的语义信息。因此，只选择和使用有用的深度描述符（并去除噪声）是必要的。然而，为了决定哪个深度描述符是有用的（即，包含我们想要检索的对象），我们不能单独计算任何单个通道。

我们提出了一种简单而有效的方法（如图2所示），其定量和定性的评价将在下一节中进行演示。虽然一个单一的通道不是很有用，但如果许多通道在同一区域触发，我们可以期望这个区域是一个对象，而不是背景。因此，在所提出的方法中，我们通过深度方向将得到的池5激活张量相加。因此hxwxd的3-D张量变成一个2-D张量，我们称它为聚合图，例如$A=\sum _{n=1}^dS_n$(其中Sn是$pool_5$中的第n个特征图)

对于聚合图A，有 hxw个求和激活响应，对应于h×w个位置。基于上述观察，可以直接说，一个特定位置(i，j)的激活响应越高，其相应区域成为物体的一部分的可能性就越大。此外，细粒度的图像检索是一个无监督的问题，其中我们没有如何处理它的先验知识。因此，我们计算A中所有位置的平均值$\overline a$作为决定哪些位置定位对象的阈值：激活响应高于$\overline a$的位置(i，j)表示主要对象，如鸟、狗或飞机，可能出现在该位置。可以获得与A大小相同的掩模映射M：

![image-20220721104854374](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721104854374.png)

在图3中，每个细粒度数据集的最后第二列的数字分别显示了鸟类、狗、鲜花、飞机和汽车的掩模地图的一些例子。对于这些图，我们首先使用**双边插值**来调整掩模映射M的大小，使其大小与输入图像相同。然后我们将相应的掩版图（用红色突出显示）叠加到原始图像上。即使所提出的方法没有在这些数据集上进行训练，但主要的对象（如鸟、狗、飞机或汽车）也可以**被粗略地**检测到。然而，从这些图中可以看出，在一个复杂的背景下，仍有几个小的噪声部分被激活。幸运的是，由于噪声部分通常比主对象要小，所以我们采用算法1来收集M的最大连通分量，记为$\widetilde{M}$，以消除噪声部分引起的干扰。在最后一列中，主要对象由$\widetilde{M}$保存，而噪声部分被丢弃，如植物、云和草。

![image-20220721113114847](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721113114847.png)

因此，我们使用$\widetilde{M}$来选择有用的和有意义的深度卷积描述符。当$\widetilde{M_{i,j}}$=1时，应保留描述符x(i，j)，而$\widetilde{M_{i,j}}$=0表示该位置(i，j)可能有背景或噪声部分：

![image-20220721110025575](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721110025575.png)

其中，F表示所选的描述符集，它将被聚合为最终的表示形式，以检索细粒度的图像。整个卷积描述符的选择过程如图2b-2e所示

### **2) Qualitative Evaluation:**

在本节中，我们给出了所提出的描述符选择过程的定性评价。由于四个细粒度的数据集(即CUB200-2011，斯坦福狗、飞机和汽车)为每个图像提供了地面真实边界框，因此需要对所提出的目标定位方法进行评估。然而，如图3所示，检测到的区域形状不规则。因此，返回包含被检测到的区域的最小矩形边界框作为我们的对象定位预测。

我们评估了所提出的方法来定位整个对象（鸟，狗，飞机或汽车）在他们的测试集。预测的示例如图4所示。从这些图来看，预测的边界盒相当准确地近似于地面真实边界盒，甚至有些结果比地面真实要好。例如，在图4所示的第一幅狗的图像中，预测的边界框可以覆盖两只狗；在第三个图中，预测框包含的背景较少，有利于检索性能。此外，在许多情况下，预测的飞机和汽车盒几乎与地面真实边界盒相同。然而，由于我们没有使用任何监督，所以一些细粒度对象的细节，例如，鸟类的尾巴，就不能被预测的边界框准确地包含起来。

![image-20220721112341681](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721112341681.png)

### *3) Quantitative Evaluation:*

我们还在表I中报告了用于对象定位的正确定位部分(PCP)度量的结果。报告的度量标准是使用>50%IOU和地面真实边界框正确本地化的整个对象框的百分比。

在这个表中，对于CUB200-2011，我们展示了在之前一些基于部分定位的细粒度分类算法中报告的两个细粒度部分（即头部和躯干）的PCP结果。在这里，我们首先比较全对象的定位率与细粒度部分的定位率，以进行粗略的比较。事实上，躯干边界盒与CUB200-2011中的整个物体高度相似。

通过比较躯干PCP和我们的整个对象的结果，我们发现，即使我们的方法是无监督的，定位性能只是略低，甚至与这些使用强监督的算法相当，例如，地面真实边界框和部分注释（甚至在测试阶段）。对于斯坦福狗，我们的方法可以获得78.86%的目标定位精度。此外，飞机和汽车飞机的检测结果分别为94.91%和90.96%，验证了所提出的无监督目标定位方法的有效性。

此外，在我们提出的方法中，保留了所得到的掩模映射的最大连通分量$\widetilde{M}$。我们进一步研究了这个过滤步骤如何通过删除这个处理来影响对象定位性能。然后，基于M，基于这些数据集的目标定位结果为：小熊200-2011、斯坦福狗、飞机和汽车的目标定位结果分别为45.18%、68.67%、59.83%和79.36%。基于M的定位精度远低于基于$\widetilde{M}$的定位精度，证明了获得最大连接分量的有效性。此外，我们还通过与地面真实边界盒的大小的关系来考虑这些下降。从这个角度来看，图5分别显示了在四个细粒度数据集上被地面真实边界框覆盖的整个图像的百分比。很明显，CUB200-2011和飞机的大多数地面真实边界框的大小都小于整个图像的50%。因此，对于这两个数据集，下降点很大。而对于汽车，如图5d所示，百分比的分布接近于正态分布。对于斯坦福狗来说，一些地面真相边界框覆盖了不到20%的图像大小或超过80%的图像大小。因此，对于这两个数据集，去除最大的连接组件处理的效果可能很小

![image-20220721113454591](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721113454591.png)

更重要的是，因为我们的方法不需要任何监督，一种最先进的无监督对象定位方法，即[39]，作为基线进行。[39]使用现成的区域建议来为对象形成一组候选边界框。然后，这些区域在图像之间使用概率霍夫变换进行匹配评估每个候选对应关系的置信度，同时考虑到外观和空间的一致性。在此之后，通过比较候选区域的得分，并选择那些突出于包含它们的其他区域的区域，来发现和定位主导对象。由于[39]不是一种基于深度学习的方法，因此它在这些细粒度数据集上的大部分定位结果都不令人满意，如表一所示。具体来说，对于许多飞机的图像，[39]将整个图像作为相应的边界框预测。而如图5c所示，只有一小部分的地面真实边界盒接近整个图像，这可以解释为什么[39]在飞机上的无监督定位精度比我们的要差得多。

![image-20220721113825014](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721113825014.png)

## *C. Aggregating Convolutional Descriptors*

经过选择过程，得到所选的描述符集$F=\{x_{(i，j)}|\widetilde{M}_{i，j}=1\}$。在下面，我们将比较几种编码或池化方法来聚合这些卷积特征，然后给出我们的建议。

*•* **Vector of Locally Aggregated Descriptors** (VLAD)

VLAD是计算机视觉中一种流行的编码方法。VLAD使用k-means找到k个质心的码本$\{c_1，……，c_K\}$,将$x_{(i，j)}$映射为一个单个向量$v_{(i，j)}=[0…0\ \ x_{(i，j)}−c_k\ …0]∈R^{K×d}$,其中ck是最接近$x_{(i，j)}$的质心。F的最终表示为$\sum_{i,j}v_{(i,j)}$

**• Fisher Vector** (FV)

FV的编码过程与VLAD相似。但是它使用了一个软赋值（即高斯混合模型），而不是使用k-means来预先计算代码本。此外，FV还包括二阶统计数据。

*•* **Pooling approaches**.

我们还尝试了两种传统的池化方法，即全局平均池化和最大池化，来聚合深度描述符，即，

![image-20220721114729154](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721114729154.png)

其中$p_{avg}$和$p_{max}$都是1×d维的。N是所选描述符的数量。

将所选描述符集F编码或池化为单个向量后，对于VLAD和FV，进行平方根归一化和l2归一化；

对于最大池和平均池方法，我们进行l2归一化（平方根归一化效果不佳）。

最后，利用余弦相似度进行最近邻搜索。我们使用两个数据集来证明哪种类型的聚合方法是最优的细粒度图像检索。使用了在数据集中提供的原始训练和测试分割。测试集中的每幅图像都被视为一个查询，训练图像被视为图库。表二报告了top-k的mAP检索性能。

![image-20220721115048976](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721115048976.png)

对于VLAD/FV的参数选择，我们遵循[40]中报道的建议。VLAD中的簇数设为2,FV中的高斯分量设为2。如表中所示，值越大，精度就越低。此外，我们发现与高维编码方法相比，全局最大池和平均池等更简单的聚合方法获得了更好的检索性能。这些观察结果也与[23]在一般图像检索中的研究结果相一致。VLAD和FV在这种情况下工作不佳的原因与需要聚合的深度描述符数量相当少有关。CUB200-2011和斯坦福狗的每张图片中选择的深度描述符的平均数量分别为40.12和46.74。然后，我们建议将全局最大池和平均池表示“avg&maxPool”连接作为我们的聚合方案。它的性能明显且始终高于其他性能。我们使用“avg&maxPool”聚合作为“SCDA特性”来表示整个细粒度的图像。

## *D. Multiple Layer Ensemble*

正如[41]，[42]所研究的，多层集成提高了最终的性能。因此，我们还加入了另一个由relu5_2层产生的SCDA特性，它是在VGG-16模型[34]中的池5前面的三层。

在池5之后，我们从relu5_2 得到掩模映射$M_{relu_{5\_2}}$ 。它的激活与语义意义的关系小于池5。如图6(c)所示，噪声部分较多。然而，鸟比池5更准确地探测到。因此，我们将$\widetilde{M}_{pool_5}$和$M_{relu5\_2}$结合在一起，得到relu5_2 的最终掩模图。

![image-20220721155638829](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721155638829.png)

$\widetilde{M}_{pool_5}$首先被上采样到$M_{relu_{5\_2}}$的大小。当描述符在$\widetilde{M}_{pool_5}$和$M_{relu_{5\_2}}$中的位置都为1时，我们保留描述符,它们是最终选择的relu5_2的描述符。聚合过程保持不变。最后，我们将relu5_2 和pool5的SCDA特征连接到一个单一的表示形式中，用“SCDA+”表示：

![image-20220721120029361](D:\文献阅读\Recent deveopments of CBIR\image\image-20220721120029361.png)

其中，α为SCDArelu5 2 的系数。对于FGIR，它被设置为0.5。

然后，我们对连接特征进行l2归一化。此外，还合并了原始图像水平翻转的另一个SCDA+，记为“SCDA flip+”(4096-d)

此外，我们还尝试结合来自更多不同层的特性，例如，池4。然而，检索性能略有提高(大约0.01%∼0.04%top-1mAP)，而特征维数比提出的SCDA特征大得多