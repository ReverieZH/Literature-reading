# Aggregating local descriptors into a compact image representation



我们在非常大的规模上解决图像搜索的问题，其中必须共同考虑三个约束条件：搜索的准确性、效率和表示的内存使用。

我们首先提出了一种简单而有效的方法，将局部图像描述符聚合为一个有限维数的向量，这可以看作是 *Fisher*核表示的一种简化。

然后我们展示如何联合优化降维和索引算法，以最好地保持向量比较的质量。

评估表明，我们的方法明显优于最新技术：对于适合20个字节的图像表示，搜索精度与特征包方法相当。搜索一个1000万个图像数据集大约需要50个ms。

# 1.Introduction

特征包图像表示(BOF)在索引和分类应用中很受欢迎，有两个原因。

首先，这种表示受益于强大的本地描述符，如SIFT描述符[12]和最近的[13,28,29]。其次，这些向量表示可以用标准距离进行比较，然后被支持向量机等鲁棒分类方法使用。

当用于大规模图像搜索时，表示图像的BOF向量被有利地选择为高维的[16,20,10]，高达100万维。

在这种情况下，搜索效率来自于使用反向列表[23]，它加快了稀疏向量之间距离的计算。

然而，有两个因素限制了在实践中可以索引的图像数量：搜索本身的效率，当考虑超过1000万张图像时，这变得禁止，以及表示图像所需的内存。

在本文中，我们解决了在一个非常大的图像数据库（1000万张或更多的图像）中搜索最相似的图像的问题。我们强调了三个约束条件的联合优化：搜索精度、搜索效率和内存使用量。最后两个是相关的[24]，因为搜索效率可以近似于要访问的内存量。

与我们的工作最相似的是[9]的方法，它提出了一种对BOF向量的近似最近邻搜索。然而，这种方法仅限于小词汇量，搜索精度较低。此外，一个图像仍然需要超过100个字节来重现一个相对较低的搜索精度的低维BOF向量。

最小哈希方法[2,3]和托雷萨尼等人的[25]方法部分解决了效率问题。然而，这些技术仍然需要每幅图像大量的内存，并在接近重复的图像检测的背景下是更相关的，因为它们的搜索精度明显低于BOF。一些作者通过使用GIST描述符[17]，并将它们转换为压缩的二进制向量[5,11,24,27]来解决效率和内存限制。这些方法受到全局GIST描述符的低不变性的限制，并且没有一种方法共同满足上述三个约束条件：[24,27]仍然需要穷举搜索，而[11]由于LSH算法的冗余性而消耗内存。

 相比之下，我们的方法通常通过20字节的表示获得了显著更高的精度。这是通过优化而获得的：

1. **表示法，即如何将局部图像描述符聚合为一个向量表示法**
2. **这些向量的降维方法；**
3. **索引算法。**

这些步骤密切相关：用高维向量表示图像通常比用低维向量提供更好的穷举搜索结果。然而，高维向量更难有效地索引。相比之下，低维向量更容易被索引，但其鉴别能力较低，可能不足以识别物体或场景。

我们的第一个贡献在于提出了一种表示，该表示提供了具有合理的向量维数的优秀搜索精度，因为我们知道该向量将随后被索引。

我们提出了一个描述符，由BOF和Fisher核[18]导出的描述符，它聚合了SIFT描述符并产生一个紧凑的表示。

它被称为VLAD（局部聚合描述符的向量）。实验结果表明，在相同尺寸下，VLAD的性能显著优于BOF。它的计算成本较低，通过主成分分析(PCA)可以将其维数降低到几百个分量，而不会显著影响其精度

作为第二个贡献，我们展示了联合优化降维和索引算法之间的权衡的优势。我们特别考虑了[7]最近的索引方法，因为我们可以直接比较由PCA引起的误差和由索引引起的误差，这是由于从其编码的索引近似重建的向量。

在展示了启发我们的两种图像向量表示，BOF和Fisher核[18]之后，我们在第2节中介绍了我们的描述符聚合方法。降维和索引的联合优化。

实验结果证明了我们的方法在第4节的性能：我们证明了BOF的性能是通过一个大约20个字节的图像表示来实现的。在内存使用、搜索准确性和效率方面，这都是对最先进的[9]的一个显著改进。



# 2. Image vector representation

在本节中，我们将简要回顾两种流行的方法，它们从一组局部描述符中生成图像的向量表示。然后，我们提出了聚合局部描述符的方法。

## 2.1. Bag of features

BOF表示法对局部描述符进行了分组。

它需要定义一个k个“视觉词”的码本，通常由k-means聚类获得。

图像中维数d的每个局部描述符都被分配给最近的质心。

BOF表示是指将所有图像描述符分配给视觉词的直方图。

因此，它产生一个k维向量，然后被归一化。关于如何规范化直方图有几个变体。当被视为经验分布时，BOF向量使用曼哈顿距离进行归一化。另一个常见的选择是使用欧几里得标准化。然后用idf（逆文档频率）项对向量分量进行加权。[16,23]已经提出了几种加权方案。下面，我们对直方图进行L2归一化，并使用[23]的idf计算。



1. 从图片当中提取特征（visual words)（一般使用SIFT特征，这个特征具体是什么不太需要了解，我们只要知道它可以从一张图片中找到若干个特征点，然后对这些点生成一个n维的向量表示）
2. 把所有图片里面得到的所有特征收集起来，然后做KNN生成k个centroid
3. 对于图片当中的每个特征，都有一个对应的centroid，我们就可以统计每个centroid在这张图片里出现的次数，作为一个新的向量（论文里叫histogram，但其实就是一个表示频率的向量），来描述这张图片（这里有一点绕，这个新的向量是在SIFT局部向量经过处理之后生成的，用来描述这张图片整体特征的向量）
4. 我们再对这些向量进行处理

已经提出了几种变体来提高这种表示的质量。最流行的[21,26]之一是使用软量化技术，而不是k-means方法。

## 2.2  Fisher kernel

Fisher核[6]是一个强大的工具，可以将一个输入的变量大小的独立样本集转换为一个固定大小的向量表示，假设样本遵循一个在一个训练集上估计的参数生成模型。这个描述向量是样本相对于该分布参数的似然梯度，由Fisher信息矩阵的平方反根缩放。它给出了参数空间的方向，应该修改学习到的分布，以更好地拟合观测数据。结果表明，在这个新的表示空间中可以学习判别分类器。

[18]等人将Fisher核应用于图像分类。他们用高斯混合模型(GMM)对视觉单词进行建模，这被限制为每个混合的k个分量的对角线方差矩阵。

得到了一个GMM的Fisher矩阵的对角近似，他们获得(2d+1)×k−1维向量表示图像特征集，或d×k维当只考虑与GMM的均值或方差相关联的分量时。与BOF表示相比，这种更复杂的表示所需要的视觉单词更少。

## 2.3. VLAD: vector of locally aggregated descriptors

提出了一种基于在特征空间中局部性准则的聚合描述符的图像向量表示。

它可以看作是Fisher核的一种简化。

对于BOF，我们首先学习了一个码本$C=\{c_1，…c_k\}$一个带有k个视觉单词的k-均值。

每个局部描述符x与其最近的视觉单词$c_i=NN(x)$相关联。VLAD描述符的思想是累积，对于每个视觉单词$c_i$，向量x的差$x-c_i$赋给$c_i$。这描述了向量相对于中心的分布。

假设局部描述符是d维的，我们表示是D=kxd维。

在下面，我们用$v_{i，j}$来表示描述符,其中，索引$i=1…k$和$j=1…d$分别索引了视觉词和局部描述符组件。

因此，v的一个分量是对所有图像描述符的和：

![image-20220712104649036](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712104649036.png)

其中$x_j$表示描述符x的第j个维度，$c_{i,j}$表示视觉单词的第j个维度 

![[公式]](https://www.zhihu.com/equation?tex=v) 中的第i个row是所有以 ![[公式]](https://www.zhihu.com/equation?tex=c_i) 为centroid的visual words到该点距离之和

向量v随后被l2归一化，$v=\frac v{||v||_2}$

这个 ![[公式]](https://www.zhihu.com/equation?tex=v) 就是VLAD中描述图片整体特征的向量。

实验结果表明，该方法可以取得良好的效果, 即使使用相对较少的可视单词k:我们考虑k=16到k=256之间的值

图1描述了当聚合128维的SIFT描述符时，与一些图像相关联的VLAD表示。我们的描述符的组件映射到SIFT描述符的组件。因此，我们对每个$v_i=1..k$采用通常的定向梯度的4×4空间网格表示。我们已经积累了16个描述符，每个视觉单词一个。与SIFT描述符相比，由于方程1中的差异，一个分量可能是正的或负的。

我们可以观察到，描述符是相对稀疏的（很少的值有显著的能量）和非常结构化的：大多数较高的描述符值位于同一个簇中，并且SIFT描述符的几何结构是可以观察到的。直观地说，如后面所示，主成分分析很可能捕获这个结构。对于足够相似的图像，描述符的紧密程度是很明显的。

# 3. From vectors to codes

本节讨论了对图像向量的编码问题。给定一个d维输入向量，我们希望生成一个编码图像表示的B位代码，这样就可以在n个编码的数据库向量集合中有效地搜索一个（非编码的）查询向量的最近邻。

我们分两个步骤来处理这个问题，这必须共同进行优化：

1)降低向量维数的投影，

2)用于索引结果向量的量化。为此，我们考虑[7]的最近的近似最近邻搜索方法，这将在下一节中简要描述。我们将通过测量每一步产生的均方欧氏误差来显示联合优化的重要性。

提取到VLAD特征后，要先用[PCA](https://so.csdn.net/so/search?q=PCA&spm=1001.2101.3001.7020)降维，然后再用ADC方法对每一幅图像建立索引

## 3.1. Approximate nearest neighbor

在计算机视觉应用程序中，处理大型数据库需要近似的最近邻搜索方法[4,11,15,24,27]

最流行的技术之一是欧几里得局部敏感哈希,在[11]中已经扩展到任意的指标

但是，这些方法和[15]方法消耗内存，因为需要几个哈希表或树。

[27]方法将向量嵌入到二进制空间中，更好地满足内存约束。然而，在内存和准确性之间的权衡方面，它明显优于使用基于乘积量化的[7]近似搜索方法。

在下面，我们使用这种方法，因为它提供了更好的精度，也因为搜索算法提供了索引向量的显式近似。这使我们能够比较由降维和量化所引入的向量近似。

我们使用这种方法的非对称距离计算(ADC)变体，它只编码数据库的向量，而不编码查询向量。该方法总结如下。

**ADC approach.**

对图片库中，除query vector x之外的所有图的vector ，做kmeans产生k个聚类中心，用bit编码这k个center的ID，比如k=16，yi属于c8，那么用4bit编码yi：

设$x∈ℜ^D$为一个查询向量a，$Y=\{y_1，……，y_n\}$是一组我们想找到x的最近邻NN(x)向量。

ADC方法包括通过一个量化版本的$ c_i=q(y_i)∈ℜ^D$对每个向量yi进行编码

对于量化器q（.）用k个质心，向量由log2(k)位编码，k是2的幂。

寻找x的最近邻NNa(x)只需要计算

![image-20220712114229717](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712114229717.png)

请注意，与[27]的嵌入方法相比，查询x没有被转换为代码：在查询端没有近似错误。

然而这里存在的问题是，k必须是一个较小的值，这样会导致信息损失较严重，因为上百万个图，最后只对应到了16种编码，搜索精度会很低。如果想用64bit编码， $k=2^{64}$，聚类中心的个数太多，kmeans计算代价很大。

所以这里参考论文《Product quantization for nearest neighbor search》中的方法，对ADC方法做优化。
这种方法是把vector y划分成m个子向量，如果y长度为D，那么每个子向量长度：D/m，定义product quantizer：
![image-20220712114635716](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712114635716.png)

也就是对每个子向量做上述的ADC编码。把$Y=\{y_1，……，y_n\}$各自的第j个子向量拿出来$y_1^j，……，y_n^j$，用kmeans把他们聚为 ks类，这个 ks是一个固定的值，所以每个子向量被编码成 $log_2k_s$bit。
此时(2)式中的距离计算就成了如下形式：
![image-20220712114856518](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712114856518.png)

在search之前，我们可以提前计算出一个lookup table保存子向量 xj分别到 ks个聚类中心的距离，生成lookup table的复杂度为 O(D∗ks)，当ks<<n时，这个值相比于(2)式的复杂度O(D*n)是可以忽略不计的。
根据这个编码，可以对 yiyi做分解：
![image-20220712114947230](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712114947230.png)

其中 εq(yi)是指这次编码所造成的损失(quantization loss)。
这样，ADC就把一个vector编码成B bit， $B=mlog_2k_s.$

## 3.2. Indexation-aware dimensionality reduction

降维是近似最近邻搜索中的一个重要步骤，因为它影响了后续的索引方法。

在本节中，对于ADC方法，我们使用单一的质量度量来表示这个操作和索引方案之间的折衷：近似误差。为了便于表示，我们假设向量的平均值是零向量。这大约是对于VLAD向量的情况

主成分分析(PCA)是降维的标准工具[1]：与经验向量协方差矩阵的D‘最大能量特征值相关的特征向量 用于定义矩阵M映射向量$x∈ℜ^D$到向量$x’=Mx∈ℜ^D$‘。矩阵M是一个正交矩阵的D‘×D的上部。

这种降维也可以在初始空间中解释为一个投影。在这种情况下，x的近似为

![image-20220712172116872](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712172116872.png)

其中，误差向量$ε_p(x)$位于M的空空间中。通过M的伪逆向量$x_p$与x‘有关，在这种情况下是M的转置。

因此，投影是$x_p=M^⊤Mx$。

为了建立索引，随后使用ADC方法将向量x‘编码为q(x’)，这也可以在原始的d维空间中解释为近似值

![image-20220712173205444](D:\文献阅读\Recent deveopments of CBIR\image\image-20220712173205444.png)

其中$ε_p(x)∈Null(M)$和$ε_q(x_p)∈Null(M)^⊥$(因为ADC量化器是在主子空间中学习的) 是正交的。在这一点上，我们做了两个观察